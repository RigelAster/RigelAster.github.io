<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Introduction to Decal Rendering</title>
      <link href="/2024/12/19/Translations/Introduction%20to%20Decal%20Rendering/"/>
      <url>/2024/12/19/Translations/Introduction%20to%20Decal%20Rendering/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>贴花(Decals)是一种将贴图细节呈现在场景中的技术。此技术可以在不改变原始贴图的情况下，赋予物体更多的贴图细节。因为贴花实现时不需要改变物体模型，所以可以在运行时实现。一个经典的案例就是，子弹打在墙体上留下的弹坑。</p><p>正是由于贴花与其作用的模型和贴图无关，贴花技术在场景设计中非常有用。相比于创建有不同裂纹的墙体贴图变体，使用一张基地贴图配合裂纹贴花显然更为便捷。在迭代产品时，更改贴花也比重置一张贴图要省力不少。</p><p>最简单的贴花可以是一个有纹理的四边形(quad)，放置在物体平面上。但是，贴花能够适用复杂的表面显然更好，所以典型的贴花是通过某种形式投影至物体表面的。</p><p><img src="/images/Translations/Decals/decal-in-Half Life.png"></p><p>这是游戏 Half Life: Alyx中的截图，可以看到墙面上的贴花。发光的油漆是贴在普通墙壁上的贴花，此外还有一系列由玩家创造的弹孔贴花。很幸运的是，这两种贴花是由前文提到的两种方式创建的，将在下文中详细论述。</p><p>为事物命名是一件麻烦的事情，许多事物都被命名了不止一次。本文将以<code>视角空间投影(view-space projection)</code> 和<code>网格生成(generated mesh)</code>来称呼两种实现方式，当然不可避免地它们会以不同的名字出现在其他文章中。</p><h2 id="view-space-projection">View-Space Projection</h2><p>当人们讨论到贴花技术时，最先想到的就是这种实现方式。<strong>视角空间投影贴花技术是通过在渲染期间执行投影过程来实现的。</strong>每个贴花都以一种简单网格的形式存在于场景中——通常是立方体。贴花通过片段着色器(fragmentshader)渲染，着色器会重建其所在位置的世界坐标。使用此世界坐标以及贴花本身的坐标和大小，便可以计算出应出现在对应片段中的贴花纹理。</p><p><img src="/images/Translations/Decals/decal-scene-01.png"></p><p>（注：原文中有可交互的场景，建议去尝试一下。由于技术原因，无法在本文中复现）</p><p>上图便是在简单场景中实现的视角空间投影贴花。场景中的其他物体已将深度信息存储在贴图中，供贴花着色器使用。已知场景深度和相机位置的情况下，可以重建每一个点的世界坐标，随后计算出贴花纹理对应的位置。</p><p>一旦片段的<strong>世界坐标</strong>已知，下一步要做的仅仅是将其转换到<strong>物体空间</strong>中。虽然听起来比较复杂，但是将实际坐标与贴花模型的<strong>物体矩阵的逆矩阵</strong>相乘即可。下图是使用Unityshader graph实现的简单案例：</p><p><img src="/images/Translations/Decals/shader-graph-01.png"></p><p><img src="/images/Translations/Decals/shader-graph-02.png"></p><p>这种渲染贴花方法的<strong>主要缺点</strong>是，需要对贴花立方体上的每个点进行场景深度采样。采样场景深度纹理和重建世界坐标的运算并非很耗时，但是随着场景中贴花的增多，其消耗也在增加。</p><p><img src="/images/Translations/Decals/decal-scene-01.png"></p><p>上图中，那些被处理但是没有被绘制贴花的部分被标记为了红色。可以看到，在一些位置上，有一大部分区域被浪费了。通过图，也可以直观看到贴花是如何通过立方体渲染的。</p><p>其实也是有一些优化手段的。通过缩放贴花立方体，使其尽可能接近被投影的表面，从而减少"红色"区域。类似的，如果你知道相机永远不会被放置在立方体内，可以剔除背面并将zTest设置为<code>lessOrEqual</code>。否则，应该例子中那样做，将zTest设置为<code>greaterOrEqual</code>来剔除正面。</p><p>在计算世界位置之后，对贴花纹理采样之间，着色器可以推断出其位于红色区域内，便可以直接丢弃该片段。只是，这种有条件的提早推出并不总是有利于着色器性能，需视情况而定。</p><p><img src="/images/Translations/Decals/decal-in-Half Life-02.png"></p><p>回到游戏《Half Life》中，我们可以使用 RenderDoc看到弹坑贴花是通过本章所述的方法实现的。上图展示了用于绘制贴花的其中一个网格。为了创建弹孔，游戏所要做的就是在子弹击中的位置放置一个适当旋转的立方体，并用贴花着色器渲染。因为弹坑贴花的区域较小，需要做的处理也比较小。一般情况下，贴花还是会在一段时间后被移除，放置过多的性能消耗。</p><h2 id="generated-mesh-decals">Generated Mesh Decals</h2><p>前文中我们讨论到，最简单的贴花实现方式可能就只是一张带贴图的四边形(texturedquad)。<strong>网格生成贴花是那种方法的拓展，创建一个与目标表面匹配的网格。</strong>此方法缺点就是，创建自定义网格比创建立方体要慢的多。</p><p><img src="/images/Translations/Decals/decal-in-Half Life-03.png"></p><p>游戏HalfLife在一些位置使用网格生成贴花来为场景添加纹理装饰，上图中的网格线便是喷漆贴花的网格（技术上来讲，是我为了方便而合并的两个网格）。贴花的网格并不是其下平面墙体和砖块的副本，而是由贴花附近的裁剪到贴花边界内的三角形组成的。图中可以看到墙面网格是如何构成一个旋转四边形的，但是其还被分割出了额外的三角形。这些额外的三角形是由墙面网格的原始三角形裁剪而来。</p><p><img src="/images/Translations/Decals/decal-scene-03.png"></p><p>上图依旧是我们的示例场景，只是这次使用了网格生成来渲染贴花，当其每次移动时都会重新生成。（注：原文中场景可交互）贴花使用了可以展示网格线的着色器，即使在现代电脑上你也会注意到贴花的相应速度比视角空间投影版本要慢，因为它需要一些时间来重新生成网格。</p><p>如果贴花需要在快速放置在场景中，或者需要适应不断变化的表面，那么网格生成贴花不是一个很好的选择。但是如果贴花被用来装饰场景或对象，并且不需要重新生成，那么它可以提供比视角空间贴花更好的渲染性能。</p><p>生成贴花网格过程包括检测附近的场景三角形，将它们转换到贴花对象的空间并裁剪以适应边界。更详细的处理过程超出了本文的范围，但这里可提供一个<ahref="https://github.com/Anatta336/driven-decals/blob/master/Runtime/MeshProjection.cs">基本实现</a>。关键的简化是，通过将三角形转换到贴花对象空间，可以使用更容易的几何操作将它们裁剪到轴对齐盒中。</p><h2 id="unity-legacy-redraw-and-overlay">Unity Legacy: Redraw andOverlay</h2><p>本章是Unity中“旧的”投影渲染系统所使用的技术。与网格生成贴花类似，其渲染被放置在表面的网格。但与网格生成贴花不同的是，其不会生成一个新的网格，而是使用不同的材质重新绘制它附近物体的整个网格。通常情况下，被重绘的物体都是透明的，会造成性能浪费。如果贴花放置在大型物体附近，那么需要处理大量的顶点和片元，即使大多数片元会被丢弃掉。</p><p>这种方法也是有优势的，其不需要读取场景的深度缓存。之前某些平台不支持读取深度缓存，但今天这种限制已经微乎其微，所以应该使用更加有效的方法。</p><h2 id="unity-hdrp-prepass-and-apply">Unity HDRP: Prepass and Apply</h2><p>我对HDRP的贴花系统没有详细了解，但我可以列出大致过程：</p><ul><li><p>Depth-only pass for the scene.</p></li><li><p>Decals pass with depth test writing the colour (and normals,metallic, and occlusion vlaues) from the decal into a set of screensized buffers. Decal projection is performed here in the fragmentshader, using the earlier depth pass to find world-spaceposition.</p></li><li><p>Standard deferred rendering of the scene to g-buffers, but thefragment shaders also read from those decal buffers and incoporatethem.</p></li><li><p>The rest of the deferred rendering process continues as it wouldwithout decals. The g-buffers are used to produce a shaded finalimage.</p></li></ul><p>此方法的缺点是，需要有一组额外的屏幕大小的缓冲区占用内存空间，并且需要作为渲染的一部分进行采样。我希望写入缓冲区的成本与前文讨论的视图空间贴花具有相同的性能特征。</p><p>值得一提的是，延迟渲染使一些惊艳的贴花效果成为可能。例如，贴花可以只写入法线缓冲，使表面看起来具有凹凸特征的同时保持其基本颜色和其他属性不变。</p><h2 id="choose-your-decal-technique">Choose your Decal Technique</h2><p>View-space projection: 需要实时创建贴花、需要适应变化的表面。</p><p>Generated mesh: 在贴花静态物体时有更好的性能。</p><p>If you're looking to try <ahref="https://github.com/Anatta336/driven-decals/">generated mesh decalsin Unity's URP</a>, you may want to try the system I made. It's free andopen source.</p><h2 id="参考资料">参考资料</h2><p><ahref="https://samdriver.xyz/article/decal-render-intro">Introduction toDecal Rendering</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Decal Rendering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Optimizing draw calls</title>
      <link href="/2024/12/12/Unity/Optimizing%20draw%20calls/"/>
      <url>/2024/12/12/Unity/Optimizing%20draw%20calls/</url>
      
        <content type="html"><![CDATA[<h2 id="optimizing-draw-calls">Optimizing draw calls</h2><p>Unity向图形API发出drawcalls指令来在屏幕上绘制几何图形，指令告诉图形API绘制什么以及如何绘制。每个drawcall都包括了图形API在屏幕上绘制所需的所有信息，利于纹理、着色器和缓冲区相关信息。drawcall调用是资源密集型的(resource intensive)，但通常调用drawcall所需的准备工作比draw call本身更占用资源。</p><p>为了调用draw call，CPU需要先准备资源(sets upresources)并改变GPU内部设置(internalsettings)，这些设置被统称为渲染状态(renderstate)。改变渲染状态，例如切换到不同的材质，通常是图形API执行的最耗费资源的操作。</p><p>由于渲染状态的改变是资源密集型的，所以对其进行优化是非常重要的。优化渲染状态改变的最主要方法就是减低其次数，有以下两种方法：</p><ul><li><p><strong>Reduce the total number of draw calls.</strong> drawcall此处减少之后，draw call之间渲染状态的改变也会随之减少。</p></li><li><p><strong>Organize draw calls in a way that reduces the number ofchanges of the render state.</strong>如果图形API可以使用相同的渲染状态去执行多个draw calls，就可以将这些drawcalls当作一组，而不需要去执行多种渲染状态之间的切换。</p></li></ul><p>drawcalls和渲染状态改变的优化，对应用有许多好处。最主要的，它可以提升帧率，除此之外还有：</p><ul><li><p>Reduces the amount of electricity your application requires.减少电量消耗和设备发热。</p></li><li><p>Improves maintainability of future development on youapplication. 增加应用的可维护性，方便后续扩展。</p></li></ul><p>在Unity有几种方式可以用来进行优化，降低drawcall数量和渲染状态的改变。每种方法都有最优的使用场景，如下：</p><ul><li><p><strong>GPU instancing</strong>: Render multiple copies of thesame mesh at the same time.GPU实例对于需要大量绘制的几何非常合适，例如树木和草。</p></li><li><p><strong>Draw call batching</strong>: Combine meshes to reducedraw calls. Unity提供了以下类型的内置批处理： -- <strong>staticbatching</strong>:预先将静态(被标记为static)物体的mesh整合。Unity将被整合的mesh传输给GPU，但是被整合的子mesh会独立进行渲染。Unity任然可以对mesh进行独立裁剪，但是由于数据的状态并不会改变，所以每个drawcall的资源消耗会变少。 -- <strong>dynamic batching</strong>:在CPU端变换mesh的顶点，成组后的顶点使用相同的设置(groups vertices thatshare the same configuration)，通过一个drawcall调用便可渲染。如果顶点存储了相同数量和类型的属性，则它们共享相同的配置。例如，位置和法线。</p></li><li><p><strong>Manully combining meshes</strong>:通过使用<code>Mesh.CombineMeshes</code>函数，手动将多个mesh合并为一个mesh。Unity会使用一个drawcall来渲染合并后的mesh。</p></li><li><p><strong>SRP Batcher</strong>:如果项目使用SRP，在物体具有同一shader变体时，可以使用 SRP Batcher来减少Unity准备和调用draw call所需的CPU时间，</p></li></ul><h3 id="optimization-priority">Optimization priority</h3><p>你可以在同一场景中使用多种drawcall优化方案，但是应注意Unity会根据优先级来确定具体的使用方案。通常情况下，如果一个物体使用了超过一种drawcall优化方案，Unity会选择优先级最高的那个。但是，这里有一种情况除外，即SRPBatcher。当使用SRP Batcher时，Unity也会对符合SRPBatcher规范的物体(GameObjects that are SRP Batchercompatible)支持静态合批。Unity依据以下优先级来调用drawcall优化方案：</p><ul><li><p>SRP Batcher and static batching</p></li><li><p>GPU instancing</p></li><li><p>Dynamic batching</p></li></ul><p>如果将一个物体标记为静态合批而且Unity成功进行合批操作，Unity就会关闭此物体的GPUinstancing功能。此状况发生时，Unity会在Inspector面板展示警告。其他同理。</p><h2 id="gpu-instancing">GPU instancing</h2><p>GPU instancing是一种draw call的优化方案，其在一个drawcall中渲染使用<strong>相同mesh和material</strong>的物体的多个副本。每一个副本被称为实例(instance)。这在渲染同一场景出现很多次物体时非常有帮助，例如树木和草地。</p><p>GPU instancing可以在一个drawcall内渲染相同mesh很多次，为了增加多样性以及降低重复性，实例之间可以拥有不同的属性，例如Color和Scale。渲染了多个实例的drawcall在FrameDebugger中会被标记为<code>Render Mesh(instanced)</code>。</p><h3 id="requirements-and-compatibility">Requirements andcompatibility</h3><p>本小节包含关于平台、渲染管线和SRP Batcher与GPUinstancing之间的兼容性。</p><h4 id="platform-compatibility">Platform compatibility</h4><p>GPU instancing在几乎所有平台都是可用的，除了WebGL 1.0。</p><h4 id="render-pipeline-compatibility">Render pipelinecompatibility</h4><table><thead><tr><th>Feature</th><th>BIRP</th><th>URP</th><th>HDRP</th><th>SRP</th></tr></thead><tbody><tr><td>GPU instancing</td><td>yes</td><td>yes(1)</td><td>yes(1)</td><td>yes(1)</td></tr></tbody></table><p><strong>Notes</strong>:</p><ol type="1"><li>Only if the shader isn't compatible with the SRP Batcher.</li></ol><h4 id="srp-batcher">SRP Batcher</h4><p><strong>GPU instancing 和 SRP Batcher并不兼容。</strong>如果项目使用了SRP Batcher，但是你想使用GPUinstancing渲染物体，可以使用以下方法：</p><ul><li><p>Use Graphics.RenderMeshInstanced.这个api绕过GameObjects的使用，使用指定参数直接在屏幕上绘制网格。</p></li><li><p>Manually remove SRP Batcher compatibility.</p></li></ul><h3 id="using-gpu-instancing">Using GPU instancing</h3><p><strong>Unity uses GPU instancing for GameObjects that share the samemesh and material.</strong> 实例化mesh和material需要符合以下要求：</p><ul><li><p>The material's shader must support GPU instancing.</p></li><li><p>The mesh must come from one of the following sources, grouped bybehavior: -- MeshRenderer组件或者Graphics.RenderMesh函数调用。 --Graphics.RenderMeshInstanced 或者Graphics.RenderMeshIndirect函数调用。</p></li></ul><h3 id="performance-implications">Performance implications</h3><p>使用GPU instancing不能有效处理低顶点数量的网格(low number ofvertices)，因为GPU不能充分利用资源来工作。这种处理效果低下，会对性能产生不利影响。不同GPU效率低下的阈值不同，一般情况下，不要对少于256个顶点的网格使用GPUinstancing。</p><p>如果你想多次渲染一个低顶点数的网格，最好的做法是创建一个包含所有网格信息的缓冲区，并是哟个它来绘制网格。</p><h2 id="draw-call-batcing">Draw call batcing</h2><p>GPU instancing是一种drawcall的优化方案，其将mesh整合使Unity能用更少的drawcall来进行渲染。可进一步分为以下两种方案：Static batching 和 Dynamicbatching。但是，其也有一些缺点。静态合批会产生内存和存储开销，而动态合批会产生额外的CPU开销。</p><h3 id="requirements-and-compatibility-1">Requirements andcompatibility</h3><table><thead><tr><th>Feature</th><th>BIRP</th><th>URP</th><th>HDRP</th><th>SRP</th></tr></thead><tbody><tr><td>Static Batching</td><td>Yes</td><td>Yes</td><td>Yes</td><td>Yes</td></tr><tr><td>----------------</td><td>----</td><td>---</td><td>----</td><td>---</td></tr><tr><td>Dynamic Batching</td><td>Yes</td><td>Yes</td><td>No</td><td>Yes</td></tr></tbody></table><h3 id="using-draw-call-batching">Using draw call batching</h3><h3 id="batching-static-gameobjects">Batching static GameObjects</h3><p>Static batching通过将静态mesh整合来减少drawcall数量。其将整合的mesh变换到世界空间，构建一个共享的 vertex buffer 和index buffer。之后Unity执行一个单独的drawcall使用整合的mesh来一次性渲染所有物体。Static batching可以显著减少drawcall的调用数量。因为静态合批不会再CPU端执行顶点变换，所以 staticbatching 比 dynamic batching 更加高效。</p><h4 id="performance-implications-1">Performance implications</h4><p>使用静态合批需要额外的CPU存储空间去存储整合过后的几何体。如果物体们使用了相同的mesh，Unity会对每一个物体都创建拷贝，然后将拷贝整合。这意味着，相同的mesh在整合的mesh中会多次出现。不管是使用编辑器还是运行时API来为静态合批提供数据，Unity都会执行上述操作。</p><p>如果想要保持较小的内存占用，可能不得不牺牲渲染性能来避免某些物体进行静态合批。例如，在茂密的森林环境将树木标记为静态可能会对内存产生严重影响。</p><p>注：静态批处理的定点数量有上限，每个静态处理批次最多包含64000顶点。如果超出限制，Unity会再额外创建一个静态批次。</p><h3 id="batching-moving-gameobjects">Batching moving GameObjects</h3><p>动态合批对meshes和dynamiclly generatedgeometries处理并不相同，更多信息请参考文章<ahref="https://docs.unity3d.com/6000.0/Documentation/Manual/dynamic-batching-meshes.html">Dynamicbatching for meshes</a>和<ahref="https://docs.unity3d.com/6000.0/Documentation/Manual/dynamic-batching-meshes.html#dynamic-batching-dynamic-geometry">Dynamicbatching for dynamically generated geometries</a></p><p>注：对网格的动态批处理被设计为优化低端设备的性能。在现代硬件上，动态批处理在CPU上所做的工作可能大于一个drawcall的开销。</p><h4 id="limitations">Limitations</h4><h2 id="参考资料">参考资料</h2><p><ahref="https://docs.unity3d.com/2022.3/Documentation/Manual/optimizing-draw-calls.html">Optimizingdraw calls</a></p>]]></content>
      
      
      <categories>
          
          <category> Unity </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
            <tag> Draw Calls </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnSRP - Draw Calls</title>
      <link href="/2024/12/11/LearnSRP/Draw%20Calls/"/>
      <url>/2024/12/11/LearnSRP/Draw%20Calls/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>本章是LearnSRP系列第二篇，原文还包括了Shader编写，但本文着重于高效的渲染物体(drawingmultiple objects efficiently)。</p><h2 id="shaders">Shaders</h2><p>在渲染物体时，CPU需要告诉GPU渲染什么以及如何渲染。渲染的对象通常为mesh，如何渲染则通过shader来定义，其是一组GPU指令。除去mesh，shader还需要额外的信息来执行渲染，包括物体的变换矩阵(transformationmatrices)和材质属性(material properties)。</p><p>本节的其他部分请参考原文献，不做展开。</p><h2 id="batching">Batching</h2><p>每一次drawcall指令都会使CPU和GPU进行数据交流。如果大量的数据需要被传输给GPU，这一过程就会花费大量时间，而且CPU在传输数据时会影响到其他事情的处理。两者都会造成帧率的降低。目前我们的渲染方式很直接：每个对象都会调用drawcall指令。虽然我们最终发送的数据很少，但这是最糟糕的调用方式。</p><p>为了展示技术效果，先在场景中摆放49个sphere，它们使用到了四个材质：red，green，yellow，blue。此时渲染这些物体需要49个drawcall，每个sphere都会调用一次。</p><p><img src="/images/LearnSRP/Draw Calls/scene-01.png" width="400"></p><h3 id="srp-batcher">SRP Batcher</h3><p>Unity的Batching是combining drawcalls的过程，用于减少CPU和GPU之间的通信时间。使用此技术最简单的方法就是其启用着色器的SRPbatcher选项，但其只对compatibleshaders生效，需要对前面的Unlit着色器进行一些修改。但是如何判断一个着色器是不是compatibleshaders呢？Unity在Inspector中给了提示。如下图：</p><p><img src="/images/LearnSRP/Draw Calls/unity-tips-01.png" ></p><p>SRP batches技术不会直接减少drawcall的数量，而是让他们变得更加高效。它将材质的属性存储在GPU中，避免每次drawcall都要传输。如此，既减少了必须通信的数据量，也减少了CPU在每个drawcall中必须完成的工作。但是，这只有在着色器遵循严格统一的数据结构时才有效。</p><p>所有材质属性都必须在特定的缓冲区中声明，而不是在全局级别。具体来说就是将属性声明在<code>cbuffer</code>块中，命名为UnityPerMaterial。写起来类似于结构体声明，但必须以分号结尾。其通过将属性存放在一个特定的常量内存缓冲区(constantmemory buffer)，但是其仍然可以全局访问。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cbuffer UnityPerMaterial&#123;</span><br><span class="line">    float4 _BaseColor;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>但是常量缓存并不是全部平台都支持（像是OpenGL ES2.0），于是需要使用Core RPLibrary提供的<code>CBUFFER_START</code>和<code>CBUFFER_END</code>宏。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">CBUFFER_START(UnityPerMaterial)</span><br><span class="line">    float4 _BaseColor;</span><br><span class="line">CBUFFER_END</span><br></pre></td></tr></table></figure><p>除去<code>UnityPerMaterial</code>，Unity还提供了<code>UnityPerDraw</code>缓存来区别不同用途，顾名思义。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CBUFFER_START(UnityPerDraw)</span><br><span class="line">    float4x4 unity_ObjectToWorld;</span><br><span class="line">    float4x4 unity_WorldToObject;</span><br><span class="line">    real4 unity_WorldTransformParams;</span><br><span class="line">CBUFFER_END</span><br></pre></td></tr></table></figure><p><img src="/images/LearnSRP/Draw Calls/unity-tips-02.png"></p><p>在着色器符合规范之后，需要开启SRP batcher，通过将<code>GraphicSettings.useScriptabeRenderPipelineBatching</code> 设置为true 。在开启之后，49个sphere的渲染在Frame Debugger中就显示为了一个SPRBatch：</p><p><img src="/images/LearnSRP/Draw Calls/framedebugger-01.png"></p><h3 id="many-colors">Many Colors</h3><p>上面我们使用了四个Material，但是只用了一个batch。SPRBatcher生效之后，所有是数据都被一次性缓存在GPU中，每次drawcall只需要包含一个offset来定位正确的数据位置。此方法的唯一限制是每个Material的内存布局必须是相同的，上文中，我们材质使用的都是相同的着色器。Unity不会对材质的内存布局进行比较，只会批量绘制使用相同着色器变体(shadervariant)的draw call。</p><p>如果我们想要一些不同的颜色，这很容易实现。但是如果我们想要每个球体都有自己的颜色，就需要创建更多的Material。如果可以为每个对象设置颜色，那就方便多了。默认情况下，这是不可能的，但可以通过创建自定义组件类型来支持它。创建<code>PerObjectMaterialProperties</code>，作为一个例子，将其放在CustomRP下的示例文件夹中。</p><p>我们的想法是，每个物体都会挂载<code>PerObjectMaterialProperties</code>脚本，其拥有一些着色器的属性设置。</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">using</span> UnityEngine;</span><br><span class="line"></span><br><span class="line">[<span class="meta">DisallowMultipleComponent</span>]</span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title">PerObjectMaterialProperties</span> : <span class="title">MoniBehaviour</span>&#123;</span><br><span class="line">    <span class="keyword">static</span> <span class="built_in">int</span> baseColorId = Shader.PropertyToID(<span class="string">&quot;_BaseColor&quot;</span>);</span><br><span class="line"></span><br><span class="line">    [<span class="meta">SerializeField</span>]</span><br><span class="line">    Color baseColor = Color.white;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通过<code>MaterialPropertyBlock</code>来设置逐物体的材质属性，我们只需要一个实例复用即可。</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> MaterialPropertyBlock block;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">OnValidate</span>()</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(block == <span class="literal">null</span>)&#123;</span><br><span class="line">        block = <span class="keyword">new</span> MaterialPropertyBlock();</span><br><span class="line">    &#125;</span><br><span class="line">    block.SetColor(baseColorId, baseColor);</span><br><span class="line">    GetComponent&lt;Renderer&gt;().SetPropertyBlock(block);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将此脚本挂载至场景中的7个sphere，分别赋予不同颜色。此时，SRPbathcer被打断，这7个sphere分别使用一个通常的draw call。</p><p><img src="/images/LearnSRP/Draw Calls/scene-02.png" width="400"></p><p><img src="/images/LearnSRP/Draw Calls/framedebugger-02.png"></p><h3 id="gpu-instancing">GPU Instancing</h3><p>GPU Instancing是另外一种整合drawcall的方法，它可以处理逐对象的材质属性。其通过整合具有相同mesh的对象，使用一次drawcall来绘制。CPU会收集所有对象的变换和材质属性，将其放入数组中，然后发送给GPU。GPU会遍历数组，并按顺序渲染它们。</p><p>因为GPUInstacing需要的数据是以数组形式提供的，目前的着色器并不支持。要做的第一步就是通过添加指令<code>#pragma multi_compile_instancing</code>。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> multi_compile_instancing</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> vertex UnlitPassVertex</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> fragment UnlitPassFragment</span></span><br></pre></td></tr></table></figure><p>如此，Unity会生成两个着色器变体，一个支持GPUinstancing，一个不支持，还会在Insptactor面板上提供切换的开关。GPUinstancing正常工作需要知道当前被渲染物体的索引值index，此值通过顶点数据(vertexdata)提供。<code>UnityInstancing.hlsl</code>提供了一些宏定义使索引值使用变的简单些，将其加入顶点数据结构即可。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Attributes</span>&#123;</span></span><br><span class="line">    float3 positionOS : POSITION;</span><br><span class="line">    UNITY_VERTEX_INPUT_INSTANCE_ID</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float4 <span class="title function_">UnlitPassVertex</span><span class="params">(Attributes input)</span>: SV_POSITION &#123;</span><br><span class="line">    UNITY_SETUP_INSTANCE_ID(input);</span><br><span class="line">    float3 positionWS = TransformObjectToWorld(input.positionOS);</span><br><span class="line">    <span class="keyword">return</span> TransformWorldToHClip(positionWS);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>尽管SPR batcher的优先级更高，渲染结果没什么不同，GPUintancing理论上来说已经可以运行了。但是，此时我们还不支持逐实例(per-instance)的材质属性。需要对代码做出以下改变：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//CBUFFER_START(UnityPerMaterial)</span></span><br><span class="line"><span class="comment">//    float4 _BaseColor;</span></span><br><span class="line"><span class="comment">//CBUFFER_END</span></span><br><span class="line"></span><br><span class="line">UNITY_INSTANCING_BUFFER_START(UnityPerMaterial)</span><br><span class="line">    UNITY_DEFINE_INSTANCED_PROP(float4, _BaseColor);</span><br><span class="line">UNITY_INSTANCING_BUFFER_END(UnityPerMaterial)</span><br></pre></td></tr></table></figure><p>当GPUinstancing使用时，还需要让实例索引在片元着色器UnlitPassFragment中可用。依旧使用到了宏：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Varyings</span>&#123;</span></span><br><span class="line">    float4 positionCS : SV_POSITION:</span><br><span class="line">    UNITY_VERTEX_INPUT_INSTANCE_ID</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Varyings <span class="title function_">UnlitPassVertex</span><span class="params">(Attributes input)</span>&#123;</span><br><span class="line">    Varyings output;</span><br><span class="line">    UNITY_SETUP_INSTANCE_ID(input);</span><br><span class="line">    UNITY_TRANSFER_INSTANCE_ID(input, output);</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float4 <span class="title function_">UnlitPassFragment</span><span class="params">(Varyings input)</span>:SV_TARGET&#123;</span><br><span class="line">    UNITY_SETUP_INSTANCE_ID(input);</span><br><span class="line">    <span class="keyword">return</span> UNITY_ACCESS_INSTANCED_PROP(UnityPerMaterial, _BaseColor);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/images/LearnSRP/Draw Calls/framedebugger-03.png"></p><p>可以看到Unity将14个（为了对比增加了一些球体）sphere进行了一些合批处理，降低了drawcall的数量。<strong>GPUInstancing只对使用相同材质的物体生效</strong>，我们在14个球中使用了4个不同材质。因为脚本存在设置颜色，我们可以使用同一材质，使其进一步进行合批处理。可以看到它们都变成了一个drawcall：</p><p><img src="/images/LearnSRP/Draw Calls/framedebugger-04.png"></p><p>需要注意的是，batch size是有限制的，根据目标平台而定。</p><h3 id="drawing-many-instanced-meshes">Drawing Many InstancedMeshes</h3><p>请阅读原文</p><h3 id="dynamic-batching">Dynamic Batching</h3><p>动态合批(dynamic batching)也是用来降低drawcall的一种方法。是一种老技术，其将使用同一材质的小型mesh整合称为一个整体mesh，然后再进行渲染。此方法也不适用于逐物体(per-object)材质属性的情况。</p><p>此方法只适用于小型mesh(顶点数较少)，通常境况下，GPUintancing比动态批处理更高效。相同的还有静态批处理，工作原理相同，但是会被提前标记为静态对象。这种处理的特点是，需要更大的内存和存储空间。</p><h2 id="参考资料">参考资料</h2><p><ahref="https://catlikecoding.com/unity/tutorials/custom-srp/draw-calls/">CatlikeCoding - Draw Calls</a></p>]]></content>
      
      
      <categories>
          
          <category> Unity SRP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
            <tag> SRP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Screen Space Reflection For Beginners</title>
      <link href="/2024/11/29/Translations/Screen%20Space%20Reflection%20for%20Beginners/"/>
      <url>/2024/11/29/Translations/Screen%20Space%20Reflection%20for%20Beginners/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>本文绝大多数内容源自文章<ahref="https://lettier.github.io/3d-game-shaders-for-beginners/screen-space-reflection.html">ScreenSpaceReflection</a>，进行部分内容的翻译，学习SSR技术。文章结构上进行了微调。</p><h2 id="screen-space-reflection-ssr">Screen Space Reflection (SSR)</h2><p><img src="/images/ScreenSpaceReflection/final.gif"></p><p>给场景添加反射效果绝对是一笔不错的买卖，湿润和具有光泽的表面会让场景更加生动。</p><p><ahref="https://lettier.github.io/3d-game-shaders-for-beginners/lighting.html">灯光章节</a>，我们曾模拟过光源的反射。回想一下，光源高光是使用光线反射方向计算的。与之类似，SSR可以模拟场景中物体反射其他物体。不是来自光源的光线反射到相机上，而是来自场景中某物体的光线反射到相机上。</p><p>SSR的原理是将屏幕图像反射到其自身上。（可能有些抽象，不过之后会理解的。）可以将其与cubemapping对比来理解，在cubemapping中，光线在场景中某点反射到了包围盒的内部；在SSR中，光线从屏幕上某处反射到屏幕上的另外一处。通过将屏幕上的某处反射到其自身，可以创造出反射现象的错觉。基于此原理，通常情况下SSR效果不错，但也会有失效的情况（当反射的目标没有位于屏幕图像上）。</p><h2 id="ray-marching">Ray Marching</h2><p>SSR使用ray marching技术决定每个片段的反射结果，raymarching(中文一般成为光线步进)是通过扩展或收缩某些向量的长度或大小，以探测或采样某些空间信息的过程。SSR中的光线是指，被场景法线反射后的位置向量(positionvector)。</p><p>通俗来讲，场景中的一束光线打中场景某处，被反射，然后沿着反射后的positionvector的反方向继续行进，被当前片段(fragment)反射，然后沿着positionvector反方向行进，最后进入相机。如此，便可以从当前片段处看到被反射的场景。</p><p>SSR就是逆向追踪这样的光线路径的过程，它尝试去寻找光线被反射然后打中当前片段的反射点。在每次迭代中，算法沿着反射光线对场景的位置或深度进行采样，每次都会检查光线是否与场景几何相交。如果相交，那么场景中那个位置就是当前片段反射的潜在候选。</p><p>理想情况下，会有一些分析方法来精确的确定第一个交点，此交点是当前片段中唯一有效的反射点。由于不知道交点在何处（如果存在的话），只能从反射光线的底部开始反向寻找，或者说调用某种算法进行检查。每次调用都会检查是否击中目标，如果击中目标，就可以进一步精确交点位置。</p><p><img src="/images/ScreenSpaceReflection/ray-marching-01.gif" height="400"></p><p>如上图所示，raymarching被用于计算每个片段的反射点。顶点法线是绿色箭头，位置向量是蓝色箭头，在viewspace 中进行的反射ray marching是红色箭头。</p><h3 id="vertex-positions">Vertex Positions</h3><p>与SSAO类似，需要视角空间中的顶点位置。细节请查看<ahref="https://lettier.github.io/3d-game-shaders-for-beginners/ssao.html#vertex-positions">SSAO</a>章节。</p><h3 id="vertex-normals">Vertex Normals</h3><p>计算反射需要视角空间的顶点法线，细节也请查看[SSAO]章节。</p><p><img src="/images/ScreenSpaceReflection/normal-in-SSR.gif" height="400"></p><p>上图中可以看到，使用到的法线并非顶点法线，需要注意。</p><h2 id="position-transformations">Position Transformations</h2><p><img src="/images/ScreenSpaceReflection/position-transformations.gif" height="400"></p><p>正如SSAO，SSR会在视角空间(view space)和屏幕空间(screenspace)中间来回切换，需要相机的投影矩阵将点从视角空间变换到裁剪空间。然后再从裁剪空间变换到UV空间，在此可以从场景中采样顶点或片段位置信息，这个位置是距离屏幕最近的位置。这便是SSR中的屏幕空间部分，因为“屏幕”是一个纹理UV映射在屏幕形状的矩形上。</p><h2 id="reflected-uv-coordinates">Reflected UV Coordinates</h2><p>SSR有多种实现方法，示例代码开始自计算每个屏幕片段的反射UV坐标。当然也可以不这样，而是直接计算场景反射点的最终颜色。</p><p>回想一下，屏幕只是一个映射到矩形上的2D纹理。了解到这一点之后，就会明白示例代码不需要场景的最终渲染结果来计算反射，可以只计算屏幕像素使用到的反射UV坐标。计算出的UV坐标可以存到帧缓存中，然后在后续渲染场景时使用。</p><p><img src="/images/ScreenSpaceReflection/reflected-uv-coordinates.gif" height="400"></p><p>如上图所示，可以看到即使还没有渲染场景，通过反射UV坐标就可以感受到场景最终渲染出的模样。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">uniform mat4 lensProjection;</span><br><span class="line"></span><br><span class="line">uniform sampler2D positionTexture;</span><br><span class="line">uniform sampler2D normalTexture;</span><br></pre></td></tr></table></figure><p>现在需要的是相机的投影矩阵，和视角空间中插值后的顶点位置、法线。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> maxDistance = <span class="number">15</span>;</span><br><span class="line"><span class="type">float</span> resolution = <span class="number">0.3</span>;</span><br><span class="line"><span class="type">int</span> steps = <span class="number">10</span>;</span><br><span class="line"><span class="type">float</span> thickness = <span class="number">0.5</span>;</span><br></pre></td></tr></table></figure><p>和其他效果一样，SSR有一些需要调整的参数，根据场景的复杂度找到合适的参数值。当反射复杂的几何形状时，调出来一个自然的SSR还是有些困难的。代码中的参数具有以下含义：</p><ul><li>maxDistance：片段反射的最远距离；</li><li>resolution：决定了在第一个通道进行光线步进时跳过片元的多少，第一个通道的目标是找到位于场景几何内部或后方的点。需要了解当其值为0时，没有反射效果；当其值为1时，会逐片元的进行光线步进。resolution值是1，且有一个较大的maxDistance时，FPS会降低。</li><li>steps：决定了第二个通道进行迭代的次数，第二个通道目标是明确反射光线和场景几何的精确交点。</li><li>thickness：控制反射命中与不命中的界限。理想情况下，希望光线遇到被相机捕获的位置和深度（就是相机看到的表面）时可以立即停止步进，此点就是光线的精确反射点。但是，现实不可能做到这么精确，所以使用thickness来调整这个界限，值越小，反射点就越精准。</li></ul><p>当thickness增大时，就会出现很多噪点，如下两幅对比图：</p><p><img src="/images/ScreenSpaceReflection/SSR-thickness-01.png" height="400"></p><p><img src="/images/ScreenSpaceReflection/SSR-thickness-02.png" height="400"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vec2 texSize = textureSize(positionTexture, <span class="number">0</span>).xy;</span><br><span class="line">vec2 texCoord = gl_FragCoord.xy / texSize;</span><br><span class="line"></span><br><span class="line">vec4 positionFrom = texture(positionTexture, texCoord);</span><br><span class="line">vec3 unitPositionFrom = normalize(positionFrom.xyz);</span><br><span class="line">vec3 normal = normalize(texture(normalTexture, texCoord).xyz);</span><br><span class="line">vec3 pivot = normalize(reflect(unitPositionFrom, normal))</span><br></pre></td></tr></table></figure><p>需要拿到的是当前片元的位置、法线以及反射后的向量。代码中变量含义如下：</p><ul><li>positionFrom: a vector from the camera position to the currentfragment position;</li><li>normal: a vector pointing in the direction of the interpolatedvertex normal for current fragment;</li><li>pivot: reflection ray or vector pointing in the reflected directionof the positionFrom vector;</li></ul><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">vec4 startView = vec4(positionFrom.xyz + (pivot * <span class="number">0</span>), <span class="number">1</span>);</span><br><span class="line">vec4 endView = vec4(positionFrom.xyz + (pivot * maxDistance), <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">vec4 startFrag = startView;</span><br><span class="line">    startFrag = lensProjection * startFrag;</span><br><span class="line">    startFrag.xyz /= startFrag.w;</span><br><span class="line">    startFrag.xy = startFrag.xy * <span class="number">0.5</span> + <span class="number">0.5</span>;</span><br><span class="line">    startFrag.xy *= texSize;</span><br><span class="line"></span><br><span class="line">vec4 endFrag = endView;</span><br><span class="line">    endFrag      = lensProjection * endFrag;</span><br><span class="line">    endFrag.xyz /= endFrag.w;</span><br><span class="line">    endFrag.xy   = endFrag.xy * <span class="number">0.5</span> + <span class="number">0.5</span>;</span><br><span class="line">    endFrag.xy  *= texSize;</span><br></pre></td></tr></table></figure><p>将这些像素的起点和终点从视角空间变换到屏幕空间，这些点就是片段位置对应的屏幕像素位置了。现在知道了光线在屏幕上的起点和终点，就可以沿着它的方向在屏幕空间进行步进。可以将其想象成在屏幕上画一条线，沿着这条线采样存储在帧缓存中的片元顶点位置。</p><p><img src="/images/ScreenSpaceReflection/from-view-to-screen.png"></p><p>需要注意的是，虽然可以在视角空间沿着这个线进行采样，但是可能会存在对帧缓存的过采样或者缺失对某些点的采样。回想一下，位置帧缓存纹理的大小和形状与屏幕相同。每个屏幕片段或像素对应于相机捕获的某个位置，反射光线在视角空间中行进很长的一段距离可能对应的只在屏幕空间行进了几个像素。所以，在视角空间行进是低效的，可能会采样相同的像素多次。通过在屏幕空间中行进，可以更有效的对光线覆盖的片元或像素进行采样。</p><p><img src="/images/ScreenSpaceReflection/from-view-to-screen.png" ></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">vec2 frag = startFrag.xy;</span><br><span class="line">    uv.xy = frag/texSize;</span><br><span class="line"></span><br><span class="line"><span class="type">float</span> deltaX = endFrag.x - startFrag.x;</span><br><span class="line"><span class="type">float</span> deltaY = endFrag.y - startFrag.y;</span><br><span class="line"></span><br><span class="line"><span class="type">float</span> useX = <span class="built_in">abs</span>(deltaX) &gt;= <span class="built_in">abs</span>(deltaY) ? <span class="number">1</span> : <span class="number">0</span>;</span><br><span class="line"><span class="type">float</span> delta = mix(<span class="built_in">abs</span>(deltaY), <span class="built_in">abs</span>(deltaX), useX) * clamp(resolution, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">vec2 increment = vec2(deltaX, deltaY) / max(delta, <span class="number">0.001</span>);</span><br></pre></td></tr></table></figure><p>第一个通道从反射光线的起始片段位置开始，并分别计算X和Y方向上的起始和终点UV坐标差值。由于反射光线可能更接近水平或竖直中的某一方向，所以需要使用差值较大的轴来确定步进幅度。再结合前文设定的<code>resolution</code>，就可以确定步进的长度<code>increment</code>。可以用实数代入以下看看：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">startFrag  = ( <span class="number">1</span>,  <span class="number">4</span>)</span><br><span class="line">endFrag    = (<span class="number">10</span>, <span class="number">14</span>)</span><br><span class="line"></span><br><span class="line">deltaX     = (<span class="number">10</span> - <span class="number">1</span>) = <span class="number">9</span></span><br><span class="line">deltaY     = (<span class="number">14</span> - <span class="number">4</span>) = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">resolution = <span class="number">0.5</span></span><br><span class="line"></span><br><span class="line">delta      = <span class="number">10</span> * <span class="number">0.5</span> = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">increment  = (deltaX, deltaY) / delta</span><br><span class="line">           = (     <span class="number">9</span>,     <span class="number">10</span>) / <span class="number">5</span></span><br><span class="line">           = ( <span class="number">9</span> / <span class="number">5</span>,      <span class="number">2</span>)</span><br></pre></td></tr></table></figure><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> search0 = <span class="number">0</span>;</span><br><span class="line"><span class="type">float</span> search1 = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">current position x = (start x) * (<span class="number">1</span> - search1) + (end x) * search1;</span><br><span class="line">current position y = (start y) * (<span class="number">1</span> - search1) + (end y) * search1;</span><br></pre></td></tr></table></figure><p>然后，我们继续。从起始片元步进到结束片元，使用线性插值。<code>search1</code>从0到1，当值为0时，当前位置是起始片元；当值为1时，当前位置为结束片元。<code>search0</code>用于记录上一个未与场景几何相交的位置，会在第二个通道用来精确定位交点。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">int</span> hit0 = <span class="number">0</span>;</span><br><span class="line"><span class="type">int</span> hit1 = <span class="number">0</span>;</span><br></pre></td></tr></table></figure><p><code>hit0</code>用来表明第一个通道是否有交点；<code>hit1</code>用来表明第二个通道是否有交点。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> viewDistance = startView.y;</span><br><span class="line"><span class="type">float</span> depth = thickness;</span><br></pre></td></tr></table></figure><p><code>viewDistance</code>用于表示在视角空间内光线行进到的当前位置距离相机有多远。需要注意的是此处代码<code>.y</code>只适用于Panda3D，其他引擎有自己的定义。如果使用的是深度缓存，那么<code>viewDistance</code>就是z深度值。<code>depth</code>值表示的是视角空间中光线行进到的当前点与场景点之间距离相机的距离差距，它表明了点在场景前面或后面多远处。</p><h2 id="first-pass">First pass</h2><p>终于，可以开始我们的第一个pass了。它运行在基于<code>delta</code>的循环中，循环结束代表着在线段上的行进完成了。</p><p><img src="/images/ScreenSpaceReflection/from-view-to-screen.png"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; <span class="type">int</span>(delta); ++i)</span><br><span class="line">&#123;</span><br><span class="line">    frag += increment;</span><br><span class="line">    uv.xy = frag / texSize;</span><br><span class="line">    positionTo = texture(positionTexture, uv.xy);</span><br><span class="line"></span><br><span class="line">    search1 = mix((frag.y - startFrag.y)/deltaY, (frag.x - startFrag.x)/deltaX, useX);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Incorrect</span></span><br><span class="line">    <span class="comment">// viewDistance = mix(startView.y, endView.y, search1);</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Correct</span></span><br><span class="line">    viewDistance = (startView * endView.y) / mix(endView.y, startView.y, search1);</span><br><span class="line"></span><br><span class="line">    depth = viewDistance - positionTo.y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(depth &gt; <span class="number">0</span> &amp;&amp; depth &lt; thickness)&#123;</span><br><span class="line">        hit0 = <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        search0 = search1;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>在循环内进行步进，更新片段位置，然后使用此位置来采样帧缓存中的位置纹理得到此时的场景位置。然后计算当前行进的百分比<code>search1</code>，我们需要此值来在视角空间内进行插值。</p><p>但在进行插值之间我们需要了解，<code>search1</code>是投影后的插值因子，不可以用于直接对投影前的内容进行线性插值，需要使用到<strong>透视正确的插值（perspecive-correctinterpolation）</strong>。具体原因请看此文章<ahref="https://www.comp.nus.edu.sg/~lowkl/publications/lowk_persp_interp_techrep.pdf">Perspective-CorrectInterpolation</a>。</p><p>了解之后便可以使用<code>search1</code>插值得到<code>viewDistance</code>，其表示当前点在视角空间内相对相机的距离，<code>depth</code>便可以顺便求出。</p><p>在得到<code>depth</code>之后，就来到了检查节点。如果<code>depth</code>位于0和<code>thickness</code>之间，表示此时发生了hit，需要把<code>hit0</code>值为1然后离开第一个通道；如果没有，表示此时发生了miss，需要把<code>search0</code>置为<code>search1</code>，然后继续循环。</p><p>至此，完成了第一个pass。将<code>search1</code>设置为最后一次miss和最后一次hit的中间位置；如果没有发生hit，此时search1就是无意义的。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">search1 = search0 + ((search1 - search0)/<span class="number">2</span>);</span><br></pre></td></tr></table></figure><h2 id="second-pass">Second pass</h2><p>随后，开始第二个pass。如果在前一个pass中没有发生hit，直接跳过第二pass。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">steps *= hit0;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; steps; ++i)&#123;</span><br><span class="line">    frag = mix(startFrag.xy, endFrag.xy, search1);</span><br><span class="line">    uv.xy = frag/texSize;</span><br><span class="line">    positionTo = texture(positionTexture, uv.xy);</span><br><span class="line"></span><br><span class="line">    viewDistance = (startView.y * endView.y) / mix(endView.y, startView.y, search1);</span><br><span class="line">    depth = viewDistance - positionTo.y;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(depth &gt; <span class="number">0</span> &amp;&amp; depth &lt; thickness)&#123;</span><br><span class="line">        hit1 = <span class="number">1</span>;</span><br><span class="line">        search1 = search0 + (search1 - search0) / <span class="number">2</span>;</span><br><span class="line">    &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">        <span class="type">float</span> temp = search1;</span><br><span class="line">        search1 = search1 + (search1 - search0) / <span class="number">2</span>;</span><br><span class="line">        search0 = temp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>和之前的pass思路类似，只不过这次通过二分法做了<code>steps</code>次来找寻两者之间更精确的解。</p><h2 id="visibility">Visibility</h2><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> visibility = hit1 * positionTo.w;</span><br></pre></td></tr></table></figure><p>在输出反射UV坐标之前，还需要进行可视性检查确定<code>visibility</code>。如果在secondpass中没有发生hit，<code>visibility</code>的值就是0；如果被反射场景位置处的alpha或<code>w</code>值为0，<code>visibility</code>也应当是0；需要注意，如果<code>w</code>是0，代表在此处没有场景位置信息。</p><p><img src="/images/ScreenSpaceReflection/ray-marching-02.gif" height="400"></p><p>有一种情况SSR会失效，就是当反射光线大致方向指向相机时。如果指向相机的反射光线击中某物体表面，那么大概率那个交点是相机看不到的背面。为了处理这种失效情况，需要根据反射向量指向相机的程度来淡出反射。如果反射向量直接指向与位置矢量相反的方向，则可见性降为0，任何其他方向能见度大于0。记得在做点乘运算时将矢量归一化。那么，上述代码就变成了这样：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> visibility = hit1 * positionTo.w</span><br><span class="line">                        * (<span class="number">1</span> - max(dot(-unitPositionFrom, pivot), <span class="number">0</span>));</span><br></pre></td></tr></table></figure><p>沿着反射光线步进采样场景位置时，希望找到反射光线第一次与场景几何相交的确切点，但很遗憾是做不到的。但是，可以通过“交点越远，反射越淡”的规则来弥补一下。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> visibility = hit1 * positionTo.w</span><br><span class="line">                        * (<span class="number">1</span> - max(dot(-unitPositionFrom, pivot), <span class="number">0</span>))</span><br><span class="line">                        * (<span class="number">1</span> - clamp(depth / thickness, <span class="number">0</span>, <span class="number">1</span>));</span><br></pre></td></tr></table></figure><p>此时反射会在<code>maxDistance</code>处突然消失，为了解决此问题，可以根据反射点离初始起点的距离逐渐淡出。于是：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> visibility = hit1 * positionTo.w</span><br><span class="line">                        * (<span class="number">1</span> - max(dot(-unitPositionFrom, pivot), <span class="number">0</span>))</span><br><span class="line">                        * (<span class="number">1</span> - clamp(depth / thickness, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">                        * (<span class="number">1</span> - clamp(length(positionTo - positionFrom)/ maxDistance, <span class="number">0</span>, <span class="number">1</span>));</span><br></pre></td></tr></table></figure><p>还有一种极限情况是，反射UV坐标超出范围，发生在光线步进时离开相机视锥体的情况下。此时也要将<code>visibility</code>置为0：</p><p><img src="/images/ScreenSpaceReflection/ray-marching-03.gif" height="400"></p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> visibility = hit1 * positionTo.w</span><br><span class="line">                        * (<span class="number">1</span> - max(dot(-unitPositionFrom, pivot), <span class="number">0</span>))</span><br><span class="line">                        * (<span class="number">1</span> - clamp(depth / thickness, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">                        * (<span class="number">1</span> - clamp(length(positionTo - positionFrom)/ maxDistance, <span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">                        * (uv.x &lt; <span class="number">0</span> || uv.x &gt; <span class="number">1</span> ? <span class="number">0</span> : <span class="number">1</span>)</span><br><span class="line">                        * (uv.y &lt; <span class="number">0</span> || uv.y &gt; <span class="number">1</span> ? <span class="number">0</span> : <span class="number">1</span>);</span><br></pre></td></tr></table></figure><p>最后将<code>visibility</code>存储在BA通道中，并输出:</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">visibility = clamp(visibility, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">uv.ba = vec2(visibility);</span><br><span class="line"></span><br><span class="line">fragColor = uv;</span><br></pre></td></tr></table></figure><h2 id="specular-map">Specular Map</h2><p><img src="/images/ScreenSpaceReflection/specular-map.gif" height="400"></p><p>除了反射UV坐标之外，还需要一张高光贴图。示例代码展示了通过片段的材质高光属性创建的高光贴图：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">define</span> MAX_SHININESS 127.75</span></span><br><span class="line"></span><br><span class="line">uniform <span class="class"><span class="keyword">struct</span> &#123;</span></span><br><span class="line">    vec3 specular;</span><br><span class="line">    <span class="type">float</span> shininess;</span><br><span class="line">&#125; p3d_Material;</span><br><span class="line"></span><br><span class="line">out vec4 fragColor;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    fragColor = vec4(p3d_Material.specular, clamp(p3d_Material.shininess / MAX_SHININESS, <span class="number">0</span>, <span class="number">1</span>));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>上述高光片段着色器非常简单，根据片段的材质，它可以将高光信息输出为颜色，将shininess输出到alpha通道。在Blender中，最大的镜面硬度或亮度是511。当从Blender导出到Panda3D时，511导出为127.75。可以在[0,1]范围内随意调整shininess值，让它满足你的实际需求即可。</p><p>示例代码是从材质的高光属性生成的高光贴图，也可以通过其他方法。例如，可以在GIMP中创建一个，然后将其作为纹理附加到3D模型上。比如，宝箱上只有闪亮的支架会反射环境，就可以将支架土城灰色，宝箱其余部分涂成黑色。（笔者也还不懂什么是GIMP，应该是一个DCC软件吧。）</p><h2 id="scene-colors">Scene Colors</h2><p><img src="/images/ScreenSpaceReflection/scene-color.png" height="400"></p><p>想要被反射的场景需要渲染，并将其存储在framebuffer纹理中。上图就是一个普通的，没有任何反射的场景。</p><h2 id="reflected-scene-colors">Reflected Scene Colors</h2><p><img src="/images/ScreenSpaceReflection/reflected-scene-colors.gif" height="400"></p><p>上图就是存储于帧缓存纹理中的反射颜色。而且，反射UV坐标也知道，绘制反射便很简单。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">uniform sampler2D unTexture;</span><br><span class="line">uniform sampler2D colorTexture;</span><br><span class="line"></span><br><span class="line">vec2 texSize = textureSize(uvTexture, <span class="number">0</span>).xy;</span><br><span class="line">vec2 texCoord = gl_FragCoord.xy / texSize;</span><br><span class="line"></span><br><span class="line">vec4 uv = texture(uvTexture, texCoord);</span><br><span class="line">vec4 color = texture(colorTexture, uv.xy);</span><br><span class="line"></span><br><span class="line"><span class="type">float</span> alpha = clamp(uv.b, <span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">fragColor = vec4(mix(vec3(<span class="number">0</span>), color.rgb, alpha), alpha);</span><br></pre></td></tr></table></figure><h2 id="blurred-reflected-scene-colors">Blurred Reflected SceneColors</h2><p><img src="/images/ScreenSpaceReflection/blurred-reflected-scene-colors.png" height="400"></p><p>也可以对反射的场景颜色进行模糊，然后存储在framebuffer的纹理中。有关模糊的详细信息请参考SSAO章节。经过模糊的反射可以用于较为粗糙的表面，这些表面更倾向于漫反射或模糊的镜面反射。</p><h2 id="reflections">Reflections</h2><p>完成最终的反射，需要之前得到的纹理，the reflected colors, the blurredreflected colors, the specular map。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">uniform sampler2D colorTexture;</span><br><span class="line">uniform sampler2D colorBlurTexture;</span><br><span class="line">uniform sampler2D specularTexture;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">vec4 specular = texture(specularTexture, texCoord);</span><br><span class="line">vec4 color = texture(colorTexture, texCoord);</span><br><span class="line">vec4 colorBlur = texture(colorBlurTexture, texCoord);</span><br></pre></td></tr></table></figure><p>将高光值从rgb映射到灰度值，如果得到的高光值无效就抛弃当前片段。然后可以通过得到的高光值控制反射的强度，这样可以通过调整材质的高光来调整反射强度。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> specularAmount = dot(specular.rgb, vec3(<span class="number">1</span>)) / <span class="number">3</span>;</span><br><span class="line"><span class="keyword">if</span>(specularAmount &lt;= <span class="number">0</span>) &#123;fragColor = vec4(<span class="number">0</span>); <span class="keyword">return</span>;&#125;</span><br></pre></td></tr></table></figure><p>通过<code>roughness</code>值来控制反射贴图的模糊程度，值越大越粗糙。前文中，将<code>shininess</code>值存储到了贴图的alpha通道，使用它就可以得到<code>roughness</code>。然后，在一般反射和模糊过的反射之间插值即可。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> roughness = <span class="number">1</span> - min(specular.a, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">fragColor = mix(color, colorBlur, roughness) * specularAmount;</span><br></pre></td></tr></table></figure><h2 id="源文件">源文件</h2><p>请查阅原文章</p><h2 id="参考资料">参考资料</h2><p><ahref="https://lettier.github.io/3d-game-shaders-for-beginners/screen-space-reflection.html">ScreenSpace Reflection (SSR)</a></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SSR </tag>
            
            <tag> Screen Space Reflection </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnOpenGL - SSAO</title>
      <link href="/2024/11/17/LearnOpenGL/SSAO/"/>
      <url>/2024/11/17/LearnOpenGL/SSAO/</url>
      
        <content type="html"><![CDATA[<h3 id="技术发展及原理">技术发展及原理</h3><p>我们曾在基础光照章节简单接触过环境光(ambientlighting)的话题，环境光照是我们加入场景总体光照的一个固定光照常量，被用来模拟光的<strong>散射(Scattering)</strong>。在现实中，光会以不同的强度向四面八方散射，所以场景的间接光照部分在空间上也应该有不同的强度分布。有一种间接光照的模拟技术被称为<strong>环境光遮罩(ambientocclusion)</strong>，它通过降低褶皱、孔洞和相互之间非常靠近的墙面的亮度来尝试模拟间接光照。这些位置很大程度会被周围的几何体遮挡，从而有很少的光线逸出，所以会显得暗。</p><p>下面是一组有无环境光遮罩的对比图，注意观察两者光照的明暗部分：</p><p><img src="/images/LearnOpenGL/SSAO/ssao_example.png"></p><p>尽管不是一个令人难以置信的效果，但是使用了环境遮罩的图片感觉更加真实一些，它会给整体环境一个深度感。</p><p>环境光遮蔽技术在计算量上是昂贵的，他需要将周围的几何体都考虑在内。我们可以使空间中一点发出大量的光线来确定其遮罩程度，但在实时渲染中这显然是个难题。在2007年，Crytek公司公布了<strong>屏幕空间环境光遮蔽技术(screen-spaceambient occlusion,SSAO)</strong>，并将技术应用于其作品孤岛危机上。此技术使用场景的屏幕空间深度缓存，代替真正的几何体数据，去决定物体遮罩程度。与实际意义上的环境遮罩相比，这种方法速度非常快，并且结果是合理的，使其成为模拟实时环境光遮罩的标准。</p><p>SSAO技术背后的技术原理非常简单：对于铺屏四边形(Screen-filledQuad)的每一个片段(fragment)，我们都会基于片段周围的深度值计算一个<strong>遮罩因子(occlusionfactor)</strong>。之后，遮罩因子会被用于降低或者抵消此片段的环境光分量。这个遮罩因子是通过使用球型采样核心(spheresamplekernel)，采样片段周围的多个深度值并与当前片段深度值进行比较得到的。高于片段深度值的样本数量，决定了遮罩因子。采样示意图如下：</p><p><img src="/images/LearnOpenGL/SSAO/ssao_crysis_circle.png"></p><p>图中，几何体内灰色的样本深度都是高于当前片段深度值的，会增大遮蔽因子；几何体内的样本数量越多，片段获得的环境光照也就越少。</p><p>显而易见，此技术渲染的质量和精度依赖于我们在周围采样的样本数量。如果样本数量过低，渲染精度会急剧下降，会出现一个被称为<strong>条带(banding)</strong>的走样；如果样本数量过高，会丢失性能。我们可以将随机性引入到采样核心当中，依此来降低样本数量。对于每个片段随机旋转采样核心，我们可以通过较低的样本数量得到一个较高质量的结果。但是这样做也是有代价的，因为随机性会引入一个明显的<strong>噪声模式(noisepattern)</strong>，必须通过模糊结果来进行修复。下面是一组图例（JohnChapman的佛像），展示了条带走样和噪声：</p><p><img src="/images/LearnOpenGL/SSAO/ssao_banding_noise.jpg"></p><p>如你所见，尽管少量的采样会造成SSAO产生banding走样，但是随后引入的随机性会完全消除走样。</p><p>Crytek公司开发的SSAO技术拥有一个特定的视觉风格。其采样核心为球形，对于平整的墙面，核心的采样有一半位于几何体内，造成墙面看看去灰蒙蒙的。下图展示了孤岛危机采用但是SSAO，其清晰展示了这种灰蒙蒙的感觉：</p><p><img src="/images/LearnOpenGL/SSAO/ssao_crysis.jpg"></p><p>因此，我们不会使用球形采样核心，而是使用表面法线位于的半球面采样核心(hemispheresample kernel)。如下图：</p><p><img src="/images/LearnOpenGL/SSAO/ssao_hemisphere.png"></p><p>通过在<strong>法向半球体(normal-orientedhemisphere)</strong>周围采样，不考虑几何体内部，这样便会消除灰蒙蒙的感觉，从而产生更真实的效果。这个SSAO的教程将会基于法向半球体和JohnChapman出色的<ahref="http://john-chapman-graphics.blogspot.nl/2013/01/ssao-tutorial.html">SSAO教程</a>。</p><h3 id="sample-buffers">Sample buffers</h3><p>因为SSAO需要一些方法去决定片段的遮罩因子，所以需要物体的几何信息。对于每个片段，我们需要以下数据：</p><ul><li>A per-fragment <strong>position</strong> vector.</li><li>A per-fragment <strong>normal</strong> vector.</li><li>A per-fragment <strong>albedo</strong> color.</li><li>A <strong>sample kernel</strong>.</li><li>A per-fragment <strong>random rotation</strong> vector used torotate the sample kernel.</li></ul><p>使用一个逐片段的视角空间位置坐标，我们可以围绕片段的视角空间表面法线旋转半球体采样核心，并使用此核心以不同偏移去采样位置缓冲贴图。对于每个样本，我们将其深度与当前片段位置贴图中存储的深度进行比较，来确定遮挡的数量。最后得到的遮罩因子，被用于限制最终的环境光分量。通过加入逐片段的旋转矢量，我们可以显著减少需要的样本数量，这部分在之后可以很快看到。</p><p><img src="/images/LearnOpenGL/SSAO/ssao_overview.png"></p><blockquote><p>In this chapter we're going to implement SSAO on top of a slightlysimplified version of the deferred renderer from the deferred shadingchapter. If you're not sure what deferred shading is, be sure to firstread up on that.</p></blockquote><p>因为SSAO是一个屏幕空间技术，遮罩是从可视视角计算的，所以可以在视角空间实现此算法。因此，顶点着色器提供的<code>FragPos</code>和<code>Normal</code>都被转换到了视角空间（通过视角矩阵）。</p><blockquote><p>从深度值重建位置向量也是可能的，可以使用Matt Pettineo在它的<ahref="https://mynameismjp.wordpress.com/2010/09/05/position-from-depth-3/">博客</a>中描述的方法。这样的话就需要在着色器中做一些额外计算，但是可以节省G-buffer中存储位置数据的空间。本文只制作了更简单的实例，并未使用此方法。</p></blockquote><p><code>gPosition</code>颜色缓冲贴图的设置如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">glGenTextures(<span class="number">1</span>, &amp;gPosition);</span><br><span class="line">glBindTexture(GL_TEXTURE_2D, gPosition);</span><br><span class="line">glTexImage2D(GL_TEXTURE_2D, <span class="number">0</span>, GL_RGBA16F, SCR_WIDTH, SCR_HEIGHT, <span class="number">0</span>, GL_RGBA, GL_FLOAT, <span class="literal">NULL</span>);</span><br><span class="line">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);</span><br><span class="line">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);</span><br><span class="line">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);</span><br><span class="line">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);</span><br></pre></td></tr></table></figure><p>这给我们了一个线性深度纹理，我们可以用它来对每一个核心样本获取深度值。注意我们把线性深度值存储为了浮点数据；这样从0.1到50.0范围深度值都不会被限制在[0.0,1.0]之间了。如果你不用浮点值存储这些深度数据，确保你首先将值除以FAR来标准化它们，再存储到gPositionDepth纹理中，并在以后的着色器中用相似的方法重建它们。同样需要注意的是GL_CLAMP_TO_EDGE的纹理封装方法。这保证了我们不会不小心采样到在屏幕空间中纹理默认坐标区域之外的深度值。</p><p>接下来我们需要真正的半球采样核心和一些方法来随机旋转它。</p><h3 id="normal-oriented-hemisphere">Normal-oriented hemisphere</h3><p>我们需要去生成一系列围绕表面法线的采样点，正如之前讨论的那样，这些采样点在半球内。在每个表面法线方向生成采样核心显然是困难和不合理的，因此我们在<strong>切线空间(tangentspace)</strong>上生成采样核心，使用法线作为正z轴。如下图：</p><p><img src="/images/LearnOpenGL/SSAO/ssao_hemisphere.png" width="400"></p><p>想象以下我们拥有一个单位半球，我们可以通过以下方式获得一个最大64采样点的采样核心：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::uniform_real_distribution&lt;<span class="type">float</span>&gt; <span class="title function_">randomFloats</span><span class="params">(<span class="number">0.0</span>, <span class="number">1.0</span>)</span>; <span class="comment">// random floats between [0.0, 1.0]</span></span><br><span class="line"><span class="built_in">std</span>::default_random_engine generator;</span><br><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;glm::vec3&gt; ssaoKernel;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">64</span>; ++i)</span><br><span class="line">&#123;</span><br><span class="line">    glm::vec3 <span class="title function_">sample</span><span class="params">(</span></span><br><span class="line"><span class="params">        randomFloats(generator) * <span class="number">2.0</span> - <span class="number">1.0</span>,</span></span><br><span class="line"><span class="params">        randomFloats(generator) * <span class="number">2.0</span> - <span class="number">1.0</span>,</span></span><br><span class="line"><span class="params">        randomFloats(generator)</span></span><br><span class="line"><span class="params">    )</span>;</span><br><span class="line">    sample = glm::normalize(sample);</span><br><span class="line">    sample *= randomFloats(generator);</span><br><span class="line">    ssaoKernel.push_back(sample);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现在，采样核心生成的所有采样点都是随机分布的，但是我们想让接近实际片段的采样点拥有更大的权重。可以使用一个加速插值函数(acceleratinginterpolation)来做到：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> scale = (<span class="type">float</span>)i / <span class="number">64.0</span>;</span><br><span class="line">scale = lerp(<span class="number">0.1f</span>, <span class="number">1.0f</span>, scale * scale);</span><br><span class="line">sample *= scale;</span><br><span class="line">ssaoKernel.push_back(sample);</span><br></pre></td></tr></table></figure><p><img src="/images/LearnOpenGL/SSAO/ssao_kernel_weight.png"></p><p>每一个采样点都被用于偏移视角空间的像素位置，然后采样。如前文所说，我们确实需要大量的采样点来得到真实的结果，但是这样会造成性能问题。但是，如果我们逐片元的引入半随机的旋转或噪音，就可以显著降低需要的采样数量。</p><h3 id="random-kernel-rotations">Random kernel rotations</h3><p>我们可以为场景中的每个片段生成一个随机旋转向量，但是那样会占用大量存储空间。所以，更好的方法是创建一个小的随机旋转向量纹理平铺在屏幕上。我们创建一个4x4朝向切线空间表面法线的随机旋转向量数组：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">std</span>::<span class="built_in">vector</span>&lt;glm::vec3&gt; ssaoNoise;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">16</span>; i++)</span><br><span class="line">&#123;</span><br><span class="line">    glm::vec3 <span class="title function_">noise</span><span class="params">(</span></span><br><span class="line"><span class="params">        randomFloats(generator) * <span class="number">2.0</span> - <span class="number">1.0</span>,</span></span><br><span class="line"><span class="params">        randomFloats(generator) * <span class="number">2.0</span> - <span class="number">1.0</span>,</span></span><br><span class="line"><span class="params">        <span class="number">0.0f</span>)</span>;</span><br><span class="line">    ssaoNoise.push_back(noise);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>由于采样核心是沿着正z方向在切线空间内旋转，我们设定z分量为0.0。接下来创建一个包含随机旋转向量的4x4纹理；记得设定它的封装方式为<code>GL_REPEAT</code>，从而保证它合适地平铺在屏幕上。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> noiseTexture;</span><br><span class="line">glGenTextures(<span class="number">1</span>, &amp;noiseTexture);</span><br><span class="line">glBindTexture(GL_TEXTURE_2D, noiseTexture);</span><br><span class="line">glTexImage2D(GL_TEXTURE_2D, <span class="number">0</span>, GL_RGBA16F, <span class="number">4</span>, <span class="number">4</span>, <span class="number">0</span>, GL_RGB, GL_FLOAT, &amp;ssaoNoise[<span class="number">0</span>]);</span><br><span class="line">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);</span><br><span class="line">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);</span><br><span class="line">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_S, GL_REPEAT);</span><br><span class="line">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_WRAP_T, GL_REPEAT);</span><br></pre></td></tr></table></figure><h3 id="the-ssao-shader">The SSAO shader</h3><p>SSAO着色器在一个2D铺屏四边形上运行，为每个片段计算遮罩值。因为需要去存储SSAO阶段的结果，所以我们要创建另外一个帧缓存：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> ssaoFBO;</span><br><span class="line">glGenFramebuffers(<span class="number">1</span>, &amp;ssaoFBO);</span><br><span class="line">glBindFramebuffer(GL_FRAMEBUFFER, ssaoFBO);</span><br><span class="line"></span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> ssaoColorBuffer;</span><br><span class="line">glGenTextures(<span class="number">1</span>, &amp;ssaoColorBuffer);</span><br><span class="line">glBindTexture(GL_TEXTURE_2D, ssaoColorBuffer);</span><br><span class="line">glTexImage2D(GL_TEXTURE_2D, <span class="number">0</span>, GL_RED, SCR_WIDTH, SCR_HEIGHT, <span class="number">0</span>, GL_RED, GL_FLOAT, <span class="literal">NULL</span>);</span><br><span class="line">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);</span><br><span class="line">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);</span><br><span class="line"></span><br><span class="line">glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, ssaoColorBuffer, <span class="number">0</span>);</span><br></pre></td></tr></table></figure><p>因为环境光遮罩的结果只是一个灰度值，所以我们只需要贴图的R通道，所以颜色缓冲内部格式被设置为<code>GL_RED</code>。</p><p>完整的渲染SSAO的流程大概如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// geometry pass: render stuff into G-buffer</span></span><br><span class="line">glBindFramebuffer(GL_FRAMEBUFFER, gBuffer);</span><br><span class="line">    [...]</span><br><span class="line">glBindFramebuffer(GL_FRAMEBUFFER, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// use G-buffer to render SSAO texture</span></span><br><span class="line">glBindFramebuffer(GL_FRAMEBUFFER, ssaoFBO);</span><br><span class="line">    glClear(GL_COLOR_BUFFER_BIT);</span><br><span class="line">    glActiveTexture(GL_TEXTURE0);</span><br><span class="line">    glBindTexture(GL_TEXTURE_2D, gPosition);</span><br><span class="line">    glActiveTexture(GL_TEXTURE1);</span><br><span class="line">    glBindTexture(GL_TEXTURE_2D, gNormal);</span><br><span class="line">    glActiveTexture(GL_TEXTURE2);</span><br><span class="line">    glBindTexture(GL_TEXTURE_2D, noiseTexture);</span><br><span class="line">    shaderSSAO.use();</span><br><span class="line">    SendKernelSamplesToShader();</span><br><span class="line">    shaderSSAO.setMat4(<span class="string">&quot;projection&quot;</span>, projection);</span><br><span class="line">    RenderQuad();</span><br><span class="line">glBindFramebuffer(GL_FRAMEBUFFER, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// lighting pass: render scene lighting</span></span><br><span class="line">glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);</span><br><span class="line">shaderLightingPass.use();</span><br><span class="line">[...]</span><br><span class="line">glActiveTexture(GL_TEXTURE3);</span><br><span class="line">glBindTexture(GL_TEXTURE_2D, ssaoColorBuffer);</span><br><span class="line">[...]</span><br><span class="line">RenderQuad();</span><br></pre></td></tr></table></figure><p>shaderSSAO着色器将G-buffer相关的纹理、噪声纹理和法向半球采样核心作为输入：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line">out <span class="type">float</span> FragColor;</span><br><span class="line"></span><br><span class="line">in vec2 TexCoords;</span><br><span class="line"></span><br><span class="line">uniform sampler2D gPosition;</span><br><span class="line">uniform sampler2D gNormal;</span><br><span class="line">uniform sampler2D texNoise;</span><br><span class="line"></span><br><span class="line">uniform vec3 samples[<span class="number">64</span>];</span><br><span class="line">uniform mat4 projection;</span><br><span class="line"></span><br><span class="line"><span class="comment">// tile noise texture over screen, based on screen dimensions divided by noise size</span></span><br><span class="line"><span class="type">const</span> vec2 noiseScale = vec2(<span class="number">800.0</span>/<span class="number">4.0</span>, <span class="number">600.0</span>/<span class="number">4</span>/<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>注意，这里有一个<code>noiseScale</code>变量。我们想要将噪声纹理平铺(tile)在屏幕上，但是由于<code>TexCoords</code>的取值在0.0和1.0之间，<code>texNoise</code>纹理将不会平铺。所以我们将通过屏幕分辨率除以噪声纹理大小的方式计算<code>TexCoords</code>的缩放大小，并在之后提取相关输入向量的时候使用。另外，有了<code>fragPos</code>和<code>normal</code>向量，我们就可以创建TBN矩阵将向量从切线空间转换到视角空间。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">vec3 fragPos = texture(gPosition, TexCoords).xyz;</span><br><span class="line">vec3 normal = texture(gNormal, TexCoords).rgb;</span><br><span class="line">vec3 randomVec = texture(texNoise, TexCoords * noiseScale).xyz;</span><br><span class="line"></span><br><span class="line">vec3 tangent = normalize(randomVec - normal * dot(randomVec, normal));</span><br><span class="line">vec3 bitangent = cross(normal, tangent);</span><br><span class="line">mat3 TBN = mat3(tangent, bitangent, normal);</span><br></pre></td></tr></table></figure><p>使用<strong>Gramm-Schmidtprocess</strong>可以创建一个正交基底，每次都基于randomVec的值进行略微倾斜。需要注意的是，因为我们使用了随机向量来构建tangentvector，所以并不需要使TBN矩阵真正贴合模型表面。因此，也不需要逐顶点的切线（和副切线）向量。</p><p>接下来，我们遍历每个核心采样点，将其变换到视角空间。然后，根据当前片段位置，叠加去采样深度值。这样，就可以比较采样点的视角空间深度值和当前片段的视角空间深度值。我们来逐步讨论这个问题：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> occlusion = <span class="number">0.0</span>;</span><br><span class="line"><span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; kernelSize; ++i)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// get sample position</span></span><br><span class="line">    vec3 samplePos = TBN * samples[i]; <span class="comment">// from tangent to view-space</span></span><br><span class="line">    samplePos = fragPos + samplePos * radius;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>此处的<code>kernelSize</code>和<code>radius</code>是用于微调效果的变量，在案例中使用的值分别为64和0.5。对于每次迭代，我们先将各自的采样点转换到视角空间，然后在视角空间将偏移采样作用于片段位置上，并通过将偏移样本乘以半径来增加（或减少）SSAO的有效采样半径。</p><p>接下来，我们要将sample变换到屏幕空间，这样我们就可以对sample的位置或深度进行采样，就好像我们直接将其位置渲染到屏幕上一样（真没看懂这半句话）。由于采样顶点数据(vector)当前处于视图空间，我们将首先将其变换到NDC，随后将NDC映射到[0.0,1.0]范围对位置贴图进行采样：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vec4 offset = vec4(samplePos, <span class="number">1.0</span>);</span><br><span class="line">offset = projection * offset; <span class="comment">// from view to clip-space</span></span><br><span class="line">offset.xyz /= offset.w; <span class="comment">// perspective divide</span></span><br><span class="line">offset.xyz = offset.xyz * <span class="number">0.5</span> + <span class="number">0.5</span>; <span class="comment">// transform to range 0.0 - 1.0</span></span><br><span class="line"></span><br><span class="line"><span class="type">float</span> sampleDepth = texture(gPosition, offset.xy).z;</span><br></pre></td></tr></table></figure><p>最后便可以对sample的深度值与存储的深度值大小，更新遮罩因子：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">occlusion += (sampleDepth &gt;= samplePos.z + bias ? <span class="number">1.0</span> : <span class="number">0.0</span>);</span><br></pre></td></tr></table></figure><p>我们在这里为原始片段深度添加了一个偏移bias（在本例中被设定为0.025）。虽然偏移bias并不是必要的，但是它有助于微调SSAO的视觉效果，并解决因为场景复杂度而出现的acne走样。</p><p>至此，我们还没有完全完成SSAO，还剩一个小问题。当一个靠近表面边缘的片段计算遮罩因子时，可能会将表面后方很远处的深度值也考虑在内。这些深度值本不应被考虑在内，会对遮罩因子造成影响。为了解决此问题，我们可以引入边缘检测，如下图（由JohnChapman提供）：</p><p><img src="/images/LearnOpenGL/SSAO/ssao_range_check.png"></p><p>引入一个范围检测从而保证只当被检测深度值在取样半径内时影响遮罩因子，故而将代码最后一样换为：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> rangeCheck = smoothstep(<span class="number">0.0</span>, <span class="number">1.0</span>, radius/<span class="built_in">abs</span>(fragPos.z - sampleDepth));</span><br><span class="line">occlusion += (sampleDepth &gt;= samplePos.z + bias ? <span class="number">1.0</span> : <span class="number">0.0</span>) * rangeCheck;</span><br></pre></td></tr></table></figure><p>这里我们使用了 GLSL 的 smoothstep 函数，如果深度差取值在 radius之内，它们的值会光滑地根据下面这个曲线插值在0.0到1.0之间：</p><p><img src="/images/LearnOpenGL/SSAO/ssao_smoothstep.png"></p><p>如果我们使用hard cut-off rangecheck，当深度值在radius之外就移除其对遮罩因子的贡献，就将会在边缘检测使用的地方看见一个明显的（很难看）的边缘。</p><p>最后，需要将遮罩贡献根据核心大小标准化，并输出结果。我们用1.0减去遮罩因子，以便直接使用遮罩因子去缩放环境光照分量。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">occlusion = <span class="number">1.0</span> - (occlusion / kernelSize);</span><br><span class="line">FragColor = occlusion;</span><br></pre></td></tr></table></figure><p>如果我们最喜欢的背包模型正在小睡，ssaoShader会产生以下纹理：</p><p><img src="/images/LearnOpenGL/SSAO/ssao_without_blur.png"></p><p>可见，ssao给了场景很强的深度感。仅通过ssao就可以清除了解，模型是躺在地板上的，而不是漂浮在空中。现在效果仍然看起来不是很完美，因为重复的纹理噪声在图中清晰可见。为了创建一个更光滑的环境光遮蔽效果，我们需要模糊环境遮罩纹理。</p><h3 id="ambient-occlusion-blur">Ambient occlusion blur</h3><p>我们想要在SSAO pass和lightingpass之间模糊SSAO贴图，所以让我们先创建另外一个帧缓存存储模糊结果：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> ssaoBlurFBO, ssaoColorBufferBlur;</span><br><span class="line">glGenFramebuffers(<span class="number">1</span>, &amp;ssaoBlurFBO);</span><br><span class="line">glBindFramebuffer(GL_FRAMEBUFFER, ssaoBlurFBO);</span><br><span class="line">glGenTextures(<span class="number">1</span>, &amp;ssaoColorBufferBlur);</span><br><span class="line">glBindTexture(GL_TEXTURE_2D, ssaoColorBufferBlur);</span><br><span class="line">glTexImage2D(GL_TEXTURE_2D, <span class="number">0</span>, GL_RED, SCR_WIDTH, SCR_HEIGHT, <span class="number">0</span>, GL_RED, GL_FLOAT, <span class="literal">NULL</span>);</span><br><span class="line">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_NEAREST);</span><br><span class="line">glTexParameteri(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_NEAREST);</span><br><span class="line">glFramebufferTexture2D(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, ssaoColorBufferBlur, <span class="number">0</span>);</span><br></pre></td></tr></table></figure><p>因为平铺的随机矢量贴图给了我们一个一致的随机性，我们可以利用这个属性来创建一个简单的模糊着色器：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line">out <span class="type">float</span> FragColor;</span><br><span class="line"></span><br><span class="line">in vec2 TexCoords;</span><br><span class="line"></span><br><span class="line">uniform sampler2D ssaoInput;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span> &#123;</span><br><span class="line">    vec2 texelSize = <span class="number">1.0</span> / vec2(textureSize(ssaoInput, <span class="number">0</span>));</span><br><span class="line">    <span class="type">float</span> result = <span class="number">0.0</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">int</span> x = <span class="number">-2</span>; x &lt; <span class="number">2</span>; ++x) </span><br><span class="line">    &#123;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> y = <span class="number">-2</span>; y &lt; <span class="number">2</span>; ++y) </span><br><span class="line">        &#123;</span><br><span class="line">            vec2 offset = vec2(<span class="type">float</span>(x), <span class="type">float</span>(y)) * texelSize;</span><br><span class="line">            result += texture(ssaoInput, TexCoords + offset).r;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    FragColor = result / (<span class="number">4.0</span> * <span class="number">4.0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里我们遍历了周围在-2.0和2.0之间的SSAO纹理单元(Texel)，采样与噪声纹理维度相同数量的SSAO纹理。我们通过使用返回vec2纹理维度的textureSize函数，根据纹理单元的真实大小偏移了每一个纹理坐标。我们平均所得的结果，获得一个简单但是有效的模糊效果。这就完成了，一个包含逐片段环境遮蔽数据的纹理，在光照处理阶段中可以直接使用。</p><p><img src="/images/LearnOpenGL/SSAO/ssao.png"></p><h3 id="applying-ambient-occlusion">Applying ambient occlusion</h3><p>应用遮蔽因子到光照方程中极其简单：我们要做的只是将逐片段环境遮蔽因子乘到光照的环境分量上。如果我们使用上个教程中的Blinn-Phong延迟光照着色器并做出一点修改，我们将会得到下面这个片段着色器：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line">out vec4 FragColor;</span><br><span class="line">  </span><br><span class="line">in vec2 TexCoords;</span><br><span class="line"></span><br><span class="line">uniform sampler2D gPosition;</span><br><span class="line">uniform sampler2D gNormal;</span><br><span class="line">uniform sampler2D gAlbedo;</span><br><span class="line">uniform sampler2D ssao;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Light</span> &#123;</span></span><br><span class="line">    vec3 Position;</span><br><span class="line">    vec3 Color;</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> Linear;</span><br><span class="line">    <span class="type">float</span> Quadratic;</span><br><span class="line">    <span class="type">float</span> Radius;</span><br><span class="line">&#125;;</span><br><span class="line">uniform Light light;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;             </span><br><span class="line">    <span class="comment">// retrieve data from gbuffer</span></span><br><span class="line">    vec3 FragPos = texture(gPosition, TexCoords).rgb;</span><br><span class="line">    vec3 Normal = texture(gNormal, TexCoords).rgb;</span><br><span class="line">    vec3 Diffuse = texture(gAlbedo, TexCoords).rgb;</span><br><span class="line">    <span class="type">float</span> AmbientOcclusion = texture(ssao, TexCoords).r;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// blinn-phong (in view-space)</span></span><br><span class="line">    vec3 ambient = vec3(<span class="number">0.3</span> * Diffuse * AmbientOcclusion); <span class="comment">// here we add occlusion factor</span></span><br><span class="line">    vec3 lighting  = ambient; </span><br><span class="line">    vec3 viewDir  = normalize(-FragPos); <span class="comment">// viewpos is (0.0.0) in view-space</span></span><br><span class="line">    <span class="comment">// diffuse</span></span><br><span class="line">    vec3 lightDir = normalize(light.Position - FragPos);</span><br><span class="line">    vec3 diffuse = max(dot(Normal, lightDir), <span class="number">0.0</span>) * Diffuse * light.Color;</span><br><span class="line">    <span class="comment">// specular</span></span><br><span class="line">    vec3 halfwayDir = normalize(lightDir + viewDir);  </span><br><span class="line">    <span class="type">float</span> spec = <span class="built_in">pow</span>(max(dot(Normal, halfwayDir), <span class="number">0.0</span>), <span class="number">8.0</span>);</span><br><span class="line">    vec3 specular = light.Color * spec;</span><br><span class="line">    <span class="comment">// attenuation</span></span><br><span class="line">    <span class="type">float</span> dist = length(light.Position - FragPos);</span><br><span class="line">    <span class="type">float</span> attenuation = <span class="number">1.0</span> / (<span class="number">1.0</span> + light.Linear * dist + light.Quadratic * dist * dist);</span><br><span class="line">    diffuse  *= attenuation;</span><br><span class="line">    specular *= attenuation;</span><br><span class="line">    lighting += diffuse + specular;</span><br><span class="line"></span><br><span class="line">    FragColor = vec4(lighting, <span class="number">1.0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>除去使用了视角空间，我们做的事情只有使用环境管遮罩AO乘以场景的环境光分量。在场景中放置一盏蓝光，我们可以得到如下结果：</p><p><img src="/images/LearnOpenGL/SSAO/ssao_final.png"></p><p>屏幕空间环境光遮蔽是一个可高度自定义的效果，它的效果很大程度上依赖于我们根据场景类型调整它的参数。对所有类型的场景并不存在什么完美的参数组合方式。一些场景只在小半径情况下工作，又有些场景会需要更大的半径和更大的样本数量才能看起来更真实。当前这个演示用了64个样本，属于比较多的了，你可以调调更小的核心大小从而获得更好的结果。</p><h3 id="资料链接">资料链接</h3><p><a href="https://learnopengl.com/Advanced-Lighting/SSAO">LearnOpenGL- SSAO</a></p><p><ahref="http://john-chapman-graphics.blogspot.nl/2013/01/ssao-tutorial.html">SSAOTutorial</a></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>URP-PBR源码学习</title>
      <link href="/2024/11/16/Unity/UnityEngine/URP-PBR%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/"/>
      <url>/2024/11/16/Unity/UnityEngine/URP-PBR%E6%BA%90%E7%A0%81%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>PBR绝对是使用最广泛的光照模型，有很多种不同的变体，了解其原理对发展有莫大的帮助。本文学习版本为URP@12.1.10。本文只注重PBR部分内容，其他内容说的比较简单。</p><h2 id="结构">结构</h2><h3 id="数据封装">数据封装</h3><p>Unity将物体表面相关属性的数据封装在BRDFData结构中，片元的属性封装在InputData结构中，让代码整洁一些。</p><h3 id="流程控制">流程控制</h3><p>Unity使用的PBR分为高光流和金属流两部分，使用关键字<code>_SPECULAR_SETUP</code>控制其开关。两者本质原理相同，所以对输入数据进行处理之后，会使用相同的渲染逻辑。</p><p>Unity采用的非电解质基础反射率为0.04，为方便后续使用将(1.0-0.04)值存储于alpha位。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BRDF.hlsl</span></span><br><span class="line"><span class="comment">// standard dielectric reflectivity coef at incident angle (= 4%)</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> kDielectricSpec half4(0.04, 0.04, 0.04, 1.0 - 0.04)</span></span><br></pre></td></tr></table></figure><p>对于两个工作流Unity都会计算出反射率reflectivity，漫反射brdfDiffuse，高光反射brdfSpecular，这部分过程也是两个工作流主要区别的部分。</p><p>高光流中，反射率直接取三分量中的最大值（OpenGLES取r分量，因为大多数金属要么是单色，要么带有微红或淡黄色）。高光颜色直接使用高光贴图中的specular，因为高光流相比与金属流就是使用更多的通道存储高光反射信息，包括非电解质材质。漫反射颜色来自漫反射贴图的albedo，根据能量守恒定律，除去反射的剩余部分即是漫反射，所以乘以oneMinusReflectivity。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">half reflectivity = ReflectivitySpecular(specular);</span><br><span class="line">half oneMinusReflectivity = half(<span class="number">1.0</span>) - reflectivity;</span><br><span class="line">half3 brdfDiffuse = albedo * oneMinusReflectivity;</span><br><span class="line">half3 brdfSpecular = specular;</span><br></pre></td></tr></table></figure><p>金属流中，反射率根据金属度得出，范围为[0.04,1]，Unity选择先计算出oneMinusReflectivity，随后求得反射率。当物体表面为金属时，其高光颜色存储于漫反射贴图中；当物体表面为非金属时，高光颜色为前文提到的预定值，是暗灰色(0.04,0.04,0.04)。所以，金属流虽然贴图通道少，但表现上限低于高光流。漫反射颜色与高光流中的处理相同。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">half <span class="title function_">OneMinusReflectivityMetallic</span><span class="params">(half metallic)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// We&#x27;ll need oneMinusReflectivity, so</span></span><br><span class="line">    <span class="comment">//   1-reflectivity = 1-lerp(dielectricSpec, 1, metallic) = lerp(1-dielectricSpec, 0, metallic)</span></span><br><span class="line">    <span class="comment">// store (1-dielectricSpec) in kDielectricSpec.a, then</span></span><br><span class="line">    <span class="comment">//   1-reflectivity = lerp(alpha, 0, metallic) = alpha + metallic*(0 - alpha) =</span></span><br><span class="line">    <span class="comment">//                  = alpha - metallic * alpha</span></span><br><span class="line">    half oneMinusDielectricSpec = kDielectricSpec.a;</span><br><span class="line">    <span class="keyword">return</span> oneMinusDielectricSpec - metallic * oneMinusDielectricSpec;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// in InitializeBRDFData</span></span><br><span class="line">half oneMinusReflectivity = OneMinusReflectivityMetallic(metallic);</span><br><span class="line">half reflectivity = half(<span class="number">1.0</span>) - oneMinusReflectivity;</span><br><span class="line">half3 brdfDiffuse = albedo * oneMinusReflectivity;</span><br><span class="line">half3 brdfSpecular = lerp(kDieletricSpec.rgb, albedo, metallic);</span><br></pre></td></tr></table></figure><p>现在完成了工作流差异处理，后续便不需要考虑此问题。之后调用<code>InitializedBRDFData()</code>将输入的数据整合，并预计算之后光照计算使用到的几个因子。</p><h3 id="clearcoat-清漆">ClearCoat 清漆</h3><p>ClearCoat材质用于实现基础层+清漆层的双层材质效果，例如车漆、打蜡木地板和镀膜材料等。基础层由通用的PBR算法实现，清漆层由ClearCoatPBR算法实现，最后将两种材质效果混合即可。其实现步骤和通用PBR基本相似：</p><ul><li>ClearCoatBRDF参数准备</li><li>间接光计算</li><li>直接光计算</li></ul><p>在具体实现上还有一些差异，在PBR流程解析完成后会独立出来说一下。</p><h3 id="shadowmask">shadowMask</h3><p>新增了shadowMask属性，用作阴影蒙版，用来剔除不想要的阴影。为了版本兼容，Unity做了以下分支判断：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">half4 <span class="title function_">CalculateShadowMask</span><span class="params">(InputData inputData)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="meta">#<span class="keyword">if</span> defined(SHADOWS_SHADOWMASK) &amp;&amp; defined(LIGHTMAP_ON)</span></span><br><span class="line">    half4 shadowMask = inputData.shadowMask;</span><br><span class="line">    <span class="meta">#<span class="keyword">elif</span> !defined (LIGHTMAP_ON)</span></span><br><span class="line">    half4 shadowMask = unity_ProbesOcclusion;</span><br><span class="line">    <span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    half4 shadowMask = half4(<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>);</span><br><span class="line">    <span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> shadowMask;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="ssao">SSAO</h3><p>Unity封装了要给aoFactor里面包含了间接光ao和直接光ao，间接光ao就是正常的ssao实现。对于直接光ao，可以进行设置，看起来像是为了调试才加进来的。</p><h3 id="混合bakegi和实时阴影">混合BakeGI和实时阴影</h3><p>使用函数<code>MixRealtimeAndBakedGI</code>函数来混和全局光照和阴影，此时还并没有应用AO，AO在后续计算中。</p><h2 id="pbr光照计算">PBR光照计算</h2><p>然后就来到了重要部分，PBR光照。光照由四部分组成，直接光漫反射、直接光高光反射、间接光（GI）漫反射和间接光（GI）高光反射</p><h3 id="间接光源计算">间接光源计算</h3><p>间接光通过函数<code>GlobalIllumination()</code>计算，分为漫反射和高光反射两部分。</p><h4 id="漫反射">漫反射</h4><p>漫反射颜色bakedGI通过采样光照贴图LightMap或球谐函数SH而来，Unity提供了相关宏SAMPLE_GI，其返回值直接用作漫反射颜色。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// PBRForwardPass.hlsl</span></span><br><span class="line"><span class="meta">#<span class="keyword">if</span> defined(DYNAMICLIGHTMAP_ON)</span></span><br><span class="line">    inputData.bakedGI = SAMPLE_GI(input.staticLightmapUV, input.dynamicLightmapUV.xy, input.sh, inputData.normalWS);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    inputData.bakedGI = SAMPLE_GI(input.staticLightmapUV, input.sh, inputData.normalWS);</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"><span class="comment">// GlobalIllumination.hlsl - GlobalIllumination</span></span><br><span class="line">half3 indirctDiffuse = bakeGI;</span><br></pre></td></tr></table></figure><h4 id="高光反射">高光反射</h4><p>高光反射是通过IBL实现的，是标准做法，通过粗糙度roughness取计算要采样CubeMap的MipLevel然后采样对应贴图得到环境光颜色值，然后在根据高光反照率计算得到最终值。</p><p>通过粗糙度得到MipLevel有一套自己的规则，通过函数<code>PerceptualRoughtnessToMipmapLevel()</code>实现。因为CubeMap可能使用了HDR，所以在得到环境光颜色值要考虑到解码。最后再与遮蔽因子occlusion相乘，但实际上在流程中这里函数的occlusion传入的都是1.0，再后面计算出高光反射率之后又做了一次遮光的乘法，所以生效的应该是最后做的那次。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ImageBasedLighting.hlsl</span></span><br><span class="line">real <span class="title function_">PerceptualRoughnessToMipmapLevel</span><span class="params">(real perceptualRoughtness, uint maxMipLevel)</span></span><br><span class="line">&#123;</span><br><span class="line">    perceptualRoughtness = perceptualRoughtness * (<span class="number">1.7</span> - <span class="number">0.7</span> * perceptualRoughtness);</span><br><span class="line">    <span class="keyword">return</span> perceptualRoughtness * maxMipLevel;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// GlobalIllumination.hlsl</span></span><br><span class="line">half3 <span class="title function_">GlossyEnvironmentReflection</span><span class="params">(half3 reflectVector, half perceptualRoughness, half occlusion)</span></span><br><span class="line">&#123;</span><br><span class="line">    ...</span><br><span class="line">    half4 encodedIrradiance = half4(SAMPLE_TEXTURECUBE_LOD(unity_SpecCube0, samplerunity_SpecCube0, reflectVector, mip));</span><br><span class="line">    ...</span><br><span class="line">    <span class="keyword">return</span> irradiance * occlusion;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>高光反照率通过衰减surfaceReducion和由fresnelTerm决定的遮罩一同决定，衰减是一条随粗糙度变化的曲线，遮罩是由菲涅尔值插值高光颜色specular和掠射颜色grazingTerm。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// BRDF.hlsl</span></span><br><span class="line"><span class="comment">// Computes the specular term for EnvironmentBRDF</span></span><br><span class="line">half3 <span class="title function_">EnvironmentBRDFSpecular</span><span class="params">(BRDFData brdfData, half fresnelTerm)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="type">float</span> surfaceReduction = <span class="number">1.0</span> / (brdfData.roughness2 + <span class="number">1.0</span>);</span><br><span class="line">    <span class="keyword">return</span> half3(surfaceReduction * lerp(brdfData.specular, brdfData.grazingTerm, fresnelTerm));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="主光源计算">主光源计算</h3><p>我们继续来看主光源计算，主光光照使用的<code>LightingPhysicallyBased()</code>函数实现。主光作为精确光源，我们按照精确光源的渲染方程走。</p><h4 id="主光漫反射">主光漫反射</h4><p>在BIRP中，主光漫反射使用的是DisneyBRDF中的漫反射光照模型。而在URP中，主光漫反射使用的是LambertBRDF光照模型，效果会稍逊一些，特别是掠射角随着粗糙度会有明暗边的细节丢失。Unity这样做可能是为了节省性能。</p><p><span class="math display">\[BRDF_{Lambert} = \frac{C_{diffuse}}{\PI}\]</span></p><p>代码中直接直接使用了brdfDiffuse，至于分母去哪了，我们后面会提到。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">half3 brdf = brdfData.diffuse;</span><br></pre></td></tr></table></figure><h4 id="主光高光反射">主光高光反射</h4><p>游戏界中主流的高光反射BRDF模型是基于微平面理论的MicrofacetCook-TorranceBRDF。Unity中也是使用了此模型来实现高光反射，在<code>DirectBRDFSpecular</code>函数中实现，反射率被提到了函数外在<code>LightingPhysicallyBased</code>函数中：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">half <span class="title function_">DirectBRDFSpecular</span><span class="params">(BRDFData brdfData, half3 normalWS, half3 lightDirectionWS, half3 viewDirectionWS)</span></span><br><span class="line">&#123;</span><br><span class="line">    float3 lightDirectionWSFloat3 = float3(lightDirectionWS);</span><br><span class="line">    float3 halfDir = SafeNormalize(lightDirectionWSFloat3 + float3(viewDirectionWS));</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> NoH = saturate(dot(float3(normalWS), halfDir));</span><br><span class="line">    half LoH = half(saturate(dot(lightDirectionWSFloat3, halfDir)));</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> d = NoH * NoH * brdfData.roughness2MinusOne + <span class="number">1.00001f</span>;</span><br><span class="line"></span><br><span class="line">    half LoH2 = LoH * LoH;</span><br><span class="line">    half specularTerm = brdfData.roughness2 / ((d*d) * max(<span class="number">0.1</span>h, LoH2) * brdfData.normalizationTerm);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">if</span> defined (SHADER_API_MOBILE) || defined (SHADER_API_SWITCH)</span></span><br><span class="line">    specularTerm = specularTerm - HALF_MIN;</span><br><span class="line">    specularTerm = clamp(specularTerm, <span class="number">0.0</span>, <span class="number">100.0</span>);</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> specularTerm;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// LightingPhysicallyBased()</span></span><br><span class="line">half3 brdf += brdfData.specular * DirectBRDFSpecular(...);</span><br></pre></td></tr></table></figure><p>高光反射方程如下，其中G项有时与分母的矫正因子合并被称为V项：</p><p>$$</p><p>f_{spec} = = </p><p>$$</p><p>在Unity实现中，D项使用了常用的GGX，其具有更好的高光拖尾：</p><p><span class="math display">\[D_{GGX}=\frac{roughness^2}{\PI ((roughness^2-1)(n \dot h)^2 + 1)^2}\]</span></p><p>而对于V项和F项，Unity又做了一些自己的拟合，具体可以看看<ahref="https://community.arm.com/cfs-file/__key/communityserver-blogs-components-weblogfiles/00-00-00-20-66/siggraph2015_2D00_mmg_2D00_renaldas_2D00_notes.pdf">这篇pdf</a>。这里简单说一下过程：</p><p>首先其对比了 Smith GGX 和 KSK两种模型，KSK虽然更简单但其未将粗糙参数考虑在内。于是Unity将两者拟合得到SKSm模型：</p><p><span class="math display">\[V_{Smith} = \frac{1}{    ((N \cdot L)(1-k)+k)((N \cdot V)(1-k)+k)}//V_{SKS} = \frac{1}{(L \cdot H)(L \cdot H)}//V_{SKSm} = \frac{1}{    (L \cdot H)^2 (1-roughness^2) + roughness^2}\]</span></p><p>对于F项，Unity在被C.Schüler提出的拟合启发后，决定将高光颜色（折射率）brdfSpecular提出高光BRDF的计算，在最后做乘法运算。在使用LoH对F项也做了拟合之后，Unity将两者公式变为：</p><p><span class="math display">\[V \cdot F = \frac{(1-L\codt H)^5}{(L \codtH)^2(1-roughness^2)+roughness^2}\]</span></p><p>然后，Unity又寻找了一个形状近似的模型来进一步简化计算，得到最终使用的公式：</p><p><span class="math display">\[V \cdot F _{approx} = \frac{1}{(L \cdot H)^2 (roughness + 0.5)}\]</span></p><p>这样处理下来，Unity在URP中实现的PBR效果相比BIRP肯定有所下降，但这也是为了性能考虑把。最终的高光BRDF为：</p><p><span class="math display">\[BRDF_{spec} = \frac{roughness^2}{    4 \PI ((N \cdot H)^2 (roughness^2 - 1) + 1)^2 \cdot (L \cdot H)^2(roughness+ 0.5)} \cdot brdfSpecular\]</span></p><p>对于一开始说到的漫反射分母中的 <spanclass="math inline">\(\PI\)</span>去哪里了，现在我们终于可以尝试解答一下了。在Unity使用的高光反射模型中，最终公式分母也包含了<spanclass="math inline">\(\PI\)</span>，只是和漫发射相同，其并未在代码中出现。而且渲染方程，或者反射方式其本身也并不具有<spanclass="math inline">\(\PI\)</span>相关常数项。所以我们只能猜测，一种可能是，因为其为常数因子，所以Unity为了节省运算省掉了。</p><p>其他部分，Unity也是正常按照渲染方程计算的，只是为了性能做了很多拟合，牺牲了质量。</p><h3 id="额外光源计算">额外光源计算</h3><p>Unity中的光源有两种类型，像素光源和顶点光源，两者进行光照计算时的对象不同。所以，除去主光源外，还需要分两者情况考虑额外光源。</p><h4 id="逐像素光源">逐像素光源</h4><p>额外的逐像素光源与主光源的光照算法相同，所以一般游戏会限制像素光源的数量来降低性能消耗。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">LIGHT_LOOP_BEGIN(pixelLightCount)</span><br><span class="line">    Light light = GetAdditionalLight(lightIndex, inputData, shadowMask, aoFactor);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (IsMatchingLightLayer(light.layerMask, meshRenderingLayers))</span><br><span class="line">    &#123;</span><br><span class="line">        lightingData.additionalLightsColor += LightingPhysicallyBased(...);</span><br><span class="line">    &#125;</span><br><span class="line">LIGHT_LOOP_END</span><br></pre></td></tr></table></figure><h4 id="逐顶点光源">逐顶点光源</h4><p>顶点光源计算在顶点着色器中实现，实现非常简单，只做Lambert处理即可。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">half3 <span class="title function_">VertexLighting</span><span class="params">(float3 positionWS, half3 normalWS)</span></span><br><span class="line">&#123;</span><br><span class="line">    half3 vertexLightColor = half3(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">ifdef</span> _ADDITIONAL_LIGHTS_VERTEX</span></span><br><span class="line">    uint lightsCount = GetAdditionalLightsCount();</span><br><span class="line">    LIGHT_LOOP_BEGIN(lightsCount)</span><br><span class="line">        Light light = GetAdditionalLight(lightIndex, positionWS);</span><br><span class="line">        half3 lightColor = light.color * light.distanceAttenuation;</span><br><span class="line">        vertexLightColor += LightingLambert(lightColor, light.direction, normalWS);</span><br><span class="line">    LIGHT_LOOP_END</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> vertexLightColor;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="clearcoatpbr光照计算">ClearCoatPBR光照计算</h2><p>清漆PBR计算和普通PBR在流程上是同时计算的，也由<code>LightingPhysicallyBased</code>函数执行。</p><p>待续-----------------</p><h2 id="final-color">Final Color</h2><p>最后，Unity使用<code>CalculateLightingColor</code>函数对上述提到的光照做最终处理，集成在一个函数的好处就是方便调试各光照效果。函数只要将所有光照做和，便能得到最终的光照效果。</p><p>至此，光照整个流程就算走完了。因为Unity版本更新很快，也在对URP等管线做调整，如果有遗留，后续再进行补充。</p><h2 id="参考资料">参考资料</h2><p><ahref="https://zhuanlan.zhihu.com/p/371395846">URP管线PBR源码剖析</a></p><p><ahref="https://zhuanlan.zhihu.com/p/60977923">【基于物理的渲染（PBR）白皮书】（三）迪士尼原则的BRDF与BSDF相关总结</a></p><p><a href="https://community.arm.com/c/e/40">Optimizing PBR forMobile</a></p>]]></content>
      
      
      <categories>
          
          <category> Game Development </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
            <tag> PBR </tag>
            
            <tag> SRP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>URP Fur Rendering</title>
      <link href="/2024/11/14/Game%20Development/URP-Fur/"/>
      <url>/2024/11/14/Game%20Development/URP-Fur/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>绒毛渲染是毛发渲染中的一部分，属于短毛类。绒毛通常拥有柔软丰富的外轮廓，其上的光影也表较柔和，但是巨量的毛发代表着极高的复杂性和庞大的计算量。在实时渲染领域有一些成熟的方案兼具了表现及性能，最常见的就是Shell-basedFur方案。</p><h2 id="shell-based-fur-技术原理">Shell-based Fur 技术原理</h2><p>Shell-basedFur使用了多层渲染，在同等长度下随着面片层数的增多，面片之间距离足够近的情况下其看起来就像是一根实体。这种方案有显而易见的缺点，当毛发变长时需要更多的层数来保持表现在令人可接受的范围内，但是这就会造成性能影响。所以，使用时需要维持表现和性能的平衡。如下图对比，分别为6层，16层和64层：</p><p><img src="/images/HairRendering/gFurHelp_Introduction_01_Shells.jpg"></p><p>如果想要模拟浓密的毛发，需要在多边形的表面上绘制多层毛发的横截面。以球体为例，我们需要复制此球体表面并稍微膨胀，在膨胀的表面上绘制出毛发的第一个横截面。然后，重复这个步骤，每一层略高于另一层，直到毛发尖端。这些层被称为“shell”，是方案名称的由来。此过程示意图如下：</p><p><img src="/images/HairRendering/gFurHelp_Introduction_02_Dense.jpg"></p><p>在渲染毛发横截面时需要使用到纹理，此纹理一般为黑白噪点图。我们可以通过对模型进行变换是毛发渲染出来不那么僵硬，当然也可以用其他方法进行调整。对模型变换效果如图：</p><p><img src="/images/HairRendering/gFurHelp_Introduction_03_Variation.jpg"></p><p>经过上述过成毛发算是有了一个基本的型，但是没有光影效果的毛发是没有灵魂的。至于光影效果的处理，我们在具体实现时细说。</p><h2 id="in-urp">In URP</h2><p>由于URP多Pass渲染的问题，我们使用几何着色器来进行实现，不了解几何着色器的朋友可以看看<ahref="https://rigelaster.github.io/2024/11/05/Game%20Development/Geometry%20Grass/">这篇文章</a>。</p><p>首先我们需要的是一张颜色贴图(BaseMap)和一张毛发贴图(FurMap)，颜色贴图随意，毛发贴图使用512*512的下图：</p><p><img src="/images/HairRendering/FurNoise.png" height = "200"></p><p>除此之外，控制毛发的属性还有层数<code>Shell Amount</code>，层之间步长<code>Shell Step</code>以及透明度裁剪值<code>Alpha Cutout</code>。随后，使用几何着色器塑造毛发的型，下面给出核心函数：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">AppendShellVertex</span><span class="params">(inout TriangleStream&lt;Varyings&gt; stream, Attributes input, <span class="type">int</span> index)</span></span><br><span class="line">&#123;</span><br><span class="line">    Varyings output = (Varyings)<span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">    VertexPositionInputs vertexInput = GetVertexPositionInputs(input.positionOS.xyz);</span><br><span class="line">    VertexNormalInputs normalInput = GetVertexNormalInputs(input.normalOS, input.tangentOS);</span><br><span class="line"></span><br><span class="line">    float3 shellDir = SafeNormalize(normalInput.normalWS);</span><br><span class="line"></span><br><span class="line">    output.positionWS = vertexInput.positionWS + shellDir * (_ShellStep * index);</span><br><span class="line">    output.positionCS = TransformWorldToHClip(output.positionWS);</span><br><span class="line">    output.uv = TRANSFORM_TEX(input.texcoord, _BaseMap);</span><br><span class="line">    output.layer = (<span class="type">float</span>)index / _ShellAmount;</span><br><span class="line"></span><br><span class="line">    stream.Append(output);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[maxvertexcount(<span class="number">42</span>)]</span><br><span class="line"><span class="type">void</span> <span class="title function_">geom</span><span class="params">(triangle Attributes input[<span class="number">3</span>], inout TriangleStream&lt;Varyings&gt; stream)</span></span><br><span class="line">&#123;</span><br><span class="line">    [loop] <span class="keyword">for</span> (<span class="type">float</span> i = <span class="number">0</span>; i &lt; _ShellAmount; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        [unroll] <span class="keyword">for</span> (<span class="type">float</span> j = <span class="number">0</span>; j &lt; <span class="number">3</span>; ++j)</span><br><span class="line">        &#123;</span><br><span class="line">            AppendShellVertex(stream, input[j], i);</span><br><span class="line">        &#125;</span><br><span class="line">        stream.RestartStrip();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float4 <span class="title function_">frag</span><span class="params">(Varyings input)</span> : SV_Target</span><br><span class="line">&#123;</span><br><span class="line">    float2 furUV = input.uv / _BaseMap_ST.xy * _FurScale;</span><br><span class="line">    float4 furColor = SAMPLE_TEXTURE2D(_FurMap, sampler_FurMap, furUV);</span><br><span class="line">    <span class="type">float</span> alpha = furColor.r * (<span class="number">1.0</span> - input.layer);</span><br><span class="line">    <span class="keyword">if</span>(input.layer &gt; <span class="number">0.0</span> &amp;&amp; alpha &lt; _AlphaCutout) discard;</span><br><span class="line"></span><br><span class="line">    float4 baseColor = SAMPLE_TEXTURE2D(_BaseMap, sampler_BaseMap, input.uv);</span><br><span class="line">    float3 color = baseColor.xyz;</span><br><span class="line">    <span class="keyword">return</span> float4(color, alpha);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/images/HairRendering/Fur-Shell-01.png" height = "400"></p><p>利用几何函数将模型表面沿法线方向挤出形成shell，越靠外的shell使用的透明度裁剪值alphaCutout越大。此时已经有了毛发的雏形，只是没用光影看起来有些怪。下一步，便是给毛发着色，为了方便，就直接使用URP的PBR着色模型，这样阴影等其他处理可以直接交给渲染管线。</p><p>由于短发的透光性，顶端部分可以被光线正常影响，但是底端因互相遮挡会造成阴影，所以遮挡可以使用层级进行如下插值：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">surfaceData.occlusion = lerp(<span class="number">1.0</span> - _Occlusion, <span class="number">1.0</span>, input.layer);</span><br></pre></td></tr></table></figure><p>再添加阴影，调整毛发密度之后，得到如下效果：</p><p><img src="/images/HairRendering/Fur-Shell-02.png" height = "400"></p><h3 id="wind">Wind</h3><p>形体和光照都有了之后，可以给毛发增加一些动态效果，这里就简单加一些风动效果。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">AppendShellVertex</span><span class="params">(inout TriangleStream&lt;Varyings&gt; stream, Attributes input, <span class="type">int</span> index)</span></span><br><span class="line">&#123;</span><br><span class="line">    [...]</span><br><span class="line">    <span class="type">float</span> moveFactor = <span class="built_in">pow</span>(<span class="built_in">abs</span>((<span class="type">float</span>)index / _ShellAmount), _BaseMove.w);</span><br><span class="line">    float3 windAngle = _Time.w * _WindFreq.xyz;</span><br><span class="line">    float3 windMove = moveFactor * _WindMove.xyz * <span class="built_in">sin</span>(windAngle + input.positionOS.xyz * _WindMove.w);</span><br><span class="line">    float3 move = moveFactor * _BaseMove.xyz;</span><br><span class="line">    float3 shellDir = SafeNormalize(normalInput.normalWS + move + windMove);</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心就是通过层做动作程度大小的差异，总之，一些都可以通过考虑层差异来实现。</p><p>https://discussions.unity.com/t/what-are-unroll-and-loop-when-to-use-them/881945</p><h2 id="参考资料">参考资料</h2><p><ahref="https://gim.studio/an-introduction-to-shell-based-fur-technique/">AnIntroduction to Shell Based Fur Technique</a></p><p><a href="https://github.com/hecomi/UnityFurURP">UnityFurURP</a></p><p><ahref="https://www.bilibili.com/video/BV1Tr4y137XM/?spm_id_from=333.788.player.switch&amp;vd_source=a6d4de83b46a19b70d53fc1e9adb6574&amp;p=2">【技术美术百人计划】图形5.6 毛发实现注意点</a></p><p><ahref="https://mp.weixin.qq.com/s/aIWMEO5Qa2gNn2yCmnHbOg?tdsourcetag=s_pctim_aiomsg">真香预警！新版“峡谷第一美”妲己尾巴毛发制作分享</a></p>]]></content>
      
      
      <categories>
          
          <category> Unity SRP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
            <tag> Fur Rendering </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>fbx From Blender to Unity</title>
      <link href="/2024/11/07/Game%20Development/fbx%20From%20Blender%20to%20Unity/"/>
      <url>/2024/11/07/Game%20Development/fbx%20From%20Blender%20to%20Unity/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>模型的导入导出在游戏开发中可谓是非常频繁的一件事情，也是经常出现问题的地方。因为各文件格式的封装和软件对某一格式文件导入导出处理不同，可能产生模型各种表现上的问题。最好的解决办法就是，问题出现时搞清楚原因避免之后出现同样的问题从而浪费时间。由于市面上的软件不断增多及版本不断更新，这样的问题遇到的只会越来越频繁。努力搞清楚遇到的问题吧。</p><h2 id="fbx-from-blender3.4-to-unity2022">fbx From Blender3.4 toUnity2022</h2><p><strong>多图预警</strong></p><h3 id="scale">Scale</h3><p>首先做的事情就是让两者的单位系统保持一致，Unity中单位长度为1m，Blender中默认情况也是如此，以防万一还是检查一下。其中的<code>Unit Scale</code>让它保持默认就好了，它会关联到导出设置时的<code>Apply Unit</code>。</p><figure><img src="/images/BlenderToUnity/BlenderUnitSystem.png"alt="blender unit system" /><figcaption aria-hidden="true">blender unit system</figcaption></figure><p>在开始测试之前，我们需要了解一下有关fbx scale的知识。根据<ahref="https://forums.autodesk.com/t5/fbx-forum/fbx-units-and-scaling/td-p/9761576">官方人员的说法</a>，fbx文件是<code>unitless</code>的，只是默认其单位为厘米(centimeter)。因为，文件中只记录了数据例如25.4，其只代表25.4个单位长度，至于其具体含义是读取文件的应用赋予的，可以是25.4厘米，也可以是25.4米。后来为了避免这种“武断”的解释，fbx引入了变量<code>UnitScaleFactor</code>，其用来将“隐式的”厘米转换为我们需要的单位。例如，fbx中有一个数据25.4，此时UnitScaleFactor为0.1，那么其表示的就是25.4毫米，也就是2.54厘米。有个这些观念，我们开始实验。</p><p>将stanford-dragon.ply导入blender中，什么也不操作。其是一个尺寸很小的，上方指向y轴正方向的中国龙模型。我们先测试缩放的问题，给模型的xyz缩放都赋予5。导出设置上，在<code>Transform</code>标签下，将<code>Scale</code>设为3.0，然后对<code>Apply Scalings</code>进行测试。</p><p><img src="/images/BlenderToUnity/dragon-scale-1.png" alt="xyz scaled 5.0" /></p><p>先使用<code>All Local</code>，将其导入Unity中如图：</p><blockquote><p>Apply custom scaling and units scaling to each object transformation,FBX scale remains at 1.0.</p></blockquote><p><img src="/images/BlenderToUnity/dragon-scale-with3-local.png" alt="all local" /></p><p>可以看到，在Unity中，模型不再是躺着的，其上方也是指向y轴正方向的，方向问题我们之后再聊。此时Unity场景中的模型大小比Blender中大一些，大约是3倍左右，应该导出时设置的<code>Scale</code>影响的。而且它在xyz上有一个1500的缩放。这个缩放是怎么来的呢？我们先回到Blender将导出设置中的ApplyScalings改为<code>FBX Units Scale</code>，其他配置不变，然后再导入Unity中：</p><blockquote><p>Apply custom scaling to each object transformation, and units scalingto FBX scale.</p></blockquote><p><img src="/images/BlenderToUnity/dragon-scale-with3-unitsScale.png" alt="fbx units scale" /></p><p>此时的模型和前面大小相同，但是其缩放从1500变为15，有趣，我们继续回到Blender将ApplyScalings改为<code>FBX Custom Scale</code>再导入Unity：</p><blockquote><p>Apply custom scaling to FBX scale, and units scaling to each objecttransformation.</p></blockquote><p><img src="/images/BlenderToUnity/dragon-scale-with3-customScale.png" alt="fbx custom scale" /></p><p>此时模型大小仍未改变，只是缩放又变为了500，好吧，我们还有最后一组，将ApplyScalings改为<code>FBX All</code>：</p><blockquote><p>Apply custom scaling and units scaling to FBX scale.</p></blockquote><p><img src="/images/BlenderToUnity/dragon-scale-with3-all.png" alt="fbx all" /></p><p>现在可以回过头来分析一下这四种情况，它们拖入到Unity场景中时的大小都是相同的，只是xyz轴缩放不同。所以，我们导出时的设置一部分影响到了模型的源数据，一部分影响到了模型的变换值，这两种数据都会被导入到最后的fbx文件中。综合分析一下其实不难得出，相关的因素是这三者：</p><p><img src="/images/BlenderToUnity/blender_fbx_export_settings.png" alt="export settings"></p><p>之前文章官方描述中的<code>custom scaling</code>指的是导出设置中的<code>Transform/Scale</code>，<code>units scaling</code>指的是文章一开始设定的场景Units，objecttransformation就是物体的Tranform。如此，我们就可以分析出每种情况下Unity中的缩放值来源。注意，由于fbx默认单位为厘米，我们设置的场景单位为米，所以米到厘米的缩放因子为100。</p><ul><li>当导出设置为<code>All Local</code>时，fbx文件的UnitScaleFactor为1.0，其他缩放因子都应用于物体的Transform上，scale= 5.0 * 3.0 * 100 = 1500;</li><li>当导出设置为<code>FBX Units Scale</code>时，场景缩放因子会被写入UnitScaleFactor为100.0，其他缩放因子作用物体上，scale= 5.0 * 3.0 = 15;</li><li>当导出设置为<code>FBX Custom Scale</code>时，导出设置的缩放因子会被写入UnitScaleFactor为3.0，其他缩放因子作用于物体上，scale= 5.0 * 100 = 500;</li><li>当导出设置为<code>FBX All</code>时，导出设置和场景的缩放因子都会影响UnitScaleFactor为300.0，其他缩放因子作用于物体上，scale= 5;</li></ul><p>推导出的数值和实际数值都符合，所以导出设置中的<code>Apply Scaling</code>作用为何已经明了。然后就来到了Unity这边，Unity在导入资源时都会有一个ImportingSettings开放给开发者，这次我们只关注其中的<code>Convert Units</code>项：</p><p><img src="/images/BlenderToUnity/Unity-fbx-import-settings.png" alt="unity fbx import settings"></p><p>在Convert Units后面有一句话，“1 custom unit(File) to0.03m(Unity)”，不必说这肯定是Unity为了处理fbx单位问题的设置。上述测试的四个模型这里的提示是不同的，分别是：</p><ul><li>Export with All Local : lcm(File) to 0.01m(Unity)</li><li>Export with FBX Units Scale : 1m(File) to 1m(Unity)</li><li>Export with FBX Custom Scale : 1 custom unit(File) to0.003m(Unity)</li><li>Export with FBX All : 1 custom unit(File) to 3m(Unity)</li></ul><p>这里结合我们文章开始提到的官方人员那段话就可以很好的理解，Unity默认fbx数据的单位为厘米，然后结合<code>UnitScaleFactor</code>去解读fbx文件的具体单位含义，然后将其再转换为Unity中的单位米。以我们最后一个导出设置为例，文件的<code>UnitScaleFactor</code>为300，此时fbx文件单位长度具体为1cm * 300 = 300cm = 3m。也就是说，Unity会把文件中的数据以3倍的比例在Unity中绘制。例如，文件中一个点的坐标为(1,1,1)，Unity在绘制它的时候就会使用(3,3,3)。也就是说，我们如果把这个勾选去掉，模型会变为原来的1/3大小。</p><p><img src="/images/BlenderToUnity/Unity-disable-convertUnits.png" alt="Convert Units"></p><p>到这里，我们就把导出导入过程中和缩放有关的问题说完了，下面是坐标系的问题。</p><h3 id="axis">Axis</h3><p>坐标轴的转换问题要更复杂（也可能是我想的太多了），花了一些时间缕了一下思路。由于一些资料的不足，不保证所有的依据完全准确，但实际使用下来应该没太大问题。有了上面Scaling的经验，我们这次先从FBX文件本身下手，搞情况其结构，而后就方便查找导入导出时产生的问题了。</p><p>FBX文件是Autodesk为3D模型开发的，有两种格式，二进制格式和ASCII格式。两者之间可以通过<ahref="https://aps.autodesk.com/developer/overview/fbx-converter-archives">官方提供的工具FBXConverter</a>来进行相互转换。其二进制模式更适合开发，ASCII模式是可以看到其内部数据的，只是数据会变大一些。我们就通过FBX的ASCII模式来一窥其内部结构，其中包含很多信息，我们将用到的也就是GlobalSettings块和Vertices块。我们先建一个8顶点的模型用来进行测试，其内部数据为：</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">GlobalSettings:  &#123;</span><br><span class="line">Version: <span class="number">1000</span></span><br><span class="line">Properties70:  &#123;</span><br><span class="line">P: <span class="string">&quot;UpAxis&quot;</span>, <span class="string">&quot;int&quot;</span>, <span class="string">&quot;Integer&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">1</span></span><br><span class="line">P: <span class="string">&quot;UpAxisSign&quot;</span>, <span class="string">&quot;int&quot;</span>, <span class="string">&quot;Integer&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">1</span></span><br><span class="line">P: <span class="string">&quot;FrontAxis&quot;</span>, <span class="string">&quot;int&quot;</span>, <span class="string">&quot;Integer&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">0</span></span><br><span class="line">P: <span class="string">&quot;FrontAxisSign&quot;</span>, <span class="string">&quot;int&quot;</span>, <span class="string">&quot;Integer&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">1</span></span><br><span class="line">P: <span class="string">&quot;CoordAxis&quot;</span>, <span class="string">&quot;int&quot;</span>, <span class="string">&quot;Integer&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">2</span></span><br><span class="line">P: <span class="string">&quot;CoordAxisSign&quot;</span>, <span class="string">&quot;int&quot;</span>, <span class="string">&quot;Integer&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">-1</span></span><br><span class="line">P: <span class="string">&quot;OriginalUpAxis&quot;</span>, <span class="string">&quot;int&quot;</span>, <span class="string">&quot;Integer&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">-1</span></span><br><span class="line">P: <span class="string">&quot;OriginalUpAxisSign&quot;</span>, <span class="string">&quot;int&quot;</span>, <span class="string">&quot;Integer&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">1</span></span><br><span class="line">P: <span class="string">&quot;UnitScaleFactor&quot;</span>, <span class="string">&quot;double&quot;</span>, <span class="string">&quot;Number&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">100</span></span><br><span class="line">P: <span class="string">&quot;OriginalUnitScaleFactor&quot;</span>, <span class="string">&quot;double&quot;</span>, <span class="string">&quot;Number&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">100</span></span><br><span class="line">P: <span class="string">&quot;AmbientColor&quot;</span>, <span class="string">&quot;ColorRGB&quot;</span>, <span class="string">&quot;Color&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span></span><br><span class="line">P: <span class="string">&quot;DefaultCamera&quot;</span>, <span class="string">&quot;KString&quot;</span>, <span class="string">&quot;&quot;</span>, <span class="string">&quot;&quot;</span>, <span class="string">&quot;Producer Perspective&quot;</span></span><br><span class="line">P: <span class="string">&quot;TimeMode&quot;</span>, <span class="string">&quot;enum&quot;</span>, <span class="string">&quot;&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">11</span></span><br><span class="line">P: <span class="string">&quot;TimeSpanStart&quot;</span>, <span class="string">&quot;KTime&quot;</span>, <span class="string">&quot;Time&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">0</span></span><br><span class="line">P: <span class="string">&quot;TimeSpanStop&quot;</span>, <span class="string">&quot;KTime&quot;</span>, <span class="string">&quot;Time&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">46186158000</span></span><br><span class="line">P: <span class="string">&quot;CustomFrameRate&quot;</span>, <span class="string">&quot;double&quot;</span>, <span class="string">&quot;Number&quot;</span>, <span class="string">&quot;&quot;</span>,<span class="number">24</span></span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Vertices: *<span class="number">24</span> &#123;</span><br><span class="line">a: <span class="number">0.100000001490116</span>,<span class="number">0.200000002980232</span>,<span class="number">0.300000011920929</span>,<span class="number">0.5</span>,<span class="number">0.600000023841858</span>,<span class="number">-0.699999988079071</span>,<span class="number">0.400000005960464</span>,<span class="number">-0.5</span>,<span class="number">0.600000023841858</span>,<span class="number">0.800000011920929</span>,<span class="number">-0.899999976158142</span>,<span class="number">-1</span>,<span class="number">-0.200000002980232</span>,<span class="number">0.300000011920929</span>,<span class="number">0.400000005960464</span>,<span class="number">-0.600000023841858</span>,<span class="number">0.699999988079071</span>,<span class="number">-0.800000011920929</span>,<span class="number">-0.300000011920929</span>,<span class="number">-0.400000005960464</span>,<span class="number">0.5</span>,<span class="number">-0.699999988079071</span>,<span class="number">-0.800000011920929</span>,<span class="number">-0.899999976158142</span></span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><p>在Settings里面可以一眼看到我们的老朋友<code>UnitScaleFactor</code>，有兴趣的可以去验证一下Scaling部分内容，这里不多说了。随后和Axis有关的部分有：UpAxis,UpAxisSign, FrontAxis, FrontAxisSign, CoordAxis, CoordAxisSign,OriginalUpAxis,OriginalUpAxisSign。由于最后的<code>OriginalUpAxis</code>没查到有啥作用，所以我们只关心前三组属性。</p><p>下面Vertices块中有24个数据，分别来自8个顶点，第一个顶点为(0.1, 0.2,0.3)，然后依次类推。我们这里只用来分析导出设置中的Up和Forward变化时，对他有没有影响。因为不适合放图，所以，我直接说结论：<strong>更改fbxExportSettings里面的forward和up对这里的数据没有影响</strong>。其值便是顶点在Blender中的坐标，只是不确定每个软件导出时都会使用自己的坐标系，还是说fbx默认使用的坐标系与Blender相同，在目前的讨论前提下我们就不纠结这个问题了。现在，已经可以确定影响轴转换的就是FBXGlobalSettings里面的那三组属性了。</p><p>然后需要搞清楚的是，Blender FbxExport的设置如何影响Axis数据写入fbx文件的。可以分别组合尝试导入，打开ASCII文件直接查看里面的内容记录，当然，这里已经有<ahref="https://projects.blender.org/blender/blender-addons/issues/43935">来自Blender官方的数据</a>:</p><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">RIGHT_HAND_AXES = &#123;</span><br><span class="line">    # Up, Forward -&gt; FBX values (tuples of (axis, sign), Up, Front, Coord).</span><br><span class="line">    ( <span class="string">&#x27;X&#x27;</span>, <span class="string">&#x27;-Y&#x27;</span>): ((<span class="number">0</span>,  <span class="number">1</span>), (<span class="number">1</span>,  <span class="number">1</span>), (<span class="number">2</span>,  <span class="number">1</span>)),</span><br><span class="line">    ( <span class="string">&#x27;X&#x27;</span>,  <span class="string">&#x27;Y&#x27;</span>): ((<span class="number">0</span>,  <span class="number">1</span>), (<span class="number">1</span>, <span class="number">-1</span>), (<span class="number">2</span>, <span class="number">-1</span>)),</span><br><span class="line">    ( <span class="string">&#x27;X&#x27;</span>, <span class="string">&#x27;-Z&#x27;</span>): ((<span class="number">0</span>,  <span class="number">1</span>), (<span class="number">2</span>,  <span class="number">1</span>), (<span class="number">1</span>, <span class="number">-1</span>)),</span><br><span class="line">    ( <span class="string">&#x27;X&#x27;</span>,  <span class="string">&#x27;Z&#x27;</span>): ((<span class="number">0</span>,  <span class="number">1</span>), (<span class="number">2</span>, <span class="number">-1</span>), (<span class="number">1</span>,  <span class="number">1</span>)),</span><br><span class="line">    (<span class="string">&#x27;-X&#x27;</span>, <span class="string">&#x27;-Y&#x27;</span>): ((<span class="number">0</span>, <span class="number">-1</span>), (<span class="number">1</span>,  <span class="number">1</span>), (<span class="number">2</span>, <span class="number">-1</span>)),</span><br><span class="line">    (<span class="string">&#x27;-X&#x27;</span>,  <span class="string">&#x27;Y&#x27;</span>): ((<span class="number">0</span>, <span class="number">-1</span>), (<span class="number">1</span>, <span class="number">-1</span>), (<span class="number">2</span>,  <span class="number">1</span>)),</span><br><span class="line">    (<span class="string">&#x27;-X&#x27;</span>, <span class="string">&#x27;-Z&#x27;</span>): ((<span class="number">0</span>, <span class="number">-1</span>), (<span class="number">2</span>,  <span class="number">1</span>), (<span class="number">1</span>,  <span class="number">1</span>)),</span><br><span class="line">    (<span class="string">&#x27;-X&#x27;</span>,  <span class="string">&#x27;Z&#x27;</span>): ((<span class="number">0</span>, <span class="number">-1</span>), (<span class="number">2</span>, <span class="number">-1</span>), (<span class="number">1</span>, <span class="number">-1</span>)),</span><br><span class="line">    ( <span class="string">&#x27;Y&#x27;</span>, <span class="string">&#x27;-X&#x27;</span>): ((<span class="number">1</span>,  <span class="number">1</span>), (<span class="number">0</span>,  <span class="number">1</span>), (<span class="number">2</span>, <span class="number">-1</span>)),</span><br><span class="line">    ( <span class="string">&#x27;Y&#x27;</span>,  <span class="string">&#x27;X&#x27;</span>): ((<span class="number">1</span>,  <span class="number">1</span>), (<span class="number">0</span>, <span class="number">-1</span>), (<span class="number">2</span>,  <span class="number">1</span>)),</span><br><span class="line">    ( <span class="string">&#x27;Y&#x27;</span>, <span class="string">&#x27;-Z&#x27;</span>): ((<span class="number">1</span>,  <span class="number">1</span>), (<span class="number">2</span>,  <span class="number">1</span>), (<span class="number">0</span>,  <span class="number">1</span>)),</span><br><span class="line">    ( <span class="string">&#x27;Y&#x27;</span>,  <span class="string">&#x27;Z&#x27;</span>): ((<span class="number">1</span>,  <span class="number">1</span>), (<span class="number">2</span>, <span class="number">-1</span>), (<span class="number">0</span>, <span class="number">-1</span>)),</span><br><span class="line">    (<span class="string">&#x27;-Y&#x27;</span>, <span class="string">&#x27;-X&#x27;</span>): ((<span class="number">1</span>, <span class="number">-1</span>), (<span class="number">0</span>,  <span class="number">1</span>), (<span class="number">2</span>,  <span class="number">1</span>)),</span><br><span class="line">    (<span class="string">&#x27;-Y&#x27;</span>,  <span class="string">&#x27;X&#x27;</span>): ((<span class="number">1</span>, <span class="number">-1</span>), (<span class="number">0</span>, <span class="number">-1</span>), (<span class="number">2</span>, <span class="number">-1</span>)),</span><br><span class="line">    (<span class="string">&#x27;-Y&#x27;</span>, <span class="string">&#x27;-Z&#x27;</span>): ((<span class="number">1</span>, <span class="number">-1</span>), (<span class="number">2</span>,  <span class="number">1</span>), (<span class="number">0</span>, <span class="number">-1</span>)),</span><br><span class="line">    (<span class="string">&#x27;-Y&#x27;</span>,  <span class="string">&#x27;Z&#x27;</span>): ((<span class="number">1</span>, <span class="number">-1</span>), (<span class="number">2</span>, <span class="number">-1</span>), (<span class="number">0</span>,  <span class="number">1</span>)),</span><br><span class="line">    ( <span class="string">&#x27;Z&#x27;</span>, <span class="string">&#x27;-X&#x27;</span>): ((<span class="number">2</span>,  <span class="number">1</span>), (<span class="number">0</span>,  <span class="number">1</span>), (<span class="number">1</span>,  <span class="number">1</span>)),</span><br><span class="line">    ( <span class="string">&#x27;Z&#x27;</span>,  <span class="string">&#x27;X&#x27;</span>): ((<span class="number">2</span>,  <span class="number">1</span>), (<span class="number">0</span>, <span class="number">-1</span>), (<span class="number">1</span>, <span class="number">-1</span>)),</span><br><span class="line">    ( <span class="string">&#x27;Z&#x27;</span>, <span class="string">&#x27;-Y&#x27;</span>): ((<span class="number">2</span>,  <span class="number">1</span>), (<span class="number">1</span>,  <span class="number">1</span>), (<span class="number">0</span>, <span class="number">-1</span>)),</span><br><span class="line">    ( <span class="string">&#x27;Z&#x27;</span>,  <span class="string">&#x27;Y&#x27;</span>): ((<span class="number">2</span>,  <span class="number">1</span>), (<span class="number">1</span>, <span class="number">-1</span>), (<span class="number">0</span>,  <span class="number">1</span>)),  # Blender system!</span><br><span class="line">    (<span class="string">&#x27;-Z&#x27;</span>, <span class="string">&#x27;-X&#x27;</span>): ((<span class="number">2</span>, <span class="number">-1</span>), (<span class="number">0</span>,  <span class="number">1</span>), (<span class="number">1</span>, <span class="number">-1</span>)),</span><br><span class="line">    (<span class="string">&#x27;-Z&#x27;</span>,  <span class="string">&#x27;X&#x27;</span>): ((<span class="number">2</span>, <span class="number">-1</span>), (<span class="number">0</span>, <span class="number">-1</span>), (<span class="number">1</span>,  <span class="number">1</span>)),</span><br><span class="line">    (<span class="string">&#x27;-Z&#x27;</span>, <span class="string">&#x27;-Y&#x27;</span>): ((<span class="number">2</span>, <span class="number">-1</span>), (<span class="number">1</span>,  <span class="number">1</span>), (<span class="number">0</span>,  <span class="number">1</span>)),</span><br><span class="line">    (<span class="string">&#x27;-Z&#x27;</span>,  <span class="string">&#x27;Y&#x27;</span>): ((<span class="number">2</span>, <span class="number">-1</span>), (<span class="number">1</span>, <span class="number">-1</span>), (<span class="number">0</span>, <span class="number">-1</span>)),</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>前半部分是导出时的设置，后半部分是三组属性对应的值。在继续下去之前，我们要了解到以下两个前提。第一，Blender使用的是<strong>Z-upY-forward Right-hand</strong>坐标系，Unity使用的是<strong>Y-up Z-forwardLeft-hand</strong>坐标系。第二，在目前的讨论中<code>front</code>和<code>forward</code>的含义是有区别的，具体可以看前一个链接中的说法。我的理解是，<code>front</code>指的是模型的前方，比如说一个人体模型，它的<code>front</code>就是面朝的方向。而<code>forward</code>多指的是开发者的前方，从屏幕表面指向屏幕内部的方向。</p><p>看回之前的数据，其中备注有<code>Blender system!</code>这行的含义为：当Blender FBX Export设置为Z-up Y-forward时，会将 UpAxis = 2, UpAxisSign = 1, FrontAxis = 1,FrontAxisSing = -1, CoordAxis = 0, CoordAxisSign = 1写入fbx的GlobalSettings中。这三组属性定义了fbx存储模型的空间特征，其他软件导入fbx时据此来处理模型的顶点数据。其中Axis有<code>0,1,2</code>三个值，分别对应存储数据的<code>x,y,z</code>轴。Sign有<code>1,-1</code>两个值，代表正向和反向。<code>up, front, coord</code>代表了模型的上、前、侧三个方向。这样fbx文件就包含了模型的方位信息，方便其他软件导入时转换为对应的坐标系。</p><p>文字说有点抽象，拿一个实际例子来说吧。现在我有一个模型如下：</p><p><img src="/images/BlenderToUnity/bunny.png" alt="bunny"></p><p>现在规定兔子上方为Up，兔子所看方向为前方Forward，然后以Blender的坐标系为基准，上方为Z轴正方向，前方为Y轴负方向。当这个fbx模型导入Unity时，Unity会先找到模型的上方Up是fbx记录数据的Z轴，而Unity自己的上方是Y轴，于是将数据中的Z当作Y使用。模型前方同理，fbx记录的是Y轴负方向，Unity自己是Z轴负方向，所以直接将数据中的Y当作Z使用。这样，即使不同软件的坐标系不同，也还是能够正确展示出模型。</p><p>你问我为什么没讨论Coord方向，因为我通过现在的资料不足以确定Coord是如何生效的，这涉及到左右手坐标系，我也还没搞清楚。</p><p>来到Unity端，其ImportSettings提供了一个和轴转换有关的选项<code>Bake Axis Conversion</code>，为了测试这个选项作用，做了一组实验，建一个简单的模型，其上有一点(1,2,3)，导出为fbx，测试不同导出和导入选项对点的影响：</p><table><colgroup><col style="width: 9%" /><col style="width: 7%" /><col style="width: 10%" /><col style="width: 10%" /><col style="width: 38%" /><col style="width: 23%" /></colgroup><thead><tr><th>实验序号</th><th>UpAxis</th><th>FrontAxis</th><th>CoordAxis</th><th>Not Bake Axis Conversion</th><th>Bake Axis Conversion</th></tr></thead><tbody><tr><td>1</td><td>Y</td><td>Z</td><td>X</td><td>(-1, 2, 3)</td><td>(1, 2, -3)</td></tr><tr><td>2</td><td>X</td><td>-Z</td><td>Y</td><td>(-1, 2, 3) with rot(0, -180, -90)</td><td>(2, 1, 3)</td></tr><tr><td>3</td><td>Z</td><td>-Y</td><td>X</td><td>(-1, 2, 3) with rot(-90, 0, 0)</td><td>(1, 3, 2)</td></tr><tr><td>4</td><td>Y</td><td>X</td><td>-Z</td><td>(-1, 2, 3) with rot(0 90, 0)</td><td>(-3, 2, -1)</td></tr></tbody></table><p>可以看到，未勾选<code>Bake Axis Conversion</code>选项时，Unity只会对fbx的源数据进行x轴翻转，然后通过Transform旋转来控制模型方向。x轴翻转取负可能是为了转换坐标系属性（右手到左手），这我不太确定Unity怎么处理的。而且之后自动赋予的初始旋转也不名所以，展示出的模型方向不统一。</p><p>勾选<code>Bake Axis Conversion</code>选项时，Unity应该会直接将fbxGlobalSettings中的轴相关属性烘焙到顶点数据上，烘培的原理应该和我上面提到的差不多，然后可能会再处理一下坐标系属性。</p><p>最后，说一下结论，当Blender的导出设置为<code>Z-up Y-forward</code>，Unity勾选<code>Bake Axis Conversion</code>时，模型在两者中的展示效果相同。</p><p><img src="/images/BlenderToUnity/bunny-in-Blender.png" alt="bunny"></p><p><img src="/images/BlenderToUnity/bunny-in-Unity.png" alt="bunny"></p><h3 id="小结">小结</h3><p>市面上繁多的软件就是会造成这种情况，个人的建议是，遇到相关问题解决并记录解决方案，等下次再遇到问题是会有更好的切入思路。也别太想搞明白底层的原理，有的软件源码没放开不知道具体操作理解起来还是有点难度的，而且网上也一直在争论，不影响开发进度的前提下，留下一些疑惑也是可以的。希望文章对你有些帮助，因为参考资料大多是各论坛上的东西，所以本文就算是个人的记录，请酌情参考。</p><p>我看到过的一些问答：</p><p><ahref="https://forums.autodesk.com/t5/fbx-forum/max-unity-and-fbx-question/td-p/7772482">MaxUnity and fbx question.</a></p><p><ahref="https://forums.autodesk.com/t5/fbx-forum/how-to-change-the-fbx-file-settings-to-fix-file-axis-direction/m-p/9405614">Howto change the FBX file settings to fix File Axis Direction?</a></p><p><ahref="https://projects.blender.org/blender/blender-addons/issues/60090">FBXExporter Coordinate System options don't appear to do anything inBlender</a></p><p><ahref="https://discussions.unity.com/t/what-does-really-mean-the-forward-direction/842678">Whatdoes really mean the “forward” direction?</a></p><h3 id="参考资料">参考资料</h3><p><ahref="https://forums.autodesk.com/t5/fbx-forum/fbx-units-and-scaling/td-p/9761576">FBXUnits and Scaling</a></p><p><ahref="https://discussions.unity.com/t/imported-models-from-blender-2-77-are-scaled-x100-in-unity-5-3/167005">Importedmodels from Blender 2.77 are scaled x100 in Unity 5.3</a></p><p><ahref="https://banexdevblog.wordpress.com/2014/06/23/a-quick-tutorial-about-the-fbx-ascii-format/">Aquick tutorial about the FBX ASCII format</a></p><p><ahref="https://help.autodesk.com/view/FBX/2015/ENU/?guid=__cpp_ref_class_fbx_global_settings_html">FbxGlobalSettingsClass Reference</a></p><p><ahref="https://help.autodesk.com/view/FBX/2015/ENU/?guid=__cpp_ref_class_fbx_axis_system_html">FbxAxisSystemClass Reference</a></p>]]></content>
      
      
      
        <tags>
            
            <tag> Unity </tag>
            
            <tag> Blender </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unity Geometry Shader Grass</title>
      <link href="/2024/11/05/Game%20Development/Geometry%20Grass/"/>
      <url>/2024/11/05/Game%20Development/Geometry%20Grass/</url>
      
        <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>看别人项目的时候突然看到GeometryShader，之前也接触过，不过过了有好一段时间没用忘的差不多了，就又来看看，顺便记录一下，方便后续查看。好像GeometryShader在商业中用的不是很广泛，有一些性能和平台的原因，有的平台不支持GeometryShader，查阅时找到的资料比较少，Unity官方的资料更是少中又少，总之先学习一下。</p><p>原文章就是一篇手把手的教学，写的非常详细，跟着做下来就会得到和案例相同的效果。本文主要还是记录一下其中用到的知识，以及过程中遇到的问题。</p><p>文章中生成草的大致过程为：向着色器中输入一个网格，使用几何着色器在网格的定点上生成草地的面片；然后，再调整面片的形状、朝向、和风的交互运动等；最后，为了更具真实性，让面片具有光影属性。</p><p>开始之前还是要提一嘴，文章作者提供了一个初始项目，里面Grass.shader中的代码用到了<code>CGINCLUDE</code>，这个东西是直接放在Shader块中的，根据文章作者以及Unity论坛中的说法，其可以将块中的内容自动include 到本着色器的所有pass中。其中实例代码中有开发人员的注释：</p><blockquote><p>Shader code pasted into all further CGPROGRAM blocks</p></blockquote><h3 id="geometry-shaders">Geometry shaders</h3><p>几何着色器(GeometryShader)是渲染管线中可配置部分，位于顶点着色器(VertexShader)之后（或者如果用到了曲面细分着色器 TessellationShader，就会在它的后面），在片元着色器(FragmentShader)之前。可以看下面Direct3D 11 图像管线的图：</p><figure><img src="/images/RoystanGeometryShaderGrass/d3d11-pipeline-stages.jpg"alt="d3d11 pipeline" /><figcaption aria-hidden="true">d3d11 pipeline</figcaption></figure><p>为了对几何着色器有个基本认知，可以先看看LearnOpenGL的<ahref="https://learnopengl.com/Advanced-OpenGL/Geometry-Shader">这篇文章</a>。</p><p>让我们来看一下，Unity中如何使用几何着色器。和顶点着色器类似，需要先在pass中做编辑声明，然后再依照格式实现，声明的函数：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// inside CGPROGRAM block</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> geometry geom</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">geometryOutput</span>&#123;</span></span><br><span class="line">    float4 pos : SV_POSITION;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">[maxvertexcount(<span class="number">3</span>)]</span><br><span class="line"><span class="type">void</span> <span class="title function_">geom</span><span class="params">(triangle float4 IN[<span class="number">3</span>] : SV_POSITION, inout TriangleStream&lt;geometryOutput&gt; triStream)</span></span><br><span class="line">&#123;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>几何着色器有两个参数，其中 <code>triangle float4 IN[3]</code>表示将一个三角形的三个顶点作为输入，第二个参数<code>TriangleStream</code>表示着色器将输出一个三角形流，其中每个顶点数据使用结构<code>geometryOutput</code>来存储。而且，函数还有最后一个参数位于函数上方<code>[maxvertexcount(3)]</code>，它告诉GPU我们每个图元最多提交3个顶点，多余的顶点会被忽略。</p><p>还有一个问题就是，我们之前提到在每个顶点处生成一个草地面片，为什么不使用点图元作为输入，而是用三角形图元？用点图元也是可以的，可以这样写：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">void</span> <span class="title function_">geom</span><span class="params">(point vertexOutput IN[<span class="number">1</span>], inout TriangleStream&lt;geometryOutput&gt; triStream)</span></span><br></pre></td></tr></table></figure><p>但是，就目前状况来看，我们输入的网格是三角形拓扑的(has a trianglemeshtopology)，上述的写法会产生输入的网格拓扑和要求输入的图元不适配的情况。这在DirectXHLSL中是允许的，但是在OpenGL中是不允许的。（多平台考虑问题还是要细心）</p><p>另外一个需要注意的问题就是，<strong>现在几何着色器要承担通常由顶点着色器承担的职责——输出顶点的裁剪空间位置信息</strong>。处理好上述问题之后，就可以正常得到使用几何着色器绘制的三角形：</p><p><img src="/images/RoystanGeometryShaderGrass/MGeometryGrass-1.png" width="400" alt="Geometry Grass 1"></p><p>几何着色器对于每个输入的图元都会执行一次，上图只出现了一个三角形是因为在几何着色器中直接使用常量作为位置信息，所有图元产生的三角形被绘制到了一起，只需要考虑输入图元的位置信息就可以将三角形分开来：</p><p><img src="/images/RoystanGeometryShaderGrass/MGeometryGrass-2.png" width="400" alt="Geometry Grass 2"></p><p>目前绘制的三角形只适用于水平面，当网格变为球形时，它的样子就变的很奇怪。为了能让三角形适用于任意网格，需要使用到一种技术叫做切线空间。</p><h3 id="tangent-space">Tangent space</h3><p>如果你不了解切线空间，请看LearnOpenGL的<ahref="https://learnopengl.com/Advanced-Lighting/Normal-Mapping">这篇文章</a>。</p><p>在切线空间绘制三角形便可以使其任意模型表面都保持正确的朝向，如下图：</p><p><img src="/images/RoystanGeometryShaderGrass/MGeometryGrass-3.png" width="400" alt="Geometry Grass 3"></p><h3 id="grass-look">Grass look</h3><p>目前绘制的三角形只是白色的面片，要让其称为叶片，需要增加一些颜色和形态。我们从增加叶片渐变颜色开始。</p><h4 id="color-gradient">Color gradient</h4><p>对于叶片的颜色，我们允许设置顶部和底部的颜色，中间的部分使用插值。可以使用uv来实现这个效果，这样我们就需要把uv传递给片元着色器。对于uv，设置为下图的样式：</p><p><img src="/images/RoystanGeometryShaderGrass/grass-uv.png" width="200" alt="Geometry Grass uv"></p><p>几何着色器也要承担起传递uv的责任：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// add to the geometryOutput struct</span></span><br><span class="line">float2 uv : TEXCOORD0;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Modify the function signature of the fragment shader</span></span><br><span class="line">float4 <span class="title function_">frag</span><span class="params">(geometryOutput i, fixed facing : VFACE)</span> : SV_Target</span><br></pre></td></tr></table></figure><p><img src="/images/RoystanGeometryShaderGrass/MGeometryGrass-4.png" width="400" alt="Geometry Grass 4"></p><h4 id="random-facing-direction-forward-bend-width-and-height">Randomfacing direction &amp; forward bend &amp; Width and height</h4><p>目前草地还是整齐排列的三角形面片，通过增加一些随机变量来使其看起来杂乱一些。这时会用到两个函数rand() 和AngleAxis3x3()。rand会根据输入的三维变量产生随机数，AngleAxis3x3根据输入的弧度角和旋转轴返回一个渲染矩阵。因为与几何着色器无关，函数具体实现不在这里展开，一般都会有先人实现的工具函数，这里使用的两个函数可以看文章<ahref="https://forum.unity.com/threads/am-i-over-complicating-this-random-function.454887/#post-2949326">rand</a>和<ahref="https://gist.github.com/keijiro/ee439d5e7388f3aafc5296005c8c3f33">AngleAxis3x3</a></p><p>在将变换融入之前的矩阵中之后，可以得到更像草地的东西：</p><p><img src="/images/RoystanGeometryShaderGrass/MGeometryGrass-5.png" width="400" alt="Geometry Grass 5"></p><p>显然，草地还是有些稀疏了，因为输入的几何网格三角形并不会特别密集，而每个图元只会产生一片草叶。为了使草地更密集，需要用到曲面细分着色器，其可以将三角形进一步划分。</p><h3 id="tessellation">Tessellation</h3><p>曲面细分也是渲染管线中可配置的渲染阶段，还记得之前那张渲染管线流程图吗？曲面细分位于顶点着色器之后，几何着色器之前，它的工作是将输入的表面细分为更多的图元。曲面细分通过两个可编程的阶段来实现：<strong>hull</strong>和 <strong>domain</strong>。</p><p>对于Unity的表面着色器(SurfaceShaders，个人认为是一个错误的设计，隐藏了一些内容，开放了一些接口)，Unity提供了BIRP下细分曲面的案例，但是我们现在没有使用表面着色器，所以需要自己去实现hullshader 和 domainshader。文章中作者并未展开这部分内容，但是我还是打算在这里详细写一下。这部分内容参考的是CatlikeCoding 的文章，链接放在文末。</p><h4 id="unity-custom-shader-tessellation">Unity Custom ShaderTessellation</h4><p>我们首先来看下数据流向图：</p><p><img src="/images/RoystanGeometryShaderGrass/shader-programs.png" width="400" alt="Shader Programs"></p><p>被处理的数据依次流过 hull program -&gt; Tessellation -&gt; dimainprogram，最后才流入几何着色器，我们依次说明。注意，只有4.6以上的着色器才能编程曲面细分。着色器编译声明都是要加的：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">pragma</span> hull hullProgramName</span></span><br><span class="line"><span class="meta">#<span class="keyword">pragma</span> domain domainProgramName</span></span><br></pre></td></tr></table></figure><h4 id="hull-shaders">Hull Shaders</h4><p>一个基础的hull函数应该具有以下结构：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">TessellationFactors</span>&#123;</span></span><br><span class="line">    <span class="type">float</span> edge[<span class="number">3</span>] : SV_TessFactor;</span><br><span class="line">    <span class="type">float</span> inside : SV_InsideTessFactor;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">TessellationFactors <span class="title function_">patchConstantFunction</span><span class="params">(InputPatch&lt;VertexInput, <span class="number">3</span>&gt; patch)</span></span><br><span class="line">&#123;</span><br><span class="line">    TessellationFactors f;</span><br><span class="line">    f.edge[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    f.edge[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">    f.edge[<span class="number">2</span>] = <span class="number">1</span>;</span><br><span class="line">    f.inside = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">return</span> f;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[UNITY_domain(<span class="string">&quot;tri&quot;</span>)]</span><br><span class="line">[UNITY_outputcontrolpoints(<span class="number">3</span>)]</span><br><span class="line">[UNITY_outputtopology(<span class="string">&quot;triangle_cw&quot;</span>)]</span><br><span class="line">[UNITY_partitioning(<span class="string">&quot;integer&quot;</span>)]</span><br><span class="line">[UNITY_patchconstantfunc(<span class="string">&quot;patchConstantFunction&quot;</span>)]</span><br><span class="line">VertexInput <span class="title function_">hull</span><span class="params">(InputPatch&lt;VertexInput, <span class="number">3</span>&gt; patch, uint id : SV_OutputControlPointID)</span>&#123;</span><br><span class="line">    <span class="keyword">return</span> patch[id];</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>hullprogram的任务就是把需要的顶点数据传递给细分曲面阶段。hull函数肯定是必须的，其具有两个参数：InputPatch&lt;VertexInput,3&gt; patch 和 SV_OutputControlPointIDid。patch是网格顶点集，VertexInput代表其数据格式，数字表示每个patch包含的顶点个数。SV_OutputControlPointID语义可以当作顶点在patch中的索引，其传递当前函数在哪个顶点运行的信息。这里就要提到，虽然hull函数输入了整个patch，但是函数还会在每个顶点上调用(<strong>getinvoked once per vertex in the patch</strong>)，输出一个顶点信息。</p><p>函数上方的属性信息给hull函数设定了一些属性，含义如下：</p><ul><li><code>UNITY_domain("tri")</code> 函数运行在三角形上</li><li><code>UNITY_outputcontrolpoints(3)</code>每个patch会输出3个控制点</li><li><code>UNITY_outputtopology("triangle_cw")</code>GPU创建三角形时需要知道是顺时针(clockwise)还是逆时针(counterclockwise)，Unity中是顺时针</li><li><code>UNITY_partitioning("integer")</code>GPU需要知道如何分割patch，有几种类型，这里先使用integer</li><li><code>UNITY_patchconstantfunc("patchConstantFunction")</code>GPU还需要知道要将patch分为几份，并不是一个定值，需要使用函数来返回值，这里使用的函数名称为patchConstantFunction</li></ul><p>每个patch如何被细分只需要确认一次就够了，所以patch constantfunction在每个patch上只会调用一次(only invoked once perpatch)。为了效率，此函数和hull函数是并行调用的，示意图如下：</p><p><img src="/images/RoystanGeometryShaderGrass/hull-shader.png" width="400" alt="inside a hull shader"></p><p>GPU需要四个因子来确定如何细分一个三角形，三角形每个边具有一个，还有一个因子在三角形内部。边因子使用语义SV_TessFactor，中心因子使用语义SV_InsideTessFactor，正如上述代码中所写的TessellationFactors结构体。</p><h4 id="domain-shaders">Domain Shaders</h4><p>一个domain函数具有一个结构：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">vertexOutput <span class="title function_">tessVert</span><span class="params">(vertexInput v)</span></span><br><span class="line">&#123;</span><br><span class="line">vertexOutput o;</span><br><span class="line"><span class="comment">// Note that the vertex is NOT transformed to clip</span></span><br><span class="line"><span class="comment">// space here; this is done in the grass geometry shader.</span></span><br><span class="line">o.vertex = v.vertex;</span><br><span class="line">o.normal = v.normal;</span><br><span class="line">o.tangent = v.tangent;</span><br><span class="line"><span class="keyword">return</span> o;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">[UNITY_domain(<span class="string">&quot;tri&quot;</span>)]</span><br><span class="line">vertexOutput <span class="title function_">domain</span><span class="params">(TessellationFactors factors, OutputPatch&lt;VertexInput, <span class="number">3</span>&gt; patch, float3 barycentricCoordinates : SV_DomainLocation)</span></span><br><span class="line">&#123;</span><br><span class="line">    vertexInput v;</span><br><span class="line"><span class="meta">#<span class="keyword">define</span> MY_DOMAIN_PROGRAM_INTERPOLATE(fieldName) v.fieldName = \</span></span><br><span class="line"><span class="meta">patch[0].fieldName * barycentricCoordinates.x + \</span></span><br><span class="line"><span class="meta">patch[1].fieldName * barycentricCoordinates.y + \</span></span><br><span class="line"><span class="meta">patch[2].fieldName * barycentricCoordinates.z;</span></span><br><span class="line"></span><br><span class="line">MY_DOMAIN_PROGRAM_INTERPOLATE(vertex)</span><br><span class="line">MY_DOMAIN_PROGRAM_INTERPOLATE(normal)</span><br><span class="line">MY_DOMAIN_PROGRAM_INTERPOLATE(tangent)</span><br><span class="line">    <span class="keyword">return</span> tessVert(v);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>domain函数看起来比hull函数简单些，只有一个属性和三个参数。factors便是我们上面提到的曲面细分因子，patch和hull函数的参数相同，最后一个参数barycentricCoordinates表示重心坐标。</p><p>流程图中，曲面细分阶段在domainprogram之前，它之决定了patch如何细分，但是并没有生成新的顶点，而是作为替代生成了这些新顶点的重心坐标。所以，domain函数的任务就是通过这些重心坐标生成最终的顶点数据。如果我没理解错的话，domain函数会在每个新顶点上调用一次(invokedonce per vertex)，通过语义 SV_DomainLocation提供重心坐标。新顶点的数据都是通过插值获得的。</p><h4 id="different-tessellation-factors">Different TessellationFactors</h4><p>在Unity中使用曲面细分的基础就是上面那些了，平时手动调整的也就是曲面细分因子factors了。此外还有一些进阶的用处，这里便不细说了，推荐看原文，这里放几张不同因子的对比图：</p><div data-align="center"><p><img src = "/images/RoystanGeometryShaderGrass/tessellation-factors-2.png" width="200"/><img src = "/images/RoystanGeometryShaderGrass/tessellation-factors-5.png" width="200"/><img src = "/images/RoystanGeometryShaderGrass/tessellation-factors-1-inside-7-outside.png" width="200"/><img src = "/images/RoystanGeometryShaderGrass/tessellation-factors-7-inside-1-outside.png" width="200"/></p></div><h4 id="tessellation-grass">Tessellation Grass</h4><p>了解完Unity中曲面细分如何使用之后，我们回到草地渲染中。将曲面细分加入草地渲染流程，然后调整曲面细分的因子，使三角形面增多，生成的草叶也就越密集。如图因子为7时：</p><p><img src="/images/RoystanGeometryShaderGrass/MGeometryGrass-6.png" width="400" alt="Shader Programs-6"></p><h3 id="wind">Wind</h3><p>风模拟是通过一张扰动贴图(distortiontexture)实现的，和法线贴图类似，但是只有xy两通道包含数据。我们会使用这两个值表示风的水平方向，不考虑风力的竖直方向。然后设定风力的强度<code>_WindStrength</code>和频率<code>_WindFrequency</code>，配合Unity内置时间变量<code>_Time</code>制作草地动画。使用的贴图如下（如果风力效果不对，可以尝试将图片导入设置中的sRGB勾选去掉，这是一个和伽马矫正有关的设置）：</p><p><img src="/images/RoystanGeometryShaderGrass/DistortionTexture.png" width="300" alt="Shader Programs-6"></p><p>这部分整体还是容易理解的，这里只提两个比较容易产生困扰的点。</p><p>第一点，我们需要一个uv来采样贴图，使用position坐标来采样贴图非常常见，这没有问题。但是文章作者说用<code>pos.xz</code>采样就可以在场景中有多个草地mesh时使它们看起来使用了同一个风力系统，这里的pos是物体空间的，要达成作者所说的效果需要使用世界坐标空间的position，个人判断这里作者写错了。</p><p>第二点，是一连串的矩阵乘法<code>float3x3 transformationMatrix = mul(mul(mul(tangentToLocal, windRotation), facingRotationMatrix), bendRotationMatrix)</code>，正确理解这里很重要。Unity用的是矩阵右乘，所以在外侧的矩阵变换是最先作用到顶点的。也就是说，这串矩阵乘法做的变换是：在切线空间中沿x轴给叶片一个旋转使叶片有一个前后的倾角，沿z轴旋转叶片使其有一个随机的朝向，让叶片随风方向摇摆，最后从切线空间变换到物体空间。</p><p>然后再看一下作者文章提到的叶片底部脱离地面的问题：</p><p><img src="/images/RoystanGeometryShaderGrass/blade-intersection-error.png" width="300" alt="blade intersection error"></p><p>在理解上面提到的那一串矩阵乘法之后很容易看出问题，因为叶片的三个顶点在随风摇摆时，底部顶点可能就会离开地面。所以，通过只让随机方向矩阵作用于底部顶点即可，并不会影响整体的表现。</p><h3 id="blade-curvature">Blade curvature</h3><p>目前的叶片由一个三角形组成，在动画时显然会表现的很僵硬。现在要做的就是将叶片划分成多个三角形，再用曲线模拟叶片的形状。我们打算将每个叶片(blade)划分为几个片段(segment)，每个片段是一个矩形，由两个三角形构成，除了叶尖还是用一个三角表示。TriangleStrip可以让我们很方便的构建一个三角形带，它先使用最前面的3个点构成一个三角形，随后每个点都会和之前的两个点构成一个新三角形，可以看如下动画演示：</p><p><img src="/images/RoystanGeometryShaderGrass/grass-construction.gif" width="300" alt="grass construction"></p><p>现在需要输出更多的顶点，不要忘记将几何着色器的属性<code>maxvertexcount</code>调大。通过宏定义<code>BLADE_SEGMENTS</code>来构建叶片的片段，越靠近底部叶片越宽，越靠近顶部叶片越尖，直接上代码可能更直观：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">[loop]<span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; BLADE_SEGMENTS; i++)</span><br><span class="line">&#123;</span><br><span class="line"><span class="type">float</span> t = i / (<span class="type">float</span>)BLADE_SEGMENTS;</span><br><span class="line"><span class="type">float</span> segmentHeight = height * t;</span><br><span class="line"><span class="type">float</span> segmentWidth = width * (<span class="number">1</span>-t);</span><br><span class="line"></span><br><span class="line">float3x3 transformMatrix = i == <span class="number">0</span> ? transformationMatrixFacing : transformationMatrix;</span><br><span class="line"></span><br><span class="line">triStream.Append(GenerateGrassVertex(pos, segmentWidth, segmentHeight, float2(<span class="number">0</span>,t), transformMatrix));</span><br><span class="line">triStream.Append(GenerateGrassVertex(pos, -segmentWidth, segmentHeight, float2(<span class="number">1</span>,t), transformMatrix));</span><br><span class="line">&#125;</span><br><span class="line">triStream.Append(GenerateGrassVertex(pos, <span class="number">0</span>, height, float2(<span class="number">0.5</span>,<span class="number">1</span>), transformationMatrix));</span><br></pre></td></tr></table></figure><p>画出来的叶片是这个样子：</p><p><img src="/images/RoystanGeometryShaderGrass/MGeometryGrass-7.png" width="400" alt="Shader Programs-7"/></p><p>再给画出来的叶片加一个前倾属性，将形态画成曲线，这样草地会显得更加自然：</p><p><img src="/images/RoystanGeometryShaderGrass/MGeometryGrass-8.png" width="400" alt="Shader Programs-8"/></p><h3 id="lighting-and-shadows">Lighting and shadows</h3><p>现在的草地形状已经基本完成了，之后要做的就是光影部分。</p><h4 id="casting-shadows">Casting shadows</h4><p>在Unity中想要投射阴影的话需要增加一个ShadowCasterPass，这样才能在光源生成shadowmap时写入深度，从而投射阴影。有一点需要注意，我们需要在这个pass中绘制出和之前相同的形状，也要使用geometryshader。这样，我们之前将vertex shader和geometry shader写入<code>CGINCLUDE</code> 块就有好处了，直接复用就可以。</p><p><img src="/images/RoystanGeometryShaderGrass/MGeometryGrass-9.png" width="300" alt="Shader Programs-9"/></p><h4 id="receiving-shadows">Receiving shadows</h4><p>Unity在将一个灯光的阴影渲染到shadowmap中后，会运行一个阴影收集(shadowcollecting)函数将阴影写入一个屏幕空间的贴图(screen spacetexture)。为了采样这个贴图，需要计算顶点的屏幕空间坐标。这个过程Unity已经帮我们做了大部分工作，使用内置的数据和函数可以比较简单的完成阴影接收。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// geometryOutput struct</span></span><br><span class="line">unityShadowCoord4 _ShadowCoord : TEXCOORD1;</span><br><span class="line"></span><br><span class="line"><span class="comment">// VertexOutput function</span></span><br><span class="line">o._ShadowCoord = ComputeScreenPos(o.pos);</span><br><span class="line"></span><br><span class="line"><span class="comment">// forwardbase pass fragment shader</span></span><br><span class="line"><span class="keyword">return</span> SHADOW_ATTENUATIO(i);</span><br></pre></td></tr></table></figure><p><img src="/images/RoystanGeometryShaderGrass/MGeometryGrass-10.png" width="400" alt="Shader Programs-10"/></p><p>此时，叶片会存在自阴影线性，一般都是通过bias来解决。MSAA造成的阴影问题这里也不展开讨论了。Unity也提供了函数：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">if</span> UNITY_PASS_SHADOWCASTER</span></span><br><span class="line">    o.pos = UnityApplyLinearShadowBias(o.pos);</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br></pre></td></tr></table></figure><h4 id="lighting">Lighting</h4><p>光照部分也不复杂，只给了一个符合兰伯特定律的漫反射光<spanclass="math inline">\(I=N \cdotL\)</span>。L表示世界空间下指向主光源的向量，Unity内置变量中有提供。N是表面法线，我们之前并未考虑，需要在几何着色器中从切线空间从头开始计算一个。</p><p>考虑最简单的情况，当叶片是一个三角形时，其切线空间的法向量的指向y轴负方向，即<code>float3(0,-1,0)</code>。因为叶片是双面都渲染的，所以两面的法向量是不同的，在渲染时应该考虑到。片段着色器的<code>VFACE</code>语义可以用来处理这中差异：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">float4 <span class="title function_">frag</span><span class="params">(geometryOutput i, fixed facing : VFACE)</span> : SV_Target</span><br><span class="line">&#123;</span><br><span class="line">    float3 normal = facing &gt; <span class="number">0</span> ? i.normal : -i.normal;</span><br><span class="line">    ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>当叶片的片段数大于1时，叶片会根据<code>forward</code>向前倾，让法向量产生一个z轴的值。我们可以直接用<code>forward</code>近似法向量产生的z轴值。让法线可视化：</p><p><img src="/images/RoystanGeometryShaderGrass/MGeometryGrass-11.png" width="400" alt="Shader Programs-11"/></p><p>至此，我们已经有了所有需要的变量，将灯光和阴影都考虑进去，便可以得到最后的颜色值。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> shadow = SHADOW_ATTENUATION(i);</span><br><span class="line"><span class="type">float</span> NdotL = saturate(saturate(dot(normal, _WorldSpaceLightPos0)) + _TranslucentGain) * shadow;</span><br><span class="line"></span><br><span class="line">float3 ambient = ShadeSH9(float4(normal, <span class="number">1</span>));</span><br><span class="line">float4 lightIntensity = NdotL * _LightColor0 + float4(ambient, <span class="number">1</span>);</span><br><span class="line">float4 col = lerp(_BottomColor, _TopColor * lightIntensity, i.uv.y);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> col;</span><br></pre></td></tr></table></figure><p>绿色看习惯了，调整一个颜色值，灯光太简单产生的效果影响不大：</p><p><img src="/images/RoystanGeometryShaderGrass/MGeometryGrass-12.png" width="500" alt="Shader Programs-12"/></p><h3 id="conclusion">Conclusion</h3><p>算是跟着教程走了一篇，基本清楚了Unity几何着色器的基本流程，甚至还顺便学了曲面细分着色器。正如开头说的一样，这种制作草地的方法在商业游戏中用的不多，商业游戏中还是倾向于使用面片草地，但也不是没有。之前看到对马岛之魂的草地好像就不是面片渲染的，而且美术效果还特别不错，就是不清楚是不是用到了几何着色器，之后有机会学习一下。</p><h3 id="参考文献">参考文献</h3><p><a href="https://roystan.net/articles/grass-shader/">GrassShader</a></p><p><ahref="https://learnopengl.com/Advanced-OpenGL/Geometry-Shader">OpenGLGeometry Shader</a></p><p><ahref="https://learnopengl.com/Advanced-Lighting/Normal-Mapping">OpenGLNormal Mapping</a></p><p><ahref="https://catlikecoding.com/unity/tutorials/advanced-rendering/tessellation/">CatlikeCodingTessellation</a></p><p><a href="https://github.com/keijiro/StandardGeometryShader">StandardGeometry Shader</a></p><p><ahref="https://discussions.unity.com/t/what-is-the-command-of-cginclude/556404">whatis the command of CGINCLUDE</a></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
            <tag> Geometry Shader </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnOpenGL - Geometry Shader</title>
      <link href="/2024/11/05/LearnOpenGL/Geometry%20Shader/"/>
      <url>/2024/11/05/LearnOpenGL/Geometry%20Shader/</url>
      
        <content type="html"><![CDATA[<h3 id="基础知识">基础知识</h3><p>在顶点着色器和片元着色器之间有一个可选的着色阶段，它就是几何着色器（geometryshader），其输入是一个图元（如点或三角形）的一组顶点。在这组顶点被传输到下个着色阶段之前，几何着色器可以随意处理它们。几何着色器有趣的地方在于，它可以将原始图元（一组顶点）变换为完全不同的图元，或者生成更多的顶点。下面通过展示一个几何着色器的例子，让你更深入的了解它：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line">layout (points) in;</span><br><span class="line">layout (line_strip, max_vertices = <span class="number">2</span>) out;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    gl_Position = gl_in[<span class="number">0</span>].gl_Position + vec4(<span class="number">-0.1</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);</span><br><span class="line">    EmitVertex();</span><br><span class="line"></span><br><span class="line">    gl_Position = gl_in[<span class="number">0</span>].gl_Position + vec4(<span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);</span><br><span class="line">    EmitVertex();</span><br><span class="line"></span><br><span class="line">    EndPrimitive();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在几何着色器的起始处，我们需要声明从顶点着色器中接收的图元类型（typeof promitive）。其可以是以下值：</p><ul><li>points : when drawing <strong>GL_POINTS</strong> primitive(1).</li><li>lines : when drawing <strong>GL_LINES</strong> or<strong>GL_LINE_STRIP</strong>(2).</li><li>lines_adjacency : <strong>GL_LINES_ADJACENCY</strong> or<strong>GL_LINE_STRIP_ADJACENCY</strong>(4).</li><li>triangles : <strong>GL_TRIANGLES</strong>,<strong>GL_TRIANGLE_STRIP</strong> or<strong>GL_TRIANGLE_FAN</strong>(3).</li><li>triangles_adjacency : <strong>GL_TRIANGLES_ADJACENCY</strong> or<strong>GL_TRIANGLE_STRIP_ADJACENCY</strong>(6).</li></ul><p>这些便是我们可以用作参数调用类似 glDrawArrays函数的几乎所有的渲染图元了。如果我们使用 GL_TRIANGLES来渲染顶点，我们需要把输入修饰符变为triangles，括号中的数字表示一个图元包含的最小顶点数。此外，我还需要为几何着色器的输出指定图元类型，可以从以下几种类型中进行选择：</p><ul><li>points</li><li>line_strip</li><li>triangle_strip</li></ul><p>通过这三种输出修饰符，我们几乎可以使用输入的图元来创建几乎任何形状。我们可以将输出定义为triangle_strip，并输出三个顶点，来生成一个三角形。</p><p>几何着色器还需要我们去设定一个输出的最大顶点数，OpenGL不会绘制多出的顶点，这个也可以在out 关键字的修饰符中设置。在例子中，我们将输出一个line_strip，并将最大顶点数设置为2。所谓line_strip，是指线条，线条链接了一组点，形成了一条连续的线，它最少由两个点来组成。在渲染函数中每多加一个点，就会在这个点与前一个点之间形成一条新的线。在下面这张图中，有5个顶点：</p><figure><img src="/images/LearnOpenGL/Advanced%20OpenGL/geometry_shader_1.png"alt="line strip" /><figcaption aria-hidden="true">line strip</figcaption></figure><p>为了生成更有意义的结果，我们需要一些方法拿到前着色阶段的输出。GLSL提供了一个名为gl_in 的内置变量，其构成可能如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">in gl_Vertex</span><br><span class="line">&#123;</span><br><span class="line">    vec4 gl_Position;</span><br><span class="line">    <span class="type">float</span> gl_PointSize;</span><br><span class="line">    <span class="type">float</span> gl_ClipDistance[];</span><br><span class="line">&#125; gl_in[];</span><br></pre></td></tr></table></figure><p>其被声明为一个数组，因为大多数渲染图元包含多个顶点，几何着色器的输入是一个图元的所有顶点。有了顶点数据，我们就可以使用几何着色器的两个函数，EmitVertex和EndPrimitive，来生成新数据了。几何着色器需要你生成并输出至少一个定义为输出的图元。在我们的例子中，我们需要至少生成一个linestrip。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line">layout (points) in;</span><br><span class="line">layout (line_strip, max_vertices = <span class="number">2</span>) out;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    gl_Position = gl_in[<span class="number">0</span>].gl_Position + vec4(<span class="number">-0.1</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);</span><br><span class="line">    EmitVertex();</span><br><span class="line"></span><br><span class="line">    gl_Position = gl_in[<span class="number">0</span>].gl_Position + vec4(<span class="number">0.1</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);</span><br><span class="line">    EmitVertex();</span><br><span class="line"></span><br><span class="line">    EndPrimitive();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>每次我们调用函数 EmitVertex时，gl_Position中的向量会被添加到图元中。然后，当函数EndPrimitive被调用时，此图元中所有被 emitted的顶点都会被联结成为输出的渲染图元。在调用多次 EmitVertex 函数后调用函数EndPrimitive，重复多次便能生成多个图元。在上面的例子中，我们将点偏移两次然后联结为线条图元。其效果，如下图：</p><figure><img src="/images/LearnOpenGL/Advanced%20OpenGL/geometry_shader_2.png"alt="lines" /><figcaption aria-hidden="true">lines</figcaption></figure><p>虽然目前并没有令人惊叹的效果，但考虑到这个函数是调用下面的渲染函数得到的，还是很有意思的：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">glDrawArrays(GL_POINTS, <span class="number">0</span>, <span class="number">4</span>);</span><br></pre></td></tr></table></figure><p>虽然这是一个简单的例子，但是它向你展示了如何使用几何着色器来动态生成新形状。在之后我们会利用几何着色器创建出更有意思的效果，但仍将从创建一个简单的几何着色器开始。</p><h3 id="using-geometry-shaders">Using geometry shaders</h3><p>为了展示几何着色器的用法，我们将渲染一个非常简单的场景，在NDC空间的z平面上绘制四个点。它们的坐标为：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> points[] = &#123;</span><br><span class="line">    <span class="number">-0.5f</span>,  <span class="number">0.5f</span>, <span class="comment">// top-left</span></span><br><span class="line">     <span class="number">0.5f</span>,  <span class="number">0.5f</span>, <span class="comment">// top-right</span></span><br><span class="line">     <span class="number">0.5f</span>, <span class="number">-0.5f</span>, <span class="comment">// bottom-right</span></span><br><span class="line">    <span class="number">-0.5f</span>, <span class="number">-0.5f</span>  <span class="comment">// bottom-left</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>顶点着色器需要将顶点绘制在z平面上，所以我们创建一个基础的顶点着色器。其次，我们会将所有顶点在片元着色器中输出为绿色。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// vertex shader</span></span><br><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line">layout (location = <span class="number">0</span>) in vec2 aPos;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    gl_Postion = vec4(aPos.x, aPos.y, <span class="number">0.0</span>, <span class="number">1.0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// fragment shader</span></span><br><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line">out vec4 FragColor;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    FragColor = vec4(<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>为点的顶点数据生成一个VAO和一个VBO，然后使用 glDrawArrays进行绘制：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">shader.use();</span><br><span class="line">glBindVertexArray(VAO);</span><br><span class="line">glDrawArrays(GL_POINTS, <span class="number">0</span>, <span class="number">4</span>);</span><br></pre></td></tr></table></figure><p>结果是在黑暗的场景中出现四个绿色的点：</p><figure><img src="/images/LearnOpenGL/Advanced%20OpenGL/geometry_shader_3.png"alt="points" /><figcaption aria-hidden="true">points</figcaption></figure><p>现在我们将添加一个几何着色器，为场景增添活力。出于学习目的，我们将会创建一个传递（Pass-through）几何着色器，它会接收一个点图元，并直接将它传递（Pass）到下一个着色阶段：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line">layout (points) in;</span><br><span class="line">layout (points, max_vertices = <span class="number">1</span>) out;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    gl_Position = gl_in[<span class="number">0</span>].gl_Position;</span><br><span class="line">    EmitVertex();</span><br><span class="line">    EndPrimitive();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现在这个几何着色器应该非常容易理解，它只是简单的提交了接收到的顶点位置生成了一个点图元。几何着色器需要像顶点着色器、片元着色器一样被编译链接到程序中，但是这次在创建着色器时我们将会使用GL_GEOMETRY_SHADER 作为着色器类型：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">geometryShader = glCreateShader(GL_GEOMETRY_SHADER);</span><br><span class="line">glShaderSource(geometryShader, <span class="number">1</span>, &amp;gShaderCode, <span class="literal">NULL</span>);</span><br><span class="line">glCompileShader(geometryShader);</span><br><span class="line">...</span><br><span class="line">glAttachShader(program, geometryShader);</span><br><span class="line">glLinkProgram(program);</span><br></pre></td></tr></table></figure><p>目前渲染的结果还是和没有加入几何着色器前一样，只要没有编译和链接错误就好。</p><h3 id="lets-build-houses">Let's build houses</h3><p>绘制点和线多少有点无趣了，现在我们要使用几何着色器在每个点的位置绘制一个房子。我们需要将几何着色器的输出类型改为triangle_strip，然后每个点处画三个三角形：两个作为方形，一个作为屋顶。</p><p>OpenGL中，三角形带（trianglestrip）是高效地绘制三角形的方式，使用的顶点少。在第一个三角形绘制完之后，每个后续顶点将会在上一个三角形边上生成另一个三角形：每三个相邻的顶点将会形成一个三角形。如果我们一共有6个构成三角形带的顶点，那么我们会得到这些三角形：（1,2,3）（2,3,4）（3,4,5）和（4,5,6），共四个三角形。一个三角形带至少需要3个顶点，并会生成N-2 个三角形。下图可以很好展现这点：</p><figure><img src="/images/LearnOpenGL/Advanced%20OpenGL/geometry_shader_4.png"alt="triangle strip" /><figcaption aria-hidden="true">triangle strip</figcaption></figure><p>通过是使用三角形带作为几何着色器的输出，我们可以很容易的创建需要的房子形状，只需要以正确的顺序生成3个相连的三角形即可。下面这幅图展示了顶点绘制的顺序，蓝点表示输入点：</p><figure><img src="/images/LearnOpenGL/Advanced%20OpenGL/geometry_shader_5.png"alt="house" /><figcaption aria-hidden="true">house</figcaption></figure><p>将图片转换为代码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line">layout (points) in;</span><br><span class="line">layout (triangle_strip, max_vertices = <span class="number">5</span>) out;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">build_house</span><span class="params">(vec4 position)</span></span><br><span class="line">&#123;</span><br><span class="line">    gl_Position = position + vec4(<span class="number">-0.2</span>, <span class="number">-0.2</span>, <span class="number">0.0</span>, <span class="number">0.0</span>); <span class="comment">// 1:bottom-left</span></span><br><span class="line">    EmitVertex();</span><br><span class="line">    gl_Position = position + vec4( <span class="number">0.2</span>, <span class="number">-0.2</span>, <span class="number">0.0</span>, <span class="number">0.0</span>); <span class="comment">// 2:bottom-right</span></span><br><span class="line">    EmitVertex();</span><br><span class="line">    gl_Position = position + vec4(<span class="number">-0.2</span>,  <span class="number">0.2</span>, <span class="number">0.0</span>, <span class="number">0.0</span>); <span class="comment">// 3:top-left</span></span><br><span class="line">    EmitVertex();</span><br><span class="line">    gl_Position = position + vec4( <span class="number">0.2</span>,  <span class="number">0.2</span>, <span class="number">0.0</span>, <span class="number">0.0</span>); <span class="comment">// 4:top-left</span></span><br><span class="line">    EmitVertex();</span><br><span class="line">    gl_Position = position + vec4( <span class="number">0.2</span>,  <span class="number">0.4</span>, <span class="number">0.0</span>, <span class="number">0.0</span>); <span class="comment">// 5:top</span></span><br><span class="line">    EmitVertex();</span><br><span class="line">    EndPrimitive();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span>&#123;</span><br><span class="line">    build_house(gl_in[<span class="number">0</span>].gl_Position);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这个几何着色器生成了5个顶点，每个顶点都是原始点的位置加上一个偏移量，来组成一个大的三角带。最终的图元会被光栅化，然后片段着色器处理整个三角形带，最终在每个绘制的点处生成绿色的房子：</p><figure><img src="/images/LearnOpenGL/Advanced%20OpenGL/geometry_shader_6.png"alt="house" /><figcaption aria-hidden="true">house</figcaption></figure><p>可以看到每个房子都是由三个三角形组成的，而且都是通过空间中的一点生成的。现在，我们给每个房子一个独特的颜色。为此我们需要在顶点着色器中添加一个额外的顶点属性，将其传递至几何着色器，并再次传递到片段着色器。下面是更新后的顶点数据：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> points[] = &#123;</span><br><span class="line">    <span class="number">-0.5f</span>,  <span class="number">0.5f</span>, <span class="number">1.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="comment">// 左上</span></span><br><span class="line">     <span class="number">0.5f</span>,  <span class="number">0.5f</span>, <span class="number">0.0f</span>, <span class="number">1.0f</span>, <span class="number">0.0f</span>, <span class="comment">// 右上</span></span><br><span class="line">     <span class="number">0.5f</span>, <span class="number">-0.5f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">1.0f</span>, <span class="comment">// 右下</span></span><br><span class="line">    <span class="number">-0.5f</span>, <span class="number">-0.5f</span>, <span class="number">1.0f</span>, <span class="number">1.0f</span>, <span class="number">0.0f</span>  <span class="comment">// 左下</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>然后更新顶点着色器，使用一个接口块将颜色属性发送到几何着色器中：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line">layout (location = <span class="number">0</span>) in vec2 aPos;</span><br><span class="line">layout (location = <span class="number">1</span>) in vec3 aColor;</span><br><span class="line"></span><br><span class="line">out VS_OUT &#123;</span><br><span class="line">    vec3 color;</span><br><span class="line">&#125; vs_out;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    gl_Position = vec4(aPos.x, aPos.y, <span class="number">0.0</span>, <span class="number">1.0</span>); </span><br><span class="line">    vs_out.color = aColor;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来我们还需要在几何着色器中声明相同的接口块（使用一个不同的接口名）：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">in VS_OUT &#123;</span><br><span class="line">    vec3 color;</span><br><span class="line">&#125; gs_in[];</span><br></pre></td></tr></table></figure><p>接下来还需要为下个片段着色器声明一个输出颜色变量：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">out vec3 fColor;</span><br></pre></td></tr></table></figure><p>因为片段着色器只需要一个插值过的颜色，传输多个颜色值是没有意义的。所以fColor 不是一个数组，而是一个向量。当提交（emit）一个顶点时，会储存fColor 中的值作为顶点输出值。我们可以在第一个顶点提交之前先填充 fColor一次，这样绘制出的房子就会被填充上 fColor 最后存储的颜色。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">fColor = gs_in[<span class="number">0</span>].color; <span class="comment">// gs_in[0] since there&#x27;s only one input vertex</span></span><br><span class="line">gl_Position = position + vec4(<span class="number">-0.2</span>, <span class="number">-0.2</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);    <span class="comment">// 1:bottom-left   </span></span><br><span class="line">EmitVertex();   </span><br><span class="line">gl_Position = position + vec4( <span class="number">0.2</span>, <span class="number">-0.2</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);    <span class="comment">// 2:bottom-right</span></span><br><span class="line">EmitVertex();</span><br><span class="line">gl_Position = position + vec4(<span class="number">-0.2</span>,  <span class="number">0.2</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);    <span class="comment">// 3:top-left</span></span><br><span class="line">EmitVertex();</span><br><span class="line">gl_Position = position + vec4( <span class="number">0.2</span>,  <span class="number">0.2</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);    <span class="comment">// 4:top-right</span></span><br><span class="line">EmitVertex();</span><br><span class="line">gl_Position = position + vec4( <span class="number">0.0</span>,  <span class="number">0.4</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);    <span class="comment">// 5:top</span></span><br><span class="line">EmitVertex();</span><br><span class="line">EndPrimitive();</span><br></pre></td></tr></table></figure><figure><img src="/images/LearnOpenGL/Advanced%20OpenGL/geometry_shader_7.png"alt="houses colored" /><figcaption aria-hidden="true">houses colored</figcaption></figure><h3 id="exploding-object-visualizing-normal-vectors">Exploding object&amp; Visualizing normal vectors</h3><p>几何着色器除了绘制小房子外，当然还有其他应用。可以看看原文中的其他两个例子，当然这两个例子也不是用的很多就是了，但是可以作为参考。</p><h3 id="参考文献">参考文献</h3><p><ahref="https://learnopengl.com/Advanced-OpenGL/Geometry-Shader">GeometryShader</a></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shader </tag>
            
            <tag> OpenGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>毛发渲染</title>
      <link href="/2024/10/28/Game%20Development/%E6%AF%9B%E5%8F%91%E6%B8%B2%E6%9F%93/"/>
      <url>/2024/10/28/Game%20Development/%E6%AF%9B%E5%8F%91%E6%B8%B2%E6%9F%93/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>Unity中LinearEyeDepth Linear01Depth推导</title>
      <link href="/2024/10/25/Game%20Development/Unity%E4%B8%ADLinearEyeDepth%20Linear01Depth%E6%8E%A8%E5%AF%BC/"/>
      <url>/2024/10/25/Game%20Development/Unity%E4%B8%ADLinearEyeDepth%20Linear01Depth%E6%8E%A8%E5%AF%BC/</url>
      
        <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>Unity开发中会利用深度缓存做很多事情，了解深度缓存的由来细节可以让开发过程更顺利，本文讨论不同平台下渲染管线中深度值的由来以及LinearEyeDepthLinear01Depth函数的输出值细节。</p><h3 id="概念说明">概念说明</h3><ul><li><span class="math inline">\(f\)</span>远裁剪面到相机的距离，正值</li><li><span class="math inline">\(n\)</span>近裁剪面到相机的距离，正值</li><li><span class="math inline">\(z_{eye}\)</span> 视角空间（viewspace）中坐标的z分量</li><li><span class="math inline">\(z_{clip}\)</span>齐次裁剪空间（homogeneous clip space）中坐标的z分量</li><li><span class="math inline">\(w_{clip}\)</span>齐次裁剪空间（homogeneous clip space）中坐标的w分量</li><li><span class="math inline">\(z_{ndc}\)</span>NDC空间中坐标的z分量</li><li><span class="math inline">\(z_{depth}\)</span> depthbuffer中的深度值</li><li><span class="math inline">\(z_{linear}\)</span> 视角空间（viewspace）中顶点坐标在远近裁剪平面之间的线性值，近裁剪面处值为0，远裁剪面处值为1</li><li><span class="math inline">\(z_{linear2}\)</span> 视角空间（viewspace）中顶点坐标在相机和远裁剪平面之间的线性值，相机处值为0，远裁剪面处值为1</li></ul><h4 id="view-space">View Space</h4><p><ahref="https://docs.unity3d.com/2021.3/Documentation/ScriptReference/Camera-worldToCameraMatrix.html">Camera.worldToCameraMatrix</a></p><p>世界空间（world space）不必细说，Unity使用的viewspace是OpenGL-like的，为右手坐标系，相机看向z轴负方向。相机的worldToCameraMatrix 即空间变换中的 view matrix，将坐标从worldspace转换到view space。需要注意的是，OpenGL-like view space 中的 <spanclass="math inline">\(z_{eye}\)</span> 是负数。</p><h4 id="clip-space">Clip Space</h4><p><ahref="https://docs.unity3d.com/ScriptReference/GL.GetGPUProjectionMatrix.html">GL.GetGPUProjectionMatrix</a></p><p>view space 中的坐标经过投影变换后就来到了裁剪空间（clipspace），距离最后深度值还有几步操作。由于平台差异，变换使用的 projectionmatrix 也不尽相同。于是，Unity封装了函数 GL.GetGPUProjectionMatrix来处理平台差异。</p><h3 id="perspective-projection">Perspective Projection</h3><h4 id="opengl-like">OpenGL-like</h4><p>我们先来看类OpenGL平台下的深度值推导，例如 OpenGL 和 OpenGLES。OpenGL-like的投影矩阵推导这里不展开了，我们直接使用结果，推导过程可以看这篇文章<ahref="https://www.songho.ca/opengl/gl_projectionmatrix.html">OpenGLProjection Matrix</a>。</p><p>文章中已经得到的 <span class="math inline">\(z_{eye}\)</span> 与<span class="math inline">\(z_{ndc}\)</span> 的关系式为：</p><p><span class="math display">\[\begin{gather*}w_{clip} = -z_{eye} \\z_{clip} = \frac{f+n}{n-f}z_{eye} + \frac{2nf}{n-f} \\z_{ndc} = \frac{z_{clip}}{w_{clip}} = \frac{f+n}{f-n} +\frac{2fn}{(f-n)z_{eye}}\end{gather*}\]</span></p><p>此时 <span class="math inline">\(z_{ndc}\)</span>范围为(-1,1)，要将其写入深度缓存还需要做一次映射到(0,1)：</p><p><span class="math display">\[z_{depth} = \frac{z_{ndc}+1}{2} = \frac{f}{f-n} +\frac{fn}{(f-n)z_{eye}}\]</span></p><p>那么，用 <span class="math inline">\(z_{depth}\)</span> 得到 <spanclass="math inline">\(z_{eye}\)</span> 公式如下：</p><p><span class="math display">\[z_{eye} = \frac{1}{    z_{depth} \frac{f-n}{fn} - \frac{1}{n}}\]</span></p><p>使用深度值 <span class="math inline">\(z_{depth}\)</span>计算剩余两个经常使用的值 <span class="math inline">\(z_{linear},z_{linear2}\)</span> 推到导为：</p><p><span class="math display">\[\begin{gather*}z_{linear} = \frac{-z_{eye}-n}{f-n} =\frac{nz_{depth}}{(n-f)z_{depth}+f} = \frac{1}{    1-\frac{f}{n} + \frac{f}{nz_{depth}}} \\z_{linear2} = \frac{-z_{eye}}{f} = \frac{1}{    z_{depth}(1-\frac{f}{n}) + \frac{f}{n}}\end{gather*}\]</span></p><h4 id="direct3d-like">Direct3D-like</h4><p>类Direct3D平台的深度比类OpenGL复杂一点，其为了让深度缓存更有效率的利用进行了ReversedZ操作，所以我们要考虑两种情况。除此之外，类Direct3D平台的 <spanclass="math inline">\(z_{ndc}\)</span>范围就是(0,1)，不需要像类openGL一样做映射。注意，由于Unity中viewspace使用的是右手坐标系，所以和典型的Direct3D下的投影矩阵还是有所不同的，此时还有<span class="math inline">\(w_{clip} = -z_{eye}\)</span> 。</p><p><strong>未应用 Reversed Z</strong></p><p>还可以使用之前贴的文章里面的方法求得 <spanclass="math inline">\(z_{ndc}\)</span> 与 <spanclass="math inline">\(z_{eye}\)</span> 关系：</p><p><span class="math display">\[\begin{gather*}w_{clip} = -z_{eye} \\z_{clip} = \frac{f}{n-f}z_{eye} + \frac{nf}{n-f} \\z_{ndc} = \frac{z_{clip}}{w_{clip}} = \frac{f}{f-n} +\frac{fn}{(f-n)z_{eye}}\end{gather*}\]</span></p><p><span class="math inline">\(z_{ndc}\)</span>的范围已经是(0,1)了，可以直接写入深度缓存，所以 <spanclass="math inline">\(z_{depth} = z_{ndc}\)</span>。由于 <spanclass="math inline">\(z_{depth}\)</span> 和 <spanclass="math inline">\(z_{eye}\)</span> 的关系和类OpenGL平台相同，所以<span class="math inline">\(z_{linear}, z_{linear2}\)</span>的表达式也与之相同。</p><p><strong>应用 Reversed Z</strong></p><p>目前Unity上只有类Direct3D才会支持ReversedZ，其和普通情况下的主要区别是，近裁剪面处的深度值为1，远裁剪面处的深度值为0。投影矩阵的推导思路还与之前相同，<span class="math inline">\(z_{ndc}\)</span> 与 <spanclass="math inline">\(z_{eye}\)</span> 关系为：</p><p><span class="math display">\[\begin{gather*}w_{clip} = -z_{eye} \\z_{clip} = \frac{n}{f-n}z_{eye} + \frac{nf}{f-n} \\z_{ndc} = \frac{z_{clip}}{w_{clip}} = \frac{n}{n-f} +\frac{nf}{(n-f)z_{eye}}\end{gather*}\]</span></p><p><span class="math inline">\(z_{ndc}\)</span>虽然被映射到了(1,0)，但是也可以直接写入深度缓存，所以 <spanclass="math inline">\(z_{depth} = z_{ndc}\)</span>。此时<spanclass="math inline">\(z_{depth}\)</span> 和 <spanclass="math inline">\(z_{eye}\)</span> 的关系与之前不再相同:</p><p><span class="math display">\[\begin{gather*}z_{depth} = z_{ndc} = \frac{n}{n-f} + \frac{nf}{(n-f)z_{eye}} \\z_{eye} = \frac{1}{    z_{depth}\frac{n-f}{nf} - \frac{1}{f}}\end{gather*}\]</span></p><p><span class="math inline">\(z_{linear}, z_{linear2}\)</span>需要重新推导：</p><p><span class="math display">\[\begin{gather*}z_{linear} = \frac{-z_{eye}-n}{f-n} = \frac{1}{    1-\frac{f}{n} + \frac{f}{n(1-z_{depth})}}\\z_{linear2} = \frac{-z_{eye}}{f} = \frac{1}{z_{depth} \frac{f-n}{n} + 1}\end{gather*}\]</span></p><h3 id="unity-srp-工具函数">Unity SRP 工具函数</h3><p>Unity推出的SRP中封装了很多工具函数，里面就有计算 <spanclass="math inline">\(z_{eye}, z_{linear}, z_{linear2}\)</span>的函数，刚好可以验证一下上文中的推导结果。计算过程中使用到了**_ZBufferParams**这个变量，我们先来简单看一下它，其在代码中拥有以下注释：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Values used to linearize the Z buffer (http://www.humus.name/temp/Linearize%20depth.txt)</span></span><br><span class="line"><span class="comment">// x = 1-far/near</span></span><br><span class="line"><span class="comment">// y = far/near</span></span><br><span class="line"><span class="comment">// z = x/far</span></span><br><span class="line"><span class="comment">// w = y/far</span></span><br><span class="line"><span class="comment">// or in case of a reversed depth buffer (UNITY_REVERSED_Z is 1)</span></span><br><span class="line"><span class="comment">// x = -1+far/near</span></span><br><span class="line"><span class="comment">// y = 1</span></span><br><span class="line"><span class="comment">// z = x/far</span></span><br><span class="line"><span class="comment">// w = 1/far</span></span><br><span class="line">float4 _ZBufferParams;</span><br></pre></td></tr></table></figure><p>代码里面的注释基本已经把这个变量的含义写的清清楚楚，开发人员真贴心，它就是用来处理深度缓存的辅助变量。再去看一眼在渲染管线里面它被填入的具体内容，代码在ScriptableRenderer.cs 中：</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// in SetPerCameraShaderVariables</span></span><br><span class="line"><span class="built_in">float</span> near = camera.nearClipPlane;</span><br><span class="line"><span class="built_in">float</span> far = camera.farClipPlane;</span><br><span class="line"><span class="built_in">float</span> invNear = Mathf.Approximately(near, <span class="number">0.0f</span>) ? <span class="number">0.0f</span> : <span class="number">1.0f</span> / near;</span><br><span class="line"><span class="built_in">float</span> invFar = Mathf.Approximately(far, <span class="number">0.0f</span>) ? <span class="number">0.0f</span> : <span class="number">1.0f</span> / far;</span><br><span class="line"><span class="built_in">float</span> isOrthographic = camera.orthographic ? <span class="number">1.0f</span> : <span class="number">0.0f</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// From http://www.humus.name/temp/Linearize%20depth.txt</span></span><br><span class="line"><span class="comment">// But as depth component textures on OpenGL always return in 0..1 range (as in D3D), we have to use</span></span><br><span class="line"><span class="comment">// the same constants for both D3D and OpenGL here.</span></span><br><span class="line"><span class="comment">// OpenGL would be this:</span></span><br><span class="line"><span class="comment">// zc0 = (1.0 - far / near) / 2.0;</span></span><br><span class="line"><span class="comment">// zc1 = (1.0 + far / near) / 2.0;</span></span><br><span class="line"><span class="comment">// D3D is this:</span></span><br><span class="line"><span class="built_in">float</span> zc0 = <span class="number">1.0f</span> - far * invNear;</span><br><span class="line"><span class="built_in">float</span> zci = far * invNear;</span><br><span class="line"></span><br><span class="line">Vector4 zBufferParams = <span class="keyword">new</span> Vector4(zc0, zc1, zc0 * invFar, zc1 * invFar);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(SystemInfo.usesReversedZBuffer)</span><br><span class="line">&#123;</span><br><span class="line">    zBufferParams.y += zBufferParamszBufferParam.x;</span><br><span class="line">    zBufferParams.x = -zBufferParamszBufferParam.x;</span><br><span class="line">    zBufferParams.w += zBufferParamszBufferParam.z;</span><br><span class="line">    zBufferParams.z = -zBufferParamszBufferParam.z;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代码注释中说明，由于 OpenGL-like 和 D3D-like的z坐标最终都被映射到了深度缓存中的(0,1)范围，虽然过程中的投影矩阵不同，但正如我们上文中推导的结果表明<span class="math inline">\(z_{eye}\)</span> 与 <spanclass="math inline">\(z_{depth}\)</span>两者的转换关系是相同的，所以这里 OpenGL-like 和 D3D-like使用的参数都是相同的。当然，在 Reversed Z情况下，两者关系发生了变化，所以代码也做了差异处理。其最终填入的结果和变量声明的注释内容一致。</p><p>在非 Reversed Z 情况下，zBufferParams中各分量为：</p><p><span class="math display">\[zBufferParams = (\frac{n-f}{n}, \frac{f}{n}, \frac{n-f}{nf},\frac{1}{n})\]</span></p><p>在 Reversed Z 情况下，zBufferParams中各分量为：</p><p><span class="math display">\[zBufferParams = (\frac{f-n}{n}, 1, \frac{f-n}{nf}, \frac{1}{f})\]</span></p><p>现在，对Common.hlsl中的工具函数做分析便不会有阻碍。先来看看其中的LinearEyeDepth函数，函数重载有不同的使用场景，至于为什么如此做下文慢慢分析。</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Z buffer to linear depth.</span></span><br><span class="line"><span class="comment">// Does NOT correctly handle oblique view frustums.</span></span><br><span class="line"><span class="comment">// Does NOT work with orthographic projection.</span></span><br><span class="line"><span class="comment">// zBufferParam = &#123;(f-n)/n, 1, (f-n)/n*f, 1/f&#125;</span></span><br><span class="line"><span class="function"><span class="built_in">float</span> <span class="title">LinearEyeDepth</span>(<span class="params"><span class="built_in">float</span> depth, float4 zBufferParam</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (zBufferParam.z * depth + zBufferParam.w);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>不知道开发人员写在注释中的zBufferParam是什么意思，按理zBufferParam应该有上文中的那两种情况，并不是固定值。而且两种情况下函数返回的值都是<span class="math inline">\(-z_{eye}\)</span> 。由于view space中的 <spanclass="math inline">\(z_{eye}\)</span> 是负值，所以函数返回的 <spanclass="math inline">\(-z_{eye}\)</span> 是一个正值，是像素点在viewspace中相对相机x-y平面的距离。公式推导如下：</p><p><span class="math display">\[\begin{gather*}value = \frac{1}{    z_{depth}\frac{n-f}{nf} + \frac{1}{n}    } = -z_{eye} \\value_{reversedZ} = \frac{1}{    z_{depth}\frac{f-n}{nf} + \frac{1}{f}} = -z_{eye}\end{gather*}\]</span></p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Z buffer to linear depth.</span></span><br><span class="line"><span class="comment">// Correctly handles oblique view frustums.</span></span><br><span class="line"><span class="comment">// Does NOT work with orthographic projection.</span></span><br><span class="line"><span class="comment">// Ref: An Efficient Depth Linearization Method for Oblique View Frustums, Eq. 6.</span></span><br><span class="line"><span class="function"><span class="built_in">float</span> <span class="title">LinearEyeDepth</span>(<span class="params">float2 positionNDC, <span class="built_in">float</span> deviceDepth, float4 invProjParam</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">    float4 positionCS = float4(positionNDC * <span class="number">2.0</span> - <span class="number">1.0</span>, deviceDepth, <span class="number">1.0</span>);</span><br><span class="line">    <span class="built_in">float</span> viewSpaceZ = rcp(dot(positionCS, invProjParam));</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If the matrix is right-handed, we have to flip the Z axis to get a position value.</span></span><br><span class="line">    <span class="keyword">return</span> abs(viewSpaceZ);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>LinearEyeDepth其中一个重载函数，适用于Oblique ViewFrustums，但不适用于正交投影。说实话，我没太看懂其中的逻辑，URP中也没有使用案例，就暂且搁置在这里了。</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Z buffer to linear depth.</span></span><br><span class="line"><span class="comment">// Works in all cases.</span></span><br><span class="line"><span class="comment">// Typically, this is the cheapest variant, provided you&#x27;ve already computed &#x27;positionWS&#x27;.</span></span><br><span class="line"><span class="comment">// Assumes that the &#x27;positionWS&#x27; is in front of the camera.</span></span><br><span class="line"><span class="function"><span class="built_in">float</span> <span class="title">LinearEyeDepth</span>(<span class="params">float3 positionWS, float4x4 viewMatrix</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">float</span> viewSpaceZ = mul(viewMatrix, float4(positionWS, <span class="number">1.0</span>)).z;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If the matrix is right-handed, we have to flip the Z axis to get a position value.</span></span><br><span class="line">    <span class="keyword">return</span> abs(viewSpaceZ);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>LinearEyeDepth的另一个重载函数，适用于所有情况，因为他就是拿渲染管线流程来套的。使用视角矩阵变换世界坐标得到视角坐标，考虑到右手坐标系的负值情况，取绝对值就行。</p><p>我们来看下一个函数Linear01DepthFromNear，其返回一个范围(0,1)的线性值，在近裁剪面处为0，远裁剪面处为1，也就是上文中的<span class="math inline">\(z_{linear}\)</span> 。</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Z buffer to linear 0..1 depth (0 at near plane, 1 at far plane).</span></span><br><span class="line"><span class="comment">// Does NOT correctly handle oblique view frustums.</span></span><br><span class="line"><span class="comment">// Does NOT work with orthographic projection.</span></span><br><span class="line"><span class="comment">// zBufferParam = &#123; (f-n)/n, 1, (f-n)/n*f, 1/f &#125;</span></span><br><span class="line"><span class="function"><span class="built_in">float</span> <span class="title">Linear01DepthFromNear</span>(<span class="params"><span class="built_in">float</span> depth, float4 zBufferParam</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (zBufferParam.x + zBufferParam.y / depth);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将zBufferParam带入其中，在普通情况下，函数中公式与上文 <spanclass="math inline">\(z_{linear}\)</span> 推导结果一致。如下：</p><p><span class="math display">\[value = \frac{1}{    \frac{n-f}{n} + \frac{f}{z_{depth}n}} = z_{linear}\]</span></p><p>问题出现在 Reversed Z情况下，函数中公式和上文推导结果并不相同。也就是说，在 Reversed Z情况下，这个函数无法使用，其返回值并不是代码中注释那样。（去网上查了查好像只有本文的直接参考文献说指出了这个问题，也可能是查找方式不对，先不管了，相信以后自己的智慧）函数中代入参数结果如下：</p><p><span class="math display">\[value_{reversedZ} = \frac{1}{    \frac{f-n}{n} + \frac{1}{z_{depth}}}\]</span></p><p>下一个函数是Linear01Depth，其也返回一个范围(0,1)的线性值，但在相机处为0，远裁剪面处为1，也就是上文中的<span class="math inline">\(z_{linear2}\)</span> 。</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Z buffer to linear 0..1 depth (0 at camera position, 1 at far plane).</span></span><br><span class="line"><span class="comment">// Does NOT work with orthographic projections.</span></span><br><span class="line"><span class="comment">// Does NOT correctly handle oblique view frustums.</span></span><br><span class="line"><span class="comment">// zBufferParam = &#123; (f-n)/n, 1, (f-n)/n*f, 1/f &#125;</span></span><br><span class="line"><span class="function"><span class="built_in">float</span> <span class="title">Linear01Depth</span>(<span class="params"><span class="built_in">float</span> depth, float4 zBufferParam</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1.0</span> / (zBufferParam.x * depth + zBufferParam.y);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>将zBufferParam带入其中，在普通和 Reversed Z两种情况下，函数中公式得到的结果都与对应情况下上文中的推导结果 <spanclass="math inline">\(z_{linear2}\)</span> 一致，如下：</p><p><span class="math display">\[\begin{gather*}value = \frac{1}{    z_{depth}\frac{n-f}{n} + \frac{f}{n}    } = z_{linear2} \\value_{reversedZ} = \frac{1}{    z_{depth}\frac{f-n}{n} + 1    } = z_{linear2}\end{gather*}\]</span></p><h3 id="小结">小结</h3><p>这篇文章主要针对的是透视投影，正交投影中 <spanclass="math inline">\(z_{eye}\)</span> 和 <spanclass="math inline">\(z_{depth}\)</span>是线性关系，用的是另外的函数，涉及到 _ProjectionParams 和LinearDepthToEyeDepth函数。文章整体把透视投影下相关的函数都推导了一下，也留下了两个问题。</p><p>一是 Reversed Z 情况下，Linear01DepthFromNear函数输出值和我们推导出的<span class="math inline">\(z_{linear}\)</span>并不一致。二是，开发人员留下的文档<ahref="http://www.humus.name/temp/Linearize%20depth.txt">LinearizeDepth</a>里面公式推导没给前提条件，我也没看懂怎么推出来的。</p><p>就先这样吧，遗留的问题和正交投影等以后有需要的时候在细究。</p><h3 id="参考文献">参考文献</h3><p><ahref="https://zhuanlan.zhihu.com/p/393643084">Unity中深度值推导</a></p><p><ahref="https://www.songho.ca/opengl/gl_projectionmatrix.html">OpenGLProjection Matrix</a></p><p><ahref="https://learn.microsoft.com/en-us/windows/win32/direct3d9/projection-transform">ProjectionTransform(Direct3D 9)</a></p><p><ahref="https://docs.unity3d.com/2021.3/Documentation/Manual/SL-PlatformDifferences.html">Writingshaders for different graphics APIs</a></p><p><a href="https://www.cyanilux.com/tutorials/depth/">Depth |Cyanilux</a></p><p><a href="http://www.humus.name/temp/Linearize%20depth.txt">SRPdeveloper math</a></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
            <tag> Z-Buffer </tag>
            
            <tag> LinearDepth </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unity从深度缓存重建世界坐标</title>
      <link href="/2024/10/25/Game%20Development/Unity%E4%BB%8E%E6%B7%B1%E5%BA%A6%E7%BC%93%E5%AD%98%E9%87%8D%E5%BB%BA%E4%B8%96%E7%95%8C%E5%9D%90%E6%A0%87/"/>
      <url>/2024/10/25/Game%20Development/Unity%E4%BB%8E%E6%B7%B1%E5%BA%A6%E7%BC%93%E5%AD%98%E9%87%8D%E5%BB%BA%E4%B8%96%E7%95%8C%E5%9D%90%E6%A0%87/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>如今使用的游戏渲染技术中，有相当一部分技术有通过深度缓冲重建像素点世界坐标的需求。可以通过不同的方法来实现这一需求，下面我们来介绍几种常用的重建世界坐标方法及其原理。</p><h2id="通过ndc空间重建世界坐标unity-urp-案例">通过NDC空间重建世界坐标（UnityURP 案例）</h2><p>Unity提供了一个重建世界坐标的URP案例，可以从这里入手分析一下，直接来看它的片元着色器：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line">half4 <span class="title function_">frag</span><span class="params">(Varyings IN)</span> : SV_Target</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// To calculate the UV coordinates for sampling the depth buffer,</span></span><br><span class="line">    <span class="comment">// divide the pixel location by the render target resolution</span></span><br><span class="line">    <span class="comment">// _ScaledScreenParams.</span></span><br><span class="line">    float2 UV = IN.positionHCS.xy / _ScaledScreenParams.xy;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Sample the depth from the Camera depth texture.</span></span><br><span class="line">    <span class="meta">#<span class="keyword">if</span> UNITY_REVERSED_Z</span></span><br><span class="line">        real depth = SampleSceneDepth(UV);</span><br><span class="line">    <span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">        <span class="comment">// Adjust Z to match NDC for OpenGL ([-1, 1])</span></span><br><span class="line">        real depth = lerp(UNITY_NEAR_CLIP_VALUE, <span class="number">1</span>, SampleSceneDepth(UV));</span><br><span class="line">    <span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// Reconstruct the world space positions.</span></span><br><span class="line">    float3 worldPos = ComputeWorldSpacePosition(UV, depth, UNITY_MATRIX_I_VP);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// The following part creates the checkerboard effect.</span></span><br><span class="line">    <span class="comment">// Scale is the inverse size of the squares.</span></span><br><span class="line">    uint scale = <span class="number">10</span>;</span><br><span class="line">    <span class="comment">// Scale, mirror and snap the coordinates.</span></span><br><span class="line">    uint3 worldIntPos = uint3(<span class="built_in">abs</span>(worldPos.xyz * scale));</span><br><span class="line">    <span class="comment">// Divide the surface into squares. Calculate the color ID value.</span></span><br><span class="line">    <span class="type">bool</span> white = ((worldIntPos.x) &amp; <span class="number">1</span>) ^ (worldIntPos.y &amp; <span class="number">1</span>) ^ (worldIntPos.z &amp; <span class="number">1</span>);</span><br><span class="line">    <span class="comment">// Color the square based on the ID value (black or white).</span></span><br><span class="line">    half4 color = white ? half4(<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>) : half4(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Set the color to black in the proximity to the far clipping</span></span><br><span class="line">    <span class="comment">// plane.</span></span><br><span class="line">    <span class="meta">#<span class="keyword">if</span> UNITY_REVERSED_Z</span></span><br><span class="line">        <span class="comment">// Case for platforms with REVERSED_Z, such as D3D.</span></span><br><span class="line">        <span class="keyword">if</span>(depth &lt; <span class="number">0.0001</span>)</span><br><span class="line">            <span class="keyword">return</span> half4(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>);</span><br><span class="line">    <span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">        <span class="comment">// Case for platforms without REVERSED_Z, such as OpenGL.</span></span><br><span class="line">        <span class="keyword">if</span>(depth &gt; <span class="number">0.9999</span>)</span><br><span class="line">            <span class="keyword">return</span> half4(<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>);</span><br><span class="line">    <span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> color;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>核心代码就是<code>float3 worldPos = ComputeWorldSpacePosition(UV, depth, UNITY_MATRIX_I_VP);</code>，是Unity已经封装好的函数，使用UV和深度值depth来重建世界坐标。函数本质就是<strong>从NDC空间重建世界坐标</strong>，我们来逐行分析。首先：</p><blockquote><p>float2 UV = IN.positionHCS.xy / _ScaledScreenParams.xy;</p></blockquote><p>这里positionHCS已经不再是顶点着色器输出的裁剪空间坐标，而是变成了屏幕空间（ScreenSpace）坐标，随后除以RT分辨率得到UV坐标，用来采样深度值。而对于深度值，代码也做了平台的差异处理，也简单说一下。首先是Reversed Z情况，这种情况 <span class="math inline">\(z_{NDC}\)</span>是[1,0]，而普通情况下又分为类opengl([-1,1])和类D3D([0,1])两种情况。Unity中比较麻烦的就是这里，细心一点可以避免出现很多问题。随后就是ComputeWorldSpacePosition函数。</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">float4 <span class="title function_">ComputeClipSpacePosition</span><span class="params">(float2 positionNDC, <span class="type">float</span> deviceDepth)</span></span><br><span class="line">&#123;</span><br><span class="line">    float4 positionCS = float4(positionNDC * <span class="number">2.0</span> - <span class="number">1.0</span>, deviceDepth, <span class="number">1.0</span>);</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="keyword">if</span> UNITY_UV_STARTS_AT_TOP</span></span><br><span class="line">    <span class="comment">// Our world space, view space, screen space and NDC space are Y-up.</span></span><br><span class="line">    <span class="comment">// Our clip space is flipped upside-down due to poor legacy Unity design.</span></span><br><span class="line">    <span class="comment">// The flip is baked into the projection matrix, so we only have to flip</span></span><br><span class="line">    <span class="comment">// manually when going from CS to NDC and back.</span></span><br><span class="line">    positionCS.y = -positionCS.y;</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> positionCS;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">float3 <span class="title function_">ComputeWorldSpacePosition</span><span class="params">(float2 positionNDC, <span class="type">float</span> deviceDepth, float4x4 invViewProjMatrix)</span></span><br><span class="line">&#123;</span><br><span class="line">    float4 positionCS  = ComputeClipSpacePosition(positionNDC, deviceDepth);</span><br><span class="line">    float4 hpositionWS = mul(invViewProjMatrix, positionCS);</span><br><span class="line">    <span class="keyword">return</span> hpositionWS.xyz / hpositionWS.w;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在ComputeClipSpacePosition函数中，先把传入的positionNDC也就是原代码中的UV重新映射为[-1,1]，在加上之前做处理的z轴重新组成了进行过齐次除法之后的裁剪空间坐标positionCS。</p><p>随后在ComputeWorldSpacePosition中，将上述裁剪空间坐标positionCS和VP逆矩阵做矩阵乘法。由于此时positionCS的w是1.0，在与VP逆矩阵做矩阵乘法之后得到的是齐次坐标hpositionWS，需要再做齐次除法得到世界坐标。</p><h3id="unity中与深度缓存有关的库函数">Unity中与深度缓存有关的库函数</h3><p>下文内容需要读者具有渲染管线的基础知识，如不了解还请先阅读相关资料。</p><p>因为使用函数中用到了变量_ProjectionParams，我们就还是先从了解这个变量开始。开发人员依然贴心的为其添加了注释，而且这个变量要简单的多，我们来看下相关代码：</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// UnityInput.hlsl</span></span><br><span class="line"><span class="comment">// x = 1 or -1 (-1 if projection is flipped)</span></span><br><span class="line"><span class="comment">// y = near plane</span></span><br><span class="line"><span class="comment">// z = far plane</span></span><br><span class="line"><span class="comment">// w = 1/far plane</span></span><br><span class="line">float4 _ProjectionParams;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ScriptableRenderer.cs</span></span><br><span class="line"><span class="comment">// Projection flip sign logic is very deep in GfxDevice::SetInvertProjectionMatrix</span></span><br><span class="line"><span class="comment">// This setup is tailored especially for overlay camera game view</span></span><br><span class="line"><span class="comment">// For other scenarios this wil be overwritten correctly by SetupCameraProperties</span></span><br><span class="line"><span class="built_in">float</span> projectionFlipSign = cameraData.IsCameraProjectionMatrixFlipped() ? <span class="number">-1.0f</span> : <span class="number">1.0f</span>;</span><br><span class="line">Vector4 projectionParams = <span class="keyword">new</span> Vector4(projectionFlipSign, near, far, <span class="number">1.0</span> * invFar);</span><br><span class="line">cmd.SetGlobalVector(ShaderPropertyId.projectionParams, projectionParams);</span><br></pre></td></tr></table></figure><p>代码清晰易懂不做过多说明，提一下其中的 projectionflipped。有这个参数是因为 D3D-like 和 OpenGL-like 的 Render Texturecoordinates 不一样，D3D-like 的RT坐标顶部是0，OpenGL-like的RT坐标底部是0。当在 D3D-like平台渲染RT时，Unity就会执行一个flip的操作，cameraData中的函数就是用来判断有没有执行flip操作。</p><h2 id="通过相似三角形重建世界坐标">通过相似三角形重建世界坐标</h2><p>利用像素的世界空间坐标(fragment's world position)和视深度(eyedepth)值，也可以重建场景的世界坐标。此方法在实际使用时也会因为使用方式不同而略有差异，通常分为渲染网格(mesh)和图像处理(imageeffect)两种情况。</p><h3 id="mesh-perspective-projection">Mesh (Perspective projection)</h3><p><img src="/images/Unity/ReconWorldPos/mesh-perspective-01.png"></p><p>上图是一个在mesh上重建世界坐标的典型图例，由于使用的变量容易混淆，请好好观察此图。图中<code>view direction</code>，<code>Fragment Pos</code>，<code>Fragment Depth</code>通过片元信息可以很容易获取，而<code>Scene Dpeth</code>也可以通过深度贴图取得。随后，利用相似三角形配合相机世界坐标便可以得到场景的世界坐标。核心代码如下：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// ---- in vertex shader</span></span><br><span class="line">VertexPositionInputs inputs = GetVertexPositionInputs(IN.positionOS.xyz);</span><br><span class="line"></span><br><span class="line">OUT.positionHCS = inputs.positionCS;</span><br><span class="line">OUT.positionVS = inputs.positionVS;</span><br><span class="line">OUT.viewDir = _WorldSpaceCameraPos - inputs.positionWS;</span><br><span class="line">OUT.positionHNDC = inputs.positionNDC;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ---- in fragment shader</span></span><br><span class="line">float2 UV = IN.positionHCS.xy / _ScaledScreenParams.xy;</span><br><span class="line"><span class="meta">#<span class="keyword">if</span> UNITY_REVERSED_Z</span></span><br><span class="line">    real depth = SampleSceneDepth(UV);</span><br><span class="line"><span class="meta">#<span class="keyword">else</span></span></span><br><span class="line">    <span class="comment">// Adjust Z to match NDC for OpenGL ([-1, 1])</span></span><br><span class="line">    real depth = lerp(UNITY_NEAR_CLIP_VALUE, <span class="number">1</span>, SampleSceneDepth(UV));</span><br><span class="line"><span class="meta">#<span class="keyword">endif</span></span></span><br><span class="line"><span class="type">float</span> sceneEyeDepth = LinearEyeDepth(depth, _ZBufferParams);</span><br><span class="line"><span class="type">float</span> fragmentEyeDepth = -IN.positionVS.z;</span><br><span class="line"><span class="comment">// Reconstruct the world space positions.</span></span><br><span class="line">float3 worldPos = _WorldSpaceCameraPos - (IN.viewDir/fragmentEyeDepth) * sceneEyeDepth;</span><br></pre></td></tr></table></figure><h3 id="mesh-orthographic-projection">Mesh (Orthographicprojection)</h3><h3 id="blit-perspective-projection">Blit (Perspective projection)</h3><h3 id="参考文献">参考文献</h3><p><ahref="https://zhuanlan.zhihu.com/p/92315967">Unity从深度缓冲重建世界空间位置</a></p><p><ahref="https://www.cyanilux.com/tutorials/depth/#reconstruct-world-pos">Depth| Cyanilux</a></p><p><ahref="https://docs.unity3d.com/6000.0/Documentation/Manual/urp/writing-shaders-urp-reconstruct-world-position.html">Reconstructworld space positions in a shader in UPR</a></p><p><ahref="https://docs.unity3d.com/Manual/SL-PlatformDifferences.html">Writingshaders for different graphics APIs</a></p><p><ahref="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@14.0/api/UnityEngine.Rendering.Universal.CameraData.html">UnityEngine.Rendering.Universal.CameraData</a></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
            <tag> Z-Buffer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SRP中遇到的问题</title>
      <link href="/2024/10/25/LearnSRP/SRP%E4%B8%AD%E9%81%87%E5%88%B0%E8%BF%87%E7%9A%84%E9%97%AE%E9%A2%98/"/>
      <url>/2024/10/25/LearnSRP/SRP%E4%B8%AD%E9%81%87%E5%88%B0%E8%BF%87%E7%9A%84%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<h4 id="confused-on-positionndc">Confused on positionNDC</h4><p>阅读SRP源码时经常会遇到这个变量positionNDC，也会出现在参数中，根据代码感觉它与我们平时所说的 NDC不太一样。以函数 GetVertexPositionInputs(float3 positionOS) 为例，其positionNDC 相关代码为：</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">VertexPositionInputs <span class="title">GetVertexPositionInputs</span>(<span class="params">float3 positionOS</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">    VertexPositionInputs input;</span><br><span class="line">    input.positionWS = TransformObjectToWorld(positionOS);</span><br><span class="line">    input.positionVS = TransformWorldToView(input.positionWS);</span><br><span class="line">    input.positionCS = TransformWorldToHClip(inpout.positionWS);</span><br><span class="line"></span><br><span class="line">    float4 ndc = input.positionCS * <span class="number">0.5f</span>;</span><br><span class="line">    input.positionNDC.xy = float2(ndc.x, ndc.y * _ProjectionParams.x) + ndc.w;</span><br><span class="line">    input.positionNDC.zw = input.positionCS.zw;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> input;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>函数中的 positionNDC 可以很明显看出与平时所说的 NDC不同，因为根本没有进行透视除法这一步运算。顺着代码逻辑进行运算得到的坐标范围如下：</p><p><span class="math display">\[x_{positionNDC} = \frac{x_{clip}}{2} + \frac{w_{clip}}{2} \in [0,w_{clip}]\]</span></p><p>至于代码为何这样写，可以看<ahref="https://discussions.unity.com/t/confused-on-ndc-space/821074">Confusedon NDCspace</a>里面的讨论。为了防止链接失效，我把里面bgolus大佬的回答贴在这里</p><blockquote><p>NDC is not just clipSpace.xy / clipSpace.w. Homogeneous clip space’sx &amp; y have a -w to w range (for what’s in view), but NDC’s x &amp; yhave a 0.0 to 1.0 range. They’re essentially screen space UVs. Dividinghomogeneous clip space by its w just makes it non-homogeneous clipspace, not NDC. NDC is closer to (clipSpace.xy / clipSpace.w) * 0.5 +0.5. So the above code is basically solving that equation a littledifferently by doing:</p><p>(clipSpace.xy * 0.5 + clipSpace.w * 0.5) / clipSpace.w Only, it’s notdoing the divide by w, so it rescales the xy values to a 0.0 to wrange.</p><p>But why not divide by w?</p><p>The key here is that “homogeneous” term. Note the comment forpositionNDC refers to it as “Homogeneous normalized device coordinates”,and not just “normalized device coordinates”. That’s not a mistake. Theterm homogeneous here refers to the fact it’s a coordinate in aprojective space. Essentially is the value multiplied by w, which for aperspective projection happens to be the linear depth. If you want todig into exactly what homogeneous coordinates are, be my guest, Ihonestly still can’t chew through it. But the key thing is having valuesmultiplied by the w allows those values to be correct when beinglinearly interpolated in a perspective projection space by dividing by wafterwards.</p><p>Basically, if you divide by w in the vertex shader, then try to usethe value to sample a screen space texture, it won’t line up any moreand instead will warp mid triangle. If you’re familiar withnon-perspective correct texture mapping, like the original PS1, that’sthe kind of thing it’ll look like.</p><p>So, if you dig deep enough in the shader code, you’ll find the fewplaces it does actually use that float4 version of the positionNDC, itdivides by w in the fragment shader, converting the value fromhomogeneous NDC to “regular” NDC.</p></blockquote><p>提取一下其中的重点。NDC并不是简单的clipSpace.xy/clipSpace.w，齐次裁剪空间中的xy范围都是-w到w，但是NDC中xy的范围是0.0到1.0。从齐次裁剪空间转换到NDC要做的运算是，<code>(clipSpace.xy / clipSpace.w) * 0.5 + 0.5</code>；上文代码中用了另外一个运算形式<code>(clipSpace.xy * 0.5 + clipSpace.w * 0.5) / clipSpace.w</code>，只是没有做最后的除法。那为什么没有做除法呢？</p><p>这样可以先做线性插值在做透视除法，得到更正确的值；如果在顶点着色器中先做了透视除法，之后线性插值可能会出现一些显示问题。后面举的例子没太看懂，是我不了解的内容。总之，代码中的positionNDC 表示 “Homogeneous normalized device coordinates” 并不是“normalized device coordinates”，此时坐标还处于一个投影空间（aprojective space），开发人员并没有写错。</p><h4 id="computescreenpos函数">ComputeScreenPos函数</h4><p>既然写到 positionNDC，正好可以看一下ComputeScreenPos。这个函数在BIRP中用的也是比较多了，在SRP中因为名称比较困惑，直接被开发人员打入冷宫标记为deprecated：</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Deprecated: A confusingly named and duplicate function that scales clipspace to unity NDC range. (-w &lt; x(-y) &lt; w --&gt; 0 &lt; x(-y) &lt; w)</span></span><br><span class="line"><span class="comment">// Use GetVertexPositionInputs().positionNDC instead for vertex shader</span></span><br><span class="line"><span class="comment">// Or a similar function in Common.hlsl, ComputeNormalizedDeviceCoordinatesWithZ()</span></span><br><span class="line"><span class="function">float4 <span class="title">ComputeScreenPos</span>(<span class="params">float4 positionCS</span>)</span></span><br><span class="line">&#123;</span><br><span class="line">    float4 o = positionCS * <span class="number">0.5f</span>;</span><br><span class="line">    o.xy = float2(o.x, o.y * _ProjectionParams.x) + o.w;</span><br><span class="line">    o.zw = positionCS.zw;</span><br><span class="line">    <span class="keyword">return</span> o;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>该函数中代码和SRP新增的GetVertexPositionInputs()函数中positionNDC代码完全一致，正如上个问题中所说的那样，该函数返回的是Homogeneous NDC，一般使用之前需要做齐次除法。</p><h4 id="unity-y-flipped">Unity y-flipped</h4><p>也不算是SRP有关的问题，是对Unity怎么处理平台差异的困惑，尤其是对Unity中renderingupsied-down的困惑。（未解决）</p><p>起因是SRP开发人员的一句注释：</p><blockquote><p>Our world space, view space, screen space and NDC space are y-up. Ourclip space is flipped upside-down due to poor legacy Unity design. Theflip is baked into the projection matrix, so we only have to flipmanually when going from CS to NDC and back.</p></blockquote><p>我们都知道类D3D纹理坐标top处是0，而类OpenGL纹理坐标bottom处是0，这是Unity renderingupside-down的源头。(Unity中 UNITY_UV_STARTS_AT_TOP，文档中描述为：D3D-like平台上值为1，OpenGL-like平台上值为0。SRP源码中与文档描述一致，在D3D11，Metal，Vulkan，Switch中其值都被定义为1。)</p><p>然后我再查阅资料时，对于资料和讨论中的说法越来越困惑，感觉暂时无法想通，不能继续在这里浪费时间了，所以查阅过的资料放在这里，方便之后回来查看。</p><p>Unity 文档：<ahref="https://docs.unity3d.com/Manual/SL-PlatformDifferences.html">Writingshaders for different graphics APIs</a>，<ahref="https://docs.unity3d.com/ScriptReference/GL.GetGPUProjectionMatrix.html">GL.GetGPUProjectionMatrix</a>以及其末尾的其他相关文档，Unity内置宏和其他变量<ahref="https://docs.unity3d.com/Manual/SL-UnityShaderVariables.html">UnityShaderVariables</a><ahref="https://docs.unity3d.com/Manual/shader-branching-platform.html">shader-branching-platform</a>。</p><p>国内资料：<ahref="https://zhuanlan.zhihu.com/p/647820532">Unity坐标系转换遇到的坑</a>，<ahref="https://zhuanlan.zhihu.com/p/677941516">Vulkan/DirectX/OpenGL的坐标系差异以及实际工程中的处理方案</a>，<ahref="https://zhuanlan.zhihu.com/p/29228304">浅谈后处理技术中的各种坑</a></p><p>国外资料：<ahref="https://discussions.unity.com/t/unity-flipping-render-textures/822383">Unityflipping render textures</a>，<ahref="https://discussions.unity.com/t/y-axis-direction-in-ndc-clip-space/912676">Yaxis direction in NDC/clip space</a></p><p><ahref="https://zhuanlan.zhihu.com/p/597918725">HLSL的SV_POSITION</a></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
            <tag> SRP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnSRP - Custom Render Pipeline</title>
      <link href="/2024/10/24/LearnSRP/Custom%20Render%20Pipeline/"/>
      <url>/2024/10/24/LearnSRP/Custom%20Render%20Pipeline/</url>
      
        <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>可编程渲染管线（Scriptable Render Pipeline,SRP）是Unity推出的内置渲染管线的替代方案。SRP可以使用C#脚本控制和定制渲染管线，具有较高的灵活性。Unity使用SRP实现了两套案例，UniversalRender Pipeline（URP，前身是LWRP）和High Definition RenderPipeline（HDRP）。本系列文章跟随CatlikeCoding系列教程实现自定义渲染管线，学习SRP及渲染相关知识。</p><p>本系列教程先使用SRP搭建一个基本的渲染管线骨架，然后再扩展光照lighting，阴影shadow和一些其他模块。</p><h3 id="a-new-render-pipeline">A new Render Pipeline</h3><h4 id="project-setup">Project Setup</h4><p>我们打算在线性空间（linear colorspace）下进行创作，所以先将项目设置中的颜色空间更改为线性空间。颜色空间之间的区别，请查阅官方文档<ahref="https://docs.unity3d.com/2022.3/Documentation/Manual/LinearRendering-LinearOrGammaWorkflow.html">Linearor gamma workflow</a>。</p><h4 id="pipeline-asset">Pipeline Asset</h4><p>想要使用SRP需要先创建一个继承自<code>RenderPipelineAsset</code>的类，它用来保管用于渲染的管线物体实例。此类中需要复写函数<code>CreatePipeline()</code>，用于返回实现管线渲染逻辑的<code>RenderPipeline</code>类实例。在项目中创建一个管线资源类的实例，并将其应用于UnityProject Settings下的Scriptable Render PipelineSettings，这样Unity便会正式使用自定义的渲染管线来进行渲染</p><h4 id="render-pipeline-instance">Render Pipeline Instance</h4><p>对于渲染管线<code>RenderPipeline</code>类，需要复写函数<code>Render()</code>，是渲染逻辑的核心，Unity每帧自动调用一次此函数来渲染主视图。函数具有两个参数，ScriptableRenderContext和Camera数组。值得一提的是，由于数组会产生堆内存分配问题，在Unity2021及之后版本将相机数组改为了CameraList，在开发时应注意相关问题，具体操作可以参考URP源码。</p><h3 id="rendering">Rendering</h3><p>正如上文所说，Unity每帧都会调用<code>Render()</code>函数，将一个上下文结构体和相机数组传入函数。其中的上下文结构体可以与原始引擎产生联系，用于渲染。相机数组则是场景中处于活动状态的所有相机，相机渲染还是使用传入的上下文结构体context。</p><h4 id="pipeline">Pipeline</h4><p>在实现渲染逻辑时，上下文结构体会提供一些函数用于特定功能。例如，使用函数<code>context.DrawSkybox(camera)</code>来绘制天空盒。但是，大部分的渲染逻辑需要我们使用<code>CommandBuffer</code>类来进行完成，使用函数<code>context.ExecuteCommandBuffer(buffer)</code>用来执行我们自定义的CommandBuffer。</p><p>在渲染开始时相机的信息——视角投影矩阵(view-projectionmatrix)——需要传入上下文中，使用函数<code>context.SetupCameraProperties(camera)</code>，这样才能正确渲染场景的位置及转向。相机的设置也会对渲染结果产生影响，每个相机实例可能只会渲染场景中的部分内容。</p><p>而对于每个相机的渲染流程，通常会先渲染不透明物体，然后再渲染透明物体，如果需要的话可以在引擎中渲染辅助线等。这个流程是可由开发人员定制的，这也是SRP相对于内置管线的优势之处。</p><p>最后，在Render函数中提交给context的命令并不会直接进行执行，而是会被存储，直到显式得调用函数<code>context.Submit()</code>。</p><h4 id="settings">Settings</h4><p>相机在渲染管线中的一个重要作用就是裁剪，裁剪需要相机设置及相关矩阵，Unity提供了结构体<code>ScriptableCullingParameters</code>和<code>CullResults</code>来快速实现裁剪功能。可以通过如下类似代码实现，cullingResults便可以用于后续的渲染流程中：</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">CullingResults cullingResults;</span><br><span class="line">...</span><br><span class="line"><span class="function"><span class="built_in">bool</span> <span class="title">Cull</span>()</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(camera.TryGetCullingParameters(<span class="keyword">out</span> ScriptableCullingParameters p))&#123;</span><br><span class="line">        cullingResults = context.Cull(<span class="keyword">ref</span> p);</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后是渲染场景中物体，Unity中使用的是<code>Mesh Renderer</code>，上下文结构体便提供了函数<code>DrawRenderers()</code>。此函数有三个参数，除去裁剪结果cullingResults用于确定哪些Renderer需要渲染，还需要两个渲染设置相关的结构体<code>DrawingSettings</code>和<code>FilteringSettings</code>。</p><p>DrawingSettings描述了如何对可见对象进行排序(sortingSettings)和使用哪些着色器通道(ShaderPassName)。</p><p>Unity使用结构体<code>ShaderTagId</code>来代表不同着色器名字，DrawingsSettings便是使用它决定渲染的着色器通道。DrawingsSettings的构造函数可以传入一个pass，当然其肯定是支持多pass，之后的可以使用函数<code>SetShaderName(index, shaderTagId)</code>来传入。ShaderTagId代表的字符串和ShaderLab代码中的TagsLightMode对应。</p><p>SortingSettings结构体描述了排序物体的方法，可以通过属性<code>criteria</code>来进行设置，有<code>None</code>，<code>RenderQueue</code>和<code>CommonOpaque</code>等选项。</p><p>FilteringSettings结构体顾名思义，描述了一个过滤器，让DrawRenderers函数绘制一个符合过滤器的子集，使用它可以将场景中透明物体及不透明物体分开绘制。</p><p>具体使用代码如下：</p><figure class="highlight c#"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">static</span> ShaderTagId[] legacyShaderTagIds = &#123;</span><br><span class="line">    <span class="keyword">new</span> ShaderTagId(<span class="string">&quot;Always&quot;</span>),</span><br><span class="line">    <span class="keyword">new</span> ShaderTagId(<span class="string">&quot;ForwardBase&quot;</span>),</span><br><span class="line">    <span class="keyword">new</span> ShaderTagId(<span class="string">&quot;PrepassBase&quot;</span>),</span><br><span class="line">    <span class="keyword">new</span> ShaderTagId(<span class="string">&quot;Vertex&quot;</span>),</span><br><span class="line">    <span class="keyword">new</span> ShaderTagId(<span class="string">&quot;VertexLMRGBM&quot;</span>),</span><br><span class="line">    <span class="keyword">new</span> ShaderTagId(<span class="string">&quot;VertexLM&quot;</span>)</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">partial</span> <span class="keyword">void</span> <span class="title">DrawUnsupportedShaders</span>()</span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(errorMaterial == <span class="literal">null</span>)&#123;</span><br><span class="line">        errorMaterial = <span class="keyword">new</span> Material(Shader.Find(<span class="string">&quot;Hidden/InternalErrorShader&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">var</span> drawingSettings = <span class="keyword">new</span> DrawingSettings(</span><br><span class="line">        legacyShaderTagIds[<span class="number">0</span>], <span class="keyword">new</span> SortingSettings(camera)</span><br><span class="line">    )&#123;</span><br><span class="line">        overrideMaterial = errorMaterial</span><br><span class="line">    &#125;;</span><br><span class="line">    <span class="keyword">for</span>(<span class="built_in">int</span> i = <span class="number">1</span>; i&lt;legacyShaderTagIds.Length; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        drawingSettings.SetShaderPassName(i, legacyShaderTagIds[i]);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">var</span> filteringSettings = FilteringSettings.defaultValue;</span><br><span class="line">    context.DrawRenderers(</span><br><span class="line">        cullingResults, <span class="keyword">ref</span> drawingSettings, <span class="keyword">ref</span> filteringSettings</span><br><span class="line">    );</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="profiler">Profiler</h3><p>Unity中有两个很好用的调试工具，Profiler和framedebugger，可以显示渲染流程中的各种数据。Unity也提供了代码使用调试工具。可以使用CommandBuffer类注入Profiler采样，在特定位置调用函数<code>BeginSample</code>和<code>EndSample</code>即可。</p><p>需要注意的一点是，context.ExecuteCommandBuffer函数只是将提交的指令复制而并不会进行清除，所以使用时应当手动调用函数<code>buffer.Clear</code>进行清除。</p><h3 id="参考资料">参考资料</h3><p><ahref="https://catlikecoding.com/unity/tutorials/custom-srp/custom-render-pipeline/">CatlikeCodingCustom Render Pipeline</a></p><p><ahref="https://docs.unity3d.com/2022.3/Documentation/ScriptReference/Rendering.RenderPipeline.Render.html">UnityDocs RenderPipeline</a></p><p><ahref="https://docs.unity3d.com/Packages/com.unity.render-pipelines.universal@10.2/manual/urp-shaders/urp-shaderlab-pass-tags.html">URPDocs LightMode</a></p>]]></content>
      
      
      <categories>
          
          <category> Unity SRP </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
            <tag> SRP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Projection Matrix in OpenGL</title>
      <link href="/2024/10/24/Translations/Projection%20Matrix%20in%20OpenGL/"/>
      <url>/2024/10/24/Translations/Projection%20Matrix%20in%20OpenGL/</url>
      
        <content type="html"><![CDATA[<h3 id="opengl-projection-matrix">OpenGL Projection Matrix</h3><h4 id="概述">概述</h4><p>OpenGL中 GL_PROJECTION矩阵被用于将三维场景投影至二维图像。其首先将顶点信息从视角坐标（eyecoordinate）转换到裁剪坐标（clip coordinate），随后使用裁剪坐标的 <spanclass="math inline">\(w\)</span>分量进行齐次除法，将裁剪坐标转换为规范化设备坐标（normalized devicecoordinate, NDC）。</p><p>因此，我们需要牢记于心的是裁剪（视锥体剔除）和NDC转换都被整合在了<strong>GL_PROJECTION</strong> 矩阵中。下文内容会描述如何通过<code>left, right, bottom, top, near and far</code>这几个参数构建投影矩阵。</p><p>值得注意的是，视锥体剔除（裁剪）发生于裁剪坐标下，在执行齐次除法之前。裁剪坐标的<span class="math inline">\(x,y,z\)</span> 会被用来和 <spanclass="math inline">\(w\)</span> 执行对比操作，不在范围 <spanclass="math inline">\((-w, w)\)</span>的坐标会被剔除。而后，OpenGL会重建被剔除部分的边界。</p><h4 id="perspective-projection">Perspective Projection</h4><figure><imgsrc="/images/ProjectionMatrixInGraphic/Perspective%20Frustum%20and%20NDC.png"alt="Perspective Frustum and Normalized Device Coordinates (NDC)" /><figcaption aria-hidden="true">Perspective Frustum and Normalized DeviceCoordinates (NDC)</figcaption></figure><p>在透视投影中，在视锥体中的三维点（视角坐标）会被映射到立方体（NDC）中；x坐标从<span class="math inline">\([1,r]\)</span> 映射为 <spanclass="math inline">\([-1,1]\)</span> ，y坐标从 <spanclass="math inline">\([b,t]\)</span> 映射为 <spanclass="math inline">\([-1,1]\)</span> ，z坐标从 <spanclass="math inline">\([-n,-f]\)</span> 映射为 <spanclass="math inline">\([-1,1]\)</span>。应注意，<strong>视角坐标是右手坐标系，NDC是左手坐标系</strong>。在视角空间中，摄像机是看向<span class="math inline">\(-Z\)</span> 轴的，但在NDC中，摄像机是看向<span class="math inline">\(+Z\)</span> 轴的。由于<strong>glFrustum()</strong>只接受正数作为近平面（neardistance）和远平面（far distance），我们需要在构建 GL_PROJECTION矩阵时将它们取反。</p><p>OpenGL中，在视角空间中的三维点会被投影到近平面。下图展示了视角空间中的点<span class="math inline">\((x_e, y_e, z_e)\)</span>如何被投影到近平面的点 <span class="math inline">\((x_p, y_p,z_p)\)</span>。</p><figure><imgsrc="/images/ProjectionMatrixInGraphic/Top%20View%20of%20Frustum-1.png"alt="Top View of Frustum" /><figcaption aria-hidden="true">Top View of Frustum</figcaption></figure><p>从视锥体顶视图的相似三角形可以得出：</p><p><span class="math display">\[\frac{x_p}{x_e} = \frac{-n}{z_e} \\x_p = \frac{-n \cdot x_e}{z_e} = \frac{n \cdot x_e}{-z_e}\]</span></p><figure><imgsrc="/images/ProjectionMatrixInGraphic/Side%20View%20of%20Frustum-1.png"alt="Side View of Frustum" /><figcaption aria-hidden="true">Side View of Frustum</figcaption></figure><p>从视锥体右视图的相似三角形可以得出：</p><p><span class="math display">\[\frac{y_p}{y_e} = \frac{-n}{z_e} \\y_p = \frac{-n \cdot y_e}{z_e} = \frac{n \cdot y_e}{-z_e}\]</span></p><p>可以注意到，<span class="math inline">\(x_p\)</span> 和 <spanclass="math inline">\(y_p\)</span> 都反比于 <spanclass="math inline">\(-z_e\)</span>，我们可以从此处入手开始构建投影矩阵。当视角坐标和投影矩阵做矩阵乘法之后，裁剪坐标依然是一个齐次坐标（homogeneouscoordinates）。做齐次除法（除以w）之后，才会变为NDC。</p><p><span class="math display">\[\begin{pmatrix}x_{clip} \\ y_{clip} \\ z_{clip} \\ w_{clip}\end{pmatrix}= M_{projection} \cdot\begin{pmatrix}x_{eye} \\ y_{eye} \\ z_{eye} \\ w_{eye}\end{pmatrix} ,\begin{pmatrix}x_{ndc} \\ y_{ndc} \\ z_{ndc}\end{pmatrix}=\begin{pmatrix}x_{clip}/w_{clip} \\ y_{clip}/w_{clip} \\ z_{clip}/w_{clip}\end{pmatrix}\]</span></p><p>因此，我们可以将裁剪坐标的 w-component 设置为 <spanclass="math inline">\(-z_e\)</span>，于是投影矩阵的第四行就变成了 <spanclass="math inline">\((0, 0, -1, 0)\)</span>。</p><p><span class="math display">\[\begin{pmatrix}x_c \\ y_c \\ z_c \\ w_c\end{pmatrix}=\begin{pmatrix}.&amp;.&amp;.&amp;. \\ .&amp;.&amp;.&amp;. \\ .&amp;.&amp;.&amp;. \\0&amp;0&amp;-1&amp;0\end{pmatrix}\begin{pmatrix}x_e \\ y_e \\ z_e \\ w_e\end{pmatrix}\]</span></p><p>接着，我们把 <span class="math inline">\(x_p, y_p\)</span>线性映射到NDC下的 <span class="math inline">\(x_n, y_n\)</span>，即<spanclass="math inline">\([l,r]\Rightarrow[-1,1],[b,t]\Rightarrow[-1,1]\)</span>。</p><p><imgsrc="/images/ProjectionMatrixInGraphic/Mapping%20from%20xp%20to%20xn.png" /></p><p><span class="math display">\[x_n = \frac{1-(-1)}{r-l} \cdot x_p + \beta\]</span></p><p>带入 <span class="math inline">\((l, -1)\)</span> 和 <spanclass="math inline">\((r, 1)\)</span> 解得 <spanclass="math inline">\(\beta = -\frac{r+l}{r-l}\)</span>，于是：</p><p><span class="math display">\[x_n = \frac{2}{r-l} \cdot x_p - \frac{r+l}{r-l}\]</span></p><p><imgsrc="/images/ProjectionMatrixInGraphic/Mapping%20from%20yp%20to%20yn.png" /></p><p>同理求得：</p><p><span class="math display">\[y_n = \frac{2}{t-b} \cdot y_p - \frac{t+b}{t-b}\]</span></p><p>将通过相似三角形得到的那两个等式代回这两个等式，得到：</p><p><span class="math display">\[\begin{align}x_n = \frac{2n \cdot x_e}{(r-l)(-z_e)} - \frac{r+l}{r-l} =(\frac{2n}{r-l} \cdot x_e + \frac{r+l}{r-l} \cdot z_e) \cdot\frac{1}{-z_e}\\y_n = \frac{2n \cdot y_e}{(t-b)(-z_e)} - \frac{t+b}{t-b} =(\frac{2n}{t-b} \cdot y_e + \frac{t+b}{t-b} \cdot z_e) \cdot\frac{1}{-z_e}\end{align}\]</span></p><p>将等式变换为如此，是因为 <span class="math inline">\(-z_e\)</span>已经作为了裁剪坐标的 w 项，将 <spanclass="math inline">\(\frac{1}{-z_e}\)</span>提出后剩余部分即为裁剪坐标的<spanclass="math inline">\(x_c,y_c\)</span>。由这组等式便又可以确定投影矩阵的前两行：</p><p><span class="math display">\[\begin{pmatrix}x_c \\ y_c \\ z_c \\ w_c\end{pmatrix} =\begin{pmatrix}\frac{2n}{r-l}&amp;0&amp;\frac{r+l}{r-l}&amp;0\\ 0&amp;\frac{2n}{t-b}&amp;\frac{t+b}{t-b}&amp;0\\ .&amp;.&amp;.&amp;. \\ 0&amp;0&amp;-1&amp;0\end{pmatrix}\begin{pmatrix}x_e \\ y_e \\ z_e \\ w_e\end{pmatrix}\]</span></p><p>现在我们只剩下投影矩阵的第三行没有解决。获得 <spanclass="math inline">\(z_n\)</span> 的方式稍微有些不同，因为从三维空间投影到二维空间时，视角空间中的 <spanclass="math inline">\(z_e\)</span>总是被投影到近平面-n处。但是，我们需要更精确的坐标 <spanclass="math inline">\(z\)</span>值去做一些例如裁剪、深度测试等操作。而且，我们需要坐标的逆投影（逆变换）也具有可行性。</p><p>我们知道 <span class="math inline">\(z\)</span> 投影时不依赖于 <spanclass="math inline">\(x,y\)</span> 坐标，于是我们通过 <spanclass="math inline">\(w\)</span> 去获取 <spanclass="math inline">\(z_n\)</span> 和 <spanclass="math inline">\(z_e\)</span>的关系。可以假设矩阵第三行为如下形式：</p><p><span class="math display">\[\begin{pmatrix}x_c \\ y_c \\ z_c \\ w_c\end{pmatrix} =\begin{pmatrix}\frac{2n}{r-l}&amp;0&amp;\frac{r+l}{r-l}&amp;0\\ 0&amp;\frac{2n}{t-b}&amp;\frac{t+b}{t-b}&amp;0\\ 0&amp;0&amp;A&amp;B \\ 0&amp;0&amp;-1&amp;0\end{pmatrix}\begin{pmatrix}x_e \\ y_e \\ z_e \\ w_e\end{pmatrix}\]</span></p><p>在视角空间中 <span class="math inline">\(w_e\)</span> 等于1，于是有：</p><p><span class="math display">\[z_n = z_c/w_c = \frac{Az_e + B}{-z_e}\]</span></p><p>还记得我们之前提到的投影前后对应关系吗？将 <spanclass="math inline">\((-n,-1)(-f,1)\)</span>代入上述等式，解方程组得到：</p><p><span class="math display">\[A = -\frac{f+n}{f-n} , B = -\frac{2fn}{f-n}\]</span></p><p>于是，两者关系式为：</p><p><span class="math display">\[z_n = \frac{-\frac{f+n}{f-n} z_e -\frac{2fn}{f-n}}{-z_e}\]</span></p><p>最终的投影矩阵为：</p><p><span class="math display">\[\begin{pmatrix}\frac{2n}{r-l}&amp;0&amp;\frac{r+l}{r-l}&amp;0\\ 0&amp;\frac{2n}{t-b}&amp;\frac{t+b}{t-b}&amp;0\\ 0&amp;0&amp;-\frac{f+n}{f-n}&amp;-\frac{2fn}{f-n}\\ 0&amp;0&amp;-1&amp;0\end{pmatrix}\]</span></p><p>这个投影矩阵具有普适性，如果视锥体是轴对称的，有 <spanclass="math inline">\(r = -l,t=-b\)</span>，其便可化简为：</p><p><span class="math display">\[\begin{pmatrix}\frac{n}{r}&amp;0&amp;0&amp;0\\ 0&amp;\frac{n}{t}&amp;0&amp;0\\ 0&amp;0&amp;-\frac{f+n}{f-n}&amp;-\frac{2fn}{f-n}\\ 0&amp;0&amp;-1&amp;0\end{pmatrix}\]</span></p><p>在我们讨论其他问题之前，先看一眼 <spanclass="math inline">\(z_e\)</span> 和 <spanclass="math inline">\(z_n\)</span>两者之间的关系。你可能会注意到，两者的关系是非线性的。这意味着其在近平面有着较高的精确度，而远平面附近的精确度较低。如果<span class="math inline">\([-n, -f]\)</span>的范围增大，会引起远平面附近的精度问题，被称为z-fighting。当出现此问题时，改变远平面附近的 <spanclass="math inline">\(z_e\)</span> 值大小可能并不会使 <spanclass="math inline">\(z_n\)</span> 值发生变化。为了避免此问题，应当使<span class="math inline">\([-n, -f]\)</span> 范围足够小。</p><p><imgsrc="/images/ProjectionMatrixInGraphic/Comparison%20of%20Depth%20Buffer%20Precisions.png" /></p><h4 id="perspective-matrix-with-field-of-view-fov">Perspective Matrixwith Field of View (FOV)</h4><p>特定窗口的透视投影，对于给定的远近平面，很难正确决定4个参数（左、右、上、下）。但是，你可以通过垂直/水平视角和宽度/高度很容易得到这4个参数，当然，只限于轴对称的透视投影。</p><h5 id="using-vertical-fov">Using Vertical FOV</h5><p><imgsrc="/images/ProjectionMatrixInGraphic/Perspective%20projection%20with%20vertical%20FOV.png" /></p><p>如果垂直视角用 <span class="math inline">\(\theta\)</span>表示，屏幕的宽高比已知，左右上下四参数可以通过直角三角形很容易得到。</p><h5 id="using-horizontal-fov">Using Horizontal FOV</h5><p><imgsrc="/images/ProjectionMatrixInGraphic/Perspective%20projection%20with%20horizontal%20FOV.png" /></p><p>水平视角同理。</p><h4 id="orthographic-projection">Orthographic Projection</h4><p><imgsrc="/images/ProjectionMatrixInGraphic/Orthographic%20Volume%20and%20NDC.png" /></p><p>构建正交投影使用的投影矩阵相比于透视投影来说简单很多，视角空间中的<spanclass="math inline">\(x_e,y_e,z_e\)</span>都是被线性映射到NDC的。我们只需要将矩形缩放为立方体，然后移动到原点即可。然我们根据函数图快速写出三组关系式：</p><p><imgsrc="/images/ProjectionMatrixInGraphic/Mapping%20from%20xe%20to%20xn.png" /></p><p><span class="math display">\[\begin{align}x_n = \frac{2}{r-l} \cdot x_e - \frac{r+l}{r-l} \\y_n = \frac{2}{t-b} \cdot y_e - \frac{t+b}{t-b} \\z_n = \frac{-2}{f-n} \cdot z_e - \frac{f+n}{f-n}\end{align}\]</span></p><p>投影过程中不需要齐次除法，w-component 是无用的，所以矩阵第四行为<spanclass="math inline">\((0,0,0,1)\)</span>。因此，根据上面的关系式，得到正交投影矩阵：</p><p><span class="math display">\[\begin{pmatrix}\frac{2}{r-l}&amp;0&amp;0&amp;-\frac{r+l}{r-l}\\ 0&amp;\frac{2}{t-b}&amp;0&amp;-\frac{t+b}{t-b}&amp;\\ 0&amp;0&amp;-\frac{2}{f-n}&amp;-\frac{f+n}{f-n}\\ 0&amp;0&amp;0&amp;1\end{pmatrix}\]</span></p><h3 id="参考文献">参考文献</h3><p><ahref="https://www.songho.ca/opengl/gl_projectionmatrix.html">OpenGLProjection Matrix</a></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graphic </tag>
            
            <tag> Game Development </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IBL-Specular IBL</title>
      <link href="/2024/10/22/LearnOpenGL/IBL-Specular%20IBL/"/>
      <url>/2024/10/22/LearnOpenGL/IBL-Specular%20IBL/</url>
      
        <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>上篇文章我们讲了PBR管线中IBL的漫反射部分，将其预计算之后存储到一张irradiance map中，在渲染时可以高效的读取环境光的漫反射值。本文将专注于反射方程的高光反射部分：<span class="math display">\[L_o(p,\omega_o)=\int_\Omega  (k_d \frac{c}{\pi} + \frac{DFG}{4(w_o \cdotn)(w_i \cdot n)}) L_i(p,\omega_i)n\cdot\omega_id\omega_i\]</span> 可以注意到Cook-Torrance近似的specular portion（乘以 <spanclass="math inline">\(kS\)</span>的部分）在积分过程中并不是常量，不仅依赖于光的入射方向<spanclass="math inline">\(\omega_i\)</span>，而且和视角方向<spanclass="math inline">\(\omega_o\)</span>有关。在实时渲染中计算两者的积分着实不太可能，于是EpicGames提出了一个著名的解决方案，在为实时计算做出了妥协的同时也有不错的效果，这种方案被称为<strong>分割求和近似法（splitsum approximation）</strong>。</p><p>分割求和近似法将反射方程的高光项分为独立的两部分，可以分别做卷积计算，并随后在PBR流程中重新组合起来计算IBL的高光项。和预卷积得到irradiance map类似，分割求和近似方法也需要一张HDR的环境贴图作为卷积输入。为了更好的理解近似分割求和方法，我们先来看一下反射方程中的高光部分：<span class="math display">\[L_o(p,\omega_o)=\int_\Omega  \frac{DFG}{4(w_o \cdot n)(w_i \cdot n)}L_i(p,\omega_i)n\cdot\omega_id\omega_i\\= \int_\Omega f_r(p, \omega_i, \omega_o) L_i(p, \omega_i)n \cdot\omega_i d\omega_i\]</span></p><p>由于性能问题，我们还是希望和 irradiance map一样，将积分的与计算结果存储于一个类似于 specular IBLmap的贴图，在渲染时通过采样此贴图便可以高效的完成IBL的高光部分。但是，和IBL的漫反射部分不同，高光部分积分依赖于BRDF中的更多变量：<span class="math display">\[f_r(p, \omega_i, \omega_o) = \frac{DFG}{4(\omega_o \cdot n)(\omega_i \cdot n)}\]</span> Epic Game 的 split sum approximation通过将其分割为两个可以预计算的独立部分、随后再将两部分结合的方式，来使其满足与计算的条件。通过数学近似，反射方程的高光部分可写为：<span class="math display">\[L_o(p,\omega_o)= \int_\Omega L_i(p, \omega_i) d \omega_i * \int_\Omegaf_r (p, \omega_i, \omega_o) n \cdot \omega_i d \omega_i\]</span><u>等式的第一部分</u>（被卷积以后）被称作<strong>预滤波环境贴图（pre-filteredenvironment map）</strong>，其和 irradiance map类似是一张预计算的环境卷积贴图，但是这次会考虑粗糙度的影响。因为随着粗糙度的增加，参与到环境贴图卷积的采样向量会更分散，导致反射更模糊。所以对于卷积的每个粗糙度级别，我们将按顺序把模糊后的结果存储在预滤波贴图的mipmap中。例如，预过滤的环境贴图在其5 个 mipmap 级别中存储 5 个不同粗糙度值的预卷积结果，如下图所示：</p><p><img src="/images/LearnOpenGL/3-4.png" /></p><p>我们hi用Cook-TorranceBRDF的法线分布函数生成采样向量及其散射强度，该函数将法线normal和视角viewdirection用作输入。由于在对环境贴图进行卷积时不知道视角方向，于是EpicGames假设视角方向——即镜面反射方向——总是等于输出采样方向，以作进一步近似。代码表示如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vec3 N = <span class="built_in">normal</span>(w_o);</span><br><span class="line">vec3 R = N;</span><br><span class="line">vec3 V = R;</span><br></pre></td></tr></table></figure><p>这样，pre-filtered environment convolution就不需要关心视角方向了。但是如此做有一个缺点，我们在如下图所示的角度观察表面时，得到的镜面发射结果不是很好（图片来自《MovingFrostbite to PBR》。通常认为此近似是可以接受的：</p><p><img src="/images/LearnOpenGL/3-5.png" /></p><p><u>等式的第二部分</u>就是镜面反射积分的BRDF部分。如果我们假设入射的辐照度在每个方向都是白色（<spanclass="math inline">\(L(p,x)=1.0\)</span>），就可以在给定粗糙度roughness和法线<spanclass="math inline">\(n\)</span>与光照方向<spanclass="math inline">\(\omega_i\)</span>的夹角（或是<spanclass="math inline">\(n \cdot\omega_i\)</span>）的前提下，预计算得出BRDF的值。EpicGames将BRDF对于每个粗糙度roughness和每个法线<spanclass="math inline">\(n\)</span>与光照方向<spanclass="math inline">\(\omega_i\)</span>的夹角组合的预计算结果存储于一张2D查找表中（LUT），这张LUT被称为<strong>BRDF积分贴图（BRDFintegration map）</strong>。这张2DLUT存储的是菲涅尔函数的系数（R通道）和偏差值（G通道）。</p><blockquote><p>The 2D lookup texture outputs a scale (red) and a bias value (green)to the surface's Fresnel response giving us the second part of the splitspecular integral</p></blockquote><p><img src="/images/LearnOpenGL/3-6.png" /></p><p>这张LUT的水平坐标是 <span class="math inline">\(n \cdot\omega_i\)</span>（范围0.0 - 1.0），垂直坐标是roughness。结合这张BRDFintegration map 和 pre-filtered environment map便可以得到环境光的高光反射部分积分值。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> lod = <span class="built_in">getMipLevelFromRoughness</span>(roughness);</span><br><span class="line">vec3 prefilteredColor = <span class="built_in">textureCubeLod</span>(PrefilteredEnvMap, refVec, lod);</span><br><span class="line">vec2 envBRDF = <span class="built_in">texture2D</span>(BRDFintegrationMap, <span class="built_in">vec2</span>(NdotV, roughness)).xy;</span><br><span class="line">vec3 indirectSpecular = prefilteredColor * (F * envBRDF.x + envBRDF.y);</span><br></pre></td></tr></table></figure><h3 id="pre-filtering-an-hdr-environment-map">Pre-filtering an HDRenvironment map</h3><p>预滤波环境贴图的方法与我们对 irradiance map求卷积的方法非常相似。区别在于，此次会考虑到粗糙度，对于卷积的每个粗糙度级别，我们将按顺序把模糊后的结果存储在预滤波贴图的mipmap 中。</p><p>首先，我们需要生成一个新的立方体贴图来保存预过滤的环境贴图数据。为了确保分配足够的内存，可以调用glGenerateMipmap函数。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> prefilterMap;</span><br><span class="line"><span class="built_in">glGenTextures</span>(<span class="number">1</span>, &amp;prefilterMap);</span><br><span class="line"><span class="built_in">glBindTexture</span>(GL_TEXTURE_CUBE_MAP, prefilterMap);</span><br><span class="line"><span class="keyword">for</span>(<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">6</span>; ++i)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">glTexImage2D</span>(GL_TEXTUER_CUBE_MAP_POSITIVE_X + i, <span class="number">0</span>, GL_RGB16F, <span class="number">128</span>, <span class="number">128</span>, <span class="number">0</span>, GL_RGB, GL_FLOAT, <span class="literal">nullptr</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">glTexParameteri</span>(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);</span><br><span class="line"><span class="built_in">glTexParameteri</span>(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);</span><br><span class="line"><span class="built_in">glTexParameteri</span>(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_WRAP_R, GL_CLAMP_TO_EDGE);</span><br><span class="line"><span class="built_in">glTexParameteri</span>(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_MIN_FILTER, GL_LINEAR_MIPMAP_LINEAR); </span><br><span class="line"><span class="built_in">glTexParameteri</span>(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_MAG_FILTER, GL_LINEAR);</span><br><span class="line"></span><br><span class="line"><span class="built_in">glGenerateMipmap</span>(GL_TEXTURE_CUBE_MAP);</span><br></pre></td></tr></table></figure><p>需要注意的是，因为我们计划采样 prefilterMap 的mipmap，所以需要确保将其minification filter设置为GL_LINEAR_MIPMAP_LINEAR 以启用三线性过滤。我们使用每面128×128的分辨率作为基础 mip级别来存储预滤波的镜面反射，对于大多数场景来说已经足够了，但如果场景里有大量光滑材料（想想汽车上的反射），可能需要提高分辨率。</p><p>在前篇文章中，我们使用球面坐标生成均匀分布在半球<spanclass="math inline">\(\Omega\)</span>上的采样向量，用于对环境贴图进行卷积。虽然这个方法非常适用于辐照度，但对于镜面反射效果较差。镜面反射依赖于表面的粗糙度，反射光线可能比较松散，也可能比较紧密，但是一定会围绕着反射向量，除非表面极度粗糙：</p><p><img src="/images/LearnOpenGL/3-7.png" /></p><p>反射光线可能的出射范围构成的形状被称作<strong>镜面波瓣（specularlobe）</strong>。当表面粗糙度增加时，镜面波瓣的大小增加；波瓣的形状随着入射光线方向的改变而改变。因此，镜面波瓣的形状高度依赖于表面材质。</p><p>对于微表面模型，当给出入射光线方向后，我们就能得到微表面半程向量，从而可以想象出镜面波瓣的大致反射方向。正如所想，大部分光线都在以微表面半程向量为基础的波瓣中，这可以启发我们如何生成采样向量，此过程被称为<strong>重要性采样</strong>。（原文如下，翻译可能有些拗口）</p><blockquote><p>When it comes to the microsurface model, we can imagine the specularlobe as the reflection orientation about the microfacet halfway vectorsgiven some incoming light direction. Seeing as most light rays end up ina specular lobe reflected around the microfacet halfway vectors, itmakes sense to generate the sample vectors in a similar fashion as mostwould otherwise be wasted. This process is known as importancesampling.</p></blockquote><h4 id="monte-carlo-integration-and-importance-sampling">Monte Carlointegration and importance sampling</h4><p>为了完全理解重要性采样，我们首先要深入研究被称为蒙特卡洛积分的数学概念。蒙特卡洛积分主要出现在统计学和概率学的理论中。蒙特卡洛帮助我们离散地解决一些统计学的总体问题，而非直接考虑整体。</p><p>例如，你想去测量一个国家公民的平均身高。为了得到问题的准确答案，需要测量每个公民的身高然后取平均值。但是，由于大多数国家都有相当多的人口，测量每个公民身高并不是一个现实的方法，需要太多精力和时间。另外一个方法是，从整体中完全随机的选取部分公民，测量他们的身高，取平均值。例如可以只测量100人的身高，虽然结果并不准确，但是你会得到一个<strong>接近</strong>于真实数据的结果。这种规律可以被称为<strong>大数定律（lawof large numbers）</strong>。也就是说，如果从总体测量一个大小为<spanclass="math inline">\(N\)</span>的真正随机样本的较小集合，那么结果将相对接近真实答案，并且随着样本数量<spanclass="math inline">\(N\)</span>的增加而越来越接近。</p><p>蒙特卡洛积分便是建立在大数定律之上的，使用相同的方式解决积分问题。不使用所有可能的采样值<span class="math inline">\(x\)</span>来求得积分，而是生成几个随机采样值 <spanclass="math inline">\(N\)</span> 来解决积分，随着 <spanclass="math inline">\(N\)</span>的增大，会获得更接近准确答案的积分结果： <span class="math display">\[O = \int_{a}^{b} f(x)dx \approx \frac{1}{N} \sum_{i=0}^{N-1}\frac{f(x)}{pdf(x)}\]</span> 式中 <span class="math inline">\(pdf\)</span>表示<strong>可能性密度函数（probability densityfunction）</strong>，告诉我们在总采样集合中选取某个特定采样的概率。举个例子，一个人群身高的<span class="math inline">\(pdf\)</span>如下图所示。根据图可以得出结论，当从此人群中随机采样身高时，有很高的概率会采样到1.70附近的人，而采样到1.50的概率较低。</p><p><img src="/images/LearnOpenGL/3-8.png" /></p><p>对于蒙特卡洛积分，一些采样拥有比其他采样更高的出现概率，这也是蒙特卡洛估算会乘以或除以<span class="math inline">\(pdf\)</span>的原因。目前为止，我们估算积分的例子中采样都是平均的，每个样本都有平均的生成概率。这样，我们最终得到的结果是<strong>无偏的（unbiased）</strong>，这意味着随着样本数量的增加，结果最终<strong>收敛（converge）</strong>于积分的精确解。</p><p>但是，一些蒙特卡洛估算子<strong>是有偏的（biased）</strong>，其样本生成不是完全随机的，而是偏向某个特定值或特定方向。这种有偏的蒙特卡罗估算子具有<strong>更快的收敛速度（fasterrate ofconvergence）</strong>，只是由于其特性，其最终收敛结果可能并不准确。在图形学中，只要最终渲染结果是可以接受的，这种快速但是不准确的收敛特性也是可以商榷的。</p><p>由于蒙特卡洛积分可以很直观的以离散和高效的方式估算连续函数的积分，其在图形学中的应用是非常普遍的：取一片面积/体积进行采样，生成<span class="math inline">\(N\)</span>个随机样本，加权平均来得到最终的结果。蒙特卡洛积分是一个很庞大的数学话题，在此不多展开，只涉及到如何生成<em>随机采样（randomsamples）</em>。</p><p>大多数情况下，我们使用的是完全（伪）随机采样。但通过利用半随机（semi-random）序列的某些属性，我们可以生成一些具有有趣属性的随机样本。例如，我们可以通过一种叫做<strong>低差异序列（low-discrepancysequences）</strong>的东西来进行蒙特卡洛积分，它会产生分布均匀的随机样本：</p><p><img src="/images/LearnOpenGL/3-9.png" /></p><p>当使用低差异序列生成蒙特卡洛样本向量时，该过程被称为<strong>拟蒙特卡洛积分（Quasi-MonteCarlo）</strong>。拟蒙特卡洛积分拥有更快的收敛速度，使它对于性能繁重的应用很有用。鉴于我们刚了解蒙特卡洛和拟蒙特卡洛积分，我们可以使用一个有趣的属性来获得更快的收敛速度，那就是<strong>重要性采样（importancesampling）</strong>。前文提到，在镜面反射的情况下，反射的光被限制在镜面波瓣内，波瓣的大小取决于表面粗糙度。既然镜面波瓣外的任何随机样本都与镜面积分无关，将样本集中在镜面波瓣内生成是有意义的，但代价是蒙特卡洛估算会产生偏差。</p><p>重要性采样的核心是：只在围绕微表面半程向量并受粗糙度限制的区域生成采样向量。通过将拟蒙特卡洛和重要性采样结合，我们可以得到更快的收敛效率。因为求解速度的增快，得到足够近似度所需要的样本会更少，其速度基本满足我们的需求。</p><h4 id="a-low-discrepancy-sequence">A low-discrepancy sequence</h4><p>在本文中，我们将使用基于拟蒙特卡洛方法的随机低差异序列的重要性采样来预计算反射方程的镜面反射部分。我们将使用的序列被称为<strong>Hammersley Sequence</strong> ，Holger Dammertz曾详细描述过它。Hammersley序列基于 <strong>Van Der Corput</strong>序列，该序列是把十进制数字的二进制表示镜像翻转到小数点右边而得。</p><p>这里有一个小技巧，我们可以在着色器程序中非常有效地生成 Van Der Corput序列，我们将用它来获得 Hammersley 序列，设总样本数为<spanclass="math inline">\(N\)</span>，样本索引为<spanclass="math inline">\(i\)</span>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">float</span> <span class="title">RadicalInvers_VdC</span><span class="params">(uint bits)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    bits = (bits &lt;&lt; <span class="number">16u</span>) | (bits &gt;&gt; <span class="number">16u</span>);</span><br><span class="line">    bits = ((bits &amp; <span class="number">0x55555555</span>u) &lt;&lt; <span class="number">1u</span>) | ((bits &amp; <span class="number">0xAAAAAAAA</span>u) &gt;&gt; <span class="number">1u</span>);</span><br><span class="line">    bits = ((bits &amp; <span class="number">0x33333333</span>u) &lt;&lt; <span class="number">2u</span>) | ((bits &amp; <span class="number">0xCCCCCCCC</span>u) &gt;&gt; <span class="number">2u</span>);</span><br><span class="line">    bits = ((bits &amp; <span class="number">0x0F0F0F0F</span>u) &lt;&lt; <span class="number">4u</span>) | ((bits &amp; <span class="number">0xF0F0F0F0</span>u) &gt;&gt; <span class="number">4u</span>);</span><br><span class="line">    bits = ((bits &amp; <span class="number">0x00FF00FF</span>u) &lt;&lt; <span class="number">8u</span>) | ((bits &amp; <span class="number">0xFF00FF00</span>u) &gt;&gt; <span class="number">8u</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">float</span>(bits) * <span class="number">2.3283064365386963e-10</span>; <span class="comment">// / 0x100000000</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// ----------------------------------------------------------------------</span></span><br><span class="line"><span class="function">vec2 <span class="title">Hammersley</span><span class="params">(uint i, uint N)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">vec2</span>(<span class="built_in">float</span>(i)/<span class="built_in">float</span>(N), <span class="built_in">RadicalInverse_VdC</span>(i));</span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure><h4 id="ggx-importance-sampling">GGX Importance sampling</h4><p>相比于均匀或随机地在积分半球 <spanclass="math inline">\(\Omega\)</span>产生采样向量，我们会根据粗糙度，偏向微表面的半程向量进行采样。采样过程与之前类似，区别在于，现在使用地差异序列作为输出：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> uint SAMPLE_COUNT = <span class="number">4096u</span>;</span><br><span class="line"><span class="keyword">for</span>(uint i = <span class="number">0u</span>; i &lt; SAMPLE_COUNT; ++i)</span><br><span class="line">&#123;</span><br><span class="line">vec2 Xi = <span class="built_in">Hammersley</span>(i, SAMPLE_COUNT);</span><br></pre></td></tr></table></figure><p>此外，为了构建采样向量，我们需要一些方法来偏移采样向量，以使其朝向特定粗糙度的镜面波瓣方向。可以使用理论教程中描述的NDF，并将GGXNDF集合到Epic Games 所描述的球形采样向量的处理中：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">vec3 <span class="title">ImportanceSampleGGX</span><span class="params">(vec2 Xi, vec3 N, <span class="type">float</span> roughness)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">float</span> a = roughness * roughness;</span><br><span class="line">    <span class="type">float</span> phi = <span class="number">2.0</span> * PI * Xi.x;</span><br><span class="line">    <span class="type">float</span> cosTheta = <span class="built_in">sqrt</span>((<span class="number">1.0</span> - Xi.y)/(<span class="number">1.0</span> + (a*a - <span class="number">1.0</span>) * Xi.y));</span><br><span class="line">    <span class="type">float</span> sinTheta = <span class="built_in">sqrt</span>(<span class="number">1.0</span> - cosTheta*cosTheta);</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// form spherical coordinates to cartesian coordinates</span></span><br><span class="line">    vec3 H;</span><br><span class="line">    H.x = <span class="built_in">cos</span>(phi) * sinTheta;</span><br><span class="line">    H.y = <span class="built_in">sin</span>(phi) * sinTheta;</span><br><span class="line">    H.z = cosTheta;</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// form tangent-space vector to world-space sample vector</span></span><br><span class="line">    vec3 up        = <span class="built_in">abs</span>(N.z) &lt; <span class="number">0.999</span> ? <span class="built_in">vec3</span>(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>) : <span class="built_in">vec3</span>(<span class="number">1.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>);</span><br><span class="line">    vec3 tangent   = <span class="built_in">normalize</span>(<span class="built_in">cross</span>(up, N));</span><br><span class="line">    vec3 bitangent = <span class="built_in">cross</span>(N, tangent);</span><br><span class="line">    </span><br><span class="line">    vec3 sampleVec = tanget * H.x + bitangent * H.y + N * H.z;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">normalize</span>(sampleVec);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>基于特定粗糙度输入和低差异序列值<spanclass="math inline">\(X_i\)</span>，我们获得了一个采样向量，该向量大体围绕着预估的微表面的半程向量。值得一提的是，根据迪士尼对PBR的研究，EpicGames使用了平方粗糙度以获得更好的视觉效果。</p><p>使用低差异Hammersley序列和上述定义的样本生成方法，我们最终完成预滤波卷积着色器：</p><figure class="highlight c"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line">out vec4 FragColor;</span><br><span class="line">in vec3 localPos;</span><br><span class="line"></span><br><span class="line">uniform samplerCube environmentMap;</span><br><span class="line">uniform <span class="type">float</span> roughness;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">float</span> PI = <span class="number">3.14159265359</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">float</span> <span class="title function_">RadicalInverse_VdC</span><span class="params">(uint bits)</span>;</span><br><span class="line">vec2 <span class="title function_">Hammersley</span><span class="params">(uint i, uint N)</span>;</span><br><span class="line">vec3 <span class="title function_">ImportanceSampleGGX</span><span class="params">(vec2 Xi, vec3 N, <span class="type">float</span> roughness)</span>;</span><br><span class="line"></span><br><span class="line"><span class="type">void</span> <span class="title function_">main</span><span class="params">()</span></span><br><span class="line">&#123;</span><br><span class="line">    vec3 N = normalize(localPos);    </span><br><span class="line">    vec3 R = N;</span><br><span class="line">    vec3 V = R;</span><br><span class="line"></span><br><span class="line">    <span class="type">const</span> uint SAMPLE_COUNT = <span class="number">1024u</span>;</span><br><span class="line">    <span class="type">float</span> totalWeight = <span class="number">0.0</span>;   </span><br><span class="line">    vec3 prefilteredColor = vec3(<span class="number">0.0</span>);     </span><br><span class="line">    <span class="keyword">for</span>(uint i = <span class="number">0u</span>; i &lt; SAMPLE_COUNT; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        vec2 Xi = Hammersley(i, SAMPLE_COUNT);</span><br><span class="line">        vec3 H  = ImportanceSampleGGX(Xi, N, roughness);</span><br><span class="line">        vec3 L  = normalize(<span class="number">2.0</span> * dot(V, H) * H - V);</span><br><span class="line"></span><br><span class="line">        <span class="type">float</span> NdotL = max(dot(N, L), <span class="number">0.0</span>);</span><br><span class="line">        <span class="keyword">if</span>(NdotL &gt; <span class="number">0.0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            prefilteredColor += texture(environmentMap, L).rgb * NdotL;</span><br><span class="line">            totalWeight      += NdotL;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    prefilteredColor = prefilteredColor / totalWeight;</span><br><span class="line"></span><br><span class="line">    FragColor = vec4(prefilteredColor, <span class="number">1.0</span>);</span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure><p>输入的粗糙度随着预滤波的立方体贴图的 mipmap级别变化（从0.0到1.0），我们根据据粗糙度预滤波环境贴图，把结果存在prefilteredColor 里。再用 prefilteredColor除以采样权重总和，其中对最终结果影响较小（NdotL较小）的采样最终权重也较小。</p><h4 id="capturing-pre-filter-mipmap-levels">Capturing pre-filter mipmaplevels</h4><p>剩下要做的就是让OpenGL在多个mipmap级别上以不同的粗糙度值pre-filter环境贴图，有了之前irradiance章节的铺垫，这里做起来就十分简单：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">prefilterShader.<span class="built_in">use</span>();</span><br><span class="line">prefilterShader.<span class="built_in">setInt</span>(<span class="string">&quot;environmentMap&quot;</span>, <span class="number">0</span>);</span><br><span class="line">prefilterShader.<span class="built_in">setMat4</span>(<span class="string">&quot;projection&quot;</span>, captureProjection);</span><br><span class="line"><span class="built_in">glActiveTexture</span>(GL_TEXTURE0);</span><br><span class="line"><span class="built_in">glBindTexture</span>(GL_TEXTURE_CUBE_MAP, envCubemap);</span><br><span class="line"></span><br><span class="line"><span class="built_in">glBindFramebuffer</span>(GL_FRAMEBUFFER, captureFBO);</span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> maxMipLevels = <span class="number">5</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> mip = <span class="number">0</span>; mip &lt; maxMipLevels; ++mip)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// reisze framebuffer according to mip-level size.</span></span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> mipWidth  = <span class="number">128</span> * std::<span class="built_in">pow</span>(<span class="number">0.5</span>, mip);</span><br><span class="line">    <span class="type">unsigned</span> <span class="type">int</span> mipHeight = <span class="number">128</span> * std::<span class="built_in">pow</span>(<span class="number">0.5</span>, mip);</span><br><span class="line">    <span class="built_in">glBindRenderbuffer</span>(GL_RENDERBUFFER, captureRBO);</span><br><span class="line">    <span class="built_in">glRenderbufferStorage</span>(GL_RENDERBUFFER, GL_DEPTH_COMPONENT24, mipWidth, mipHeight);</span><br><span class="line">    <span class="built_in">glViewport</span>(<span class="number">0</span>, <span class="number">0</span>, mipWidth, mipHeight);</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> roughness = (<span class="type">float</span>)mip / (<span class="type">float</span>)(maxMipLevels - <span class="number">1</span>);</span><br><span class="line">    prefilterShader.<span class="built_in">setFloat</span>(<span class="string">&quot;roughness&quot;</span>, roughness);</span><br><span class="line">    <span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">6</span>; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        prefilterShader.<span class="built_in">setMat4</span>(<span class="string">&quot;view&quot;</span>, captureViews[i]);</span><br><span class="line">        <span class="built_in">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, </span><br><span class="line">                               GL_TEXTURE_CUBE_MAP_POSITIVE_X + i, prefilterMap, mip);</span><br><span class="line"></span><br><span class="line">        <span class="built_in">glClear</span>(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);</span><br><span class="line">        <span class="built_in">renderCube</span>();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">glBindFramebuffer</span>(GL_FRAMEBUFFER, <span class="number">0</span>);   </span><br></pre></td></tr></table></figure><p>这个过程类似于辐照度贴图卷积，但是这次我们将帧缓冲区缩放到适当的mipmap 尺寸， mip 级别每增加一级，尺寸缩小为一半。此外，我们在glFramebufferTexture2D 的最后一个参数中指定要渲染的目标 mip级别，然后将要预过滤的粗糙度传给预过滤着色器。</p><h3 id="pre-computing-the-brdf">Pre-computing the BRDF</h3><p>现在让我们专注于BRDF，split-sumapproximation的第二部分。先简单回顾一下分割求和近似的公式： <spanclass="math display">\[L_o(p,\omega_o)= \int_\Omega L_i(p, \omega_i) d \omega_i * \int_\Omegaf_r (p, \omega_i, \omega_o) n \cdot \omega_i d \omega_i\]</span>我们已经预计算了公式的左半部分，并将结果存储在根据粗糙度划分等级pre-filtermap中。在公式右半部分的BRDF卷积中，需要考虑入射角度 <spanclass="math inline">\(n\cdot \omega_o\)</span>，表面粗糙度和菲涅尔 <spanclass="math inline">\(F_0\)</span>。这等同于在纯白的环境光或辐射度恒定为0.1 的情况下，对镜面BRDF求积分。让我们先尝试将 <spanclass="math inline">\(F_0\)</span>提出镜面BRDF等式： <spanclass="math display">\[\int_{\Omega} f_r(p, \omega_i, \omega_o)n\cdot \omega_i d\omega_i =\int_{\Omega} f_r(p, \omega_i, \omega_o) \frac{F(\omega_o,h)}{F(\omega_o, h)} n \cdot \omega_i d\omega_i\]</span> 然后将Fresnel-Schlick近似带入等式右边得： <spanclass="math display">\[\int_{\Omega} \frac{f_r(p, \omega_i, \omega_o)}{F(\omega_o, h)}(F_0 + (1-F_0)(1-\omega_o\cdot h)^5)n \cdot \omega_i d\omega_i\]</span> 让我们使用 <span class="math inline">\(\alpha\)</span> 代替<span class="math inline">\((1-\omega_o\cdoth)^5\)</span>对表达式进行变换： <span class="math display">\[\begin{align}\int_{\Omega} \frac{f_r(p, \omega_i, \omega_o)}{F(\omega_o, h)}(F_0 + (1-F_0) \alpha) n \cdot \omega_i d\omega_i\\\int_{\Omega} \frac{f_r(p, \omega_i, \omega_o)}{F(\omega_o, h)}(F_0 + \alpha - F_0\alpha) n \cdot \omega_i d\omega_i\\\int_{\Omega} \frac{f_r(p, \omega_i, \omega_o)}{F(\omega_o, h)}(F_0 (1 - \alpha) + \alpha) n \cdot \omega_i d\omega_i\end{align}\]</span> 然后可以将表达式分称两个积分和： <span class="math display">\[\int_{\Omega} \frac{f_r(p, \omega_i, \omega_o)}{F(\omega_o, h)}F_0 (1 - \alpha)  n \cdot \omega_i d\omega_i+\int_{\Omega} \frac{f_r(p, \omega_i, \omega_o)}{F(\omega_o, h)}\alpha n \cdot \omega_i d\omega_i\]</span> 如此，<spanclass="math inline">\(F_0\)</span>在积分过程中便是一个常量，可以将其提出积分。然后将<span class="math inline">\(\alpha\)</span>代入回到表达式，得到最终的split sum BRDF equation： <spanclass="math display">\[F_0\int_{\Omega} f_r(p, \omega_i, \omega_o)(1 - (1-\omega_o\cdot h)^5)  n \cdot \omega_i d\omega_i+\int_{\Omega} f_r(p, \omega_i, \omega_o)(1-\omega_o\cdot h)^5 n \cdot \omega_i d\omega_i\]</span> 需要注意的是此时表达式中 <spanclass="math inline">\(f_r\)</span> 函数已经不在包含菲涅尔函数<spanclass="math inline">\(F\)</span>。再看最中得到的表达式，<strong>两个积分的结果相当于<span class="math inline">\(F_0\)</span> 的 scale 和 bias</strong>。</p><p>和之前卷积环境贴图类似，可以使用卷积来求的上述 BRDF表达式的积分，其输入是 <span class="math inline">\(n\)</span> 和 <spanclass="math inline">\(\omega_o\)</span>的夹角，以及粗糙度。将卷积后的结果存储在 2D 查找纹理（Look Up Texture,LUT）中，这张纹理被称为 <strong>BRDF 积分贴图（BRDF integrationmap）</strong>，稍后会将其用于 PBR光照着色器中，以获得间接镜面反射的最终卷积结果。</p><p>代码与预滤波器的卷积代码大体相似，不同之处在于，它现在根据 BRDF的几何函数Geometry和 Fresnel-Schlick 近似来处理采样向量：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">vec2 <span class="title">IntegrateBRDF</span><span class="params">(<span class="type">float</span> NdotV, <span class="type">float</span> roughness)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    vec3 V;</span><br><span class="line">    V.x = <span class="built_in">sqrt</span>(<span class="number">1.0</span> - NdotV * NdotV);</span><br><span class="line">    v.y = <span class="number">0.0</span>;</span><br><span class="line">    v.z = NdotV;</span><br><span class="line">    </span><br><span class="line">    <span class="type">float</span> A = <span class="number">0.0</span>;</span><br><span class="line">    <span class="type">float</span> B = <span class="number">0.0</span>;</span><br><span class="line">    </span><br><span class="line">    vec3 N = <span class="built_in">vec3</span>(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>);</span><br><span class="line">    </span><br><span class="line">    <span class="type">const</span> uint SAMPLE_COUNT = <span class="number">1024u</span>;</span><br><span class="line">    <span class="keyword">for</span>(uint i = <span class="number">0u</span>; i &lt; SAMPLE_COUNT; ++i)</span><br><span class="line">    &#123;</span><br><span class="line">        vec2 Xi = <span class="built_in">Hammersley</span>(i, SAMPLE_COUNT);</span><br><span class="line">        vec3 H  = <span class="built_in">ImportanceSampleGGX</span>(Xi, N, roughness);</span><br><span class="line">        vec3 L  = <span class="built_in">normalize</span>(<span class="number">2.0</span> * <span class="built_in">dot</span>(V, H) * H - V);</span><br><span class="line"></span><br><span class="line">        <span class="type">float</span> NdotL = <span class="built_in">max</span>(L.z, <span class="number">0.0</span>);</span><br><span class="line">        <span class="type">float</span> NdotH = <span class="built_in">max</span>(H.z, <span class="number">0.0</span>);</span><br><span class="line">        <span class="type">float</span> VdotH = <span class="built_in">max</span>(<span class="built_in">dot</span>(V, H), <span class="number">0.0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(NdotL &gt; <span class="number">0.0</span>)</span><br><span class="line">        &#123;</span><br><span class="line">            <span class="type">float</span> G = <span class="built_in">GeometrySmith</span>(N, V, L, roughness);</span><br><span class="line">            <span class="type">float</span> G_Vis = (G * VdotH) / (NdotH * NdotV);</span><br><span class="line">            <span class="type">float</span> Fc = <span class="built_in">pow</span>(<span class="number">1.0</span> - VdotH, <span class="number">5.0</span>);</span><br><span class="line"></span><br><span class="line">            A += (<span class="number">1.0</span> - Fc) * G_Vis;</span><br><span class="line">            B += Fc * G_Vis;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    A /= <span class="built_in">float</span>(SAMPLE_COUNT);</span><br><span class="line">    B /= <span class="built_in">float</span>(SAMPLE_COUNT);</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">vec2</span>(A, B);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// ---------------------------------------------------------------------</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">main</span><span class="params">()</span> </span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    vec2 integratedBRDF = <span class="built_in">IntegrateBRDF</span>(TexCoords.x, TexCoords.y);</span><br><span class="line">    FragColor = integratedBRDF;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>如你所见，BRDF卷积部分就是直接将数学公式转换为了代码。将角度 <spanclass="math inline">\(\theta\)</span>和粗糙度作为输入，使用重要性采样，再使用Geometry函数和衍生的Fresnelterm处理，然后输出<spanclass="math inline">\(F_0\)</span>的scale和bias，最后取平均值。</p><p>你可能会想起在PBR理论那篇文章中，BRDF的geometryterm用在IBL的时候是有些微区别的： <span class="math display">\[\begin{align}k_{direct} = \frac{(\alpha + 1)^2}{8}\\k_{IBL} = \frac{\alpha ^ 2}{2}\end{align}\]</span> 此时BRDF的卷积是镜面IBL积分的一部分，因此Schlick-GGXgeometry函数使用 <span class="math inline">\(k_{IBL}\)</span>：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">float</span> <span class="title">GeometrySchlickGGX</span><span class="params">(<span class="type">float</span> NdotV, <span class="type">float</span> roughness)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">float</span> a = roughness;</span><br><span class="line">    <span class="type">float</span> k = (a * a) / <span class="number">2.0</span>;</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> nom   = NdotV;</span><br><span class="line">    <span class="type">float</span> denom = NdotV * (<span class="number">1.0</span> - k) + k;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> nom / denom;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// ---------------------------------------------------------------------</span></span><br><span class="line"><span class="function"><span class="type">float</span> <span class="title">GeometrySmith</span><span class="params">(vec3 N, vec3 V, vec3 L, <span class="type">float</span> roughness)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="type">float</span> NdotV = <span class="built_in">max</span>(<span class="built_in">dot</span>(N, V), <span class="number">0.0</span>);</span><br><span class="line">    <span class="type">float</span> NdotL = <span class="built_in">max</span>(<span class="built_in">dot</span>(N, L), <span class="number">0.0</span>);</span><br><span class="line">    <span class="type">float</span> ggx2 = <span class="built_in">GeometrySchlickGGX</span>(NdotV, roughness);</span><br><span class="line">    <span class="type">float</span> ggx1 = <span class="built_in">GeometrySchlickGGX</span>(NdotL, roughness);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> ggx1 * ggx2;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><p>最后，将BRDF卷积结果存储在一张分辨率为512*512的2D纹理中：</p><p><img src="/images/LearnOpenGL/3-10.png" /></p><p>于是，有了pre-filtered environment map 和 BRDF 2DLUT，使用这两者再根据split sumapproximation就可以重建高光反射积分了。</p><h3 id="completing-the-ibl-reflectance">Completing the IBLreflectance</h3><p>现在就让我们把之前独立分析的两部分结合起来，完成IBL反射。</p><p>将预计算得到的光照数据添加到PBR shader中：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">uniform samplerCube prefilterMap;</span><br><span class="line">uniform sampler2D   brdfLUT;</span><br></pre></td></tr></table></figure><p>第一步，通过反射向量采样 pre-filtered environment map得到表面间接镜面反射。注意，需要根据表面粗糙度采样对应level的贴图，使更粗糙的表面表现出更模糊的高光反射：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">[...]</span><br><span class="line">    vec3 R = <span class="built_in">reflect</span>(-V, N);</span><br><span class="line">    </span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> MAX_REFLECTION_LOD = <span class="number">4.0</span>;</span><br><span class="line">    vec3 prefilteredColor = <span class="built_in">textureLod</span>(prefilterMap, R, roughness * MAX_REFLECTION_LOD).rgb;</span><br><span class="line">    [...]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>接下来，根据法线和视线的夹角以及表面粗糙度采样BRDF LUT：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">vec3 F= <span class="built_in">FresnelSchlickRoughness</span>(<span class="built_in">max</span>(<span class="built_in">dot</span>(N, V), <span class="number">0.0</span>), F0, roughness);</span><br><span class="line">vec2 envBRDF = <span class="built_in">texture</span>(brdfLUT, <span class="built_in">vec2</span>(<span class="built_in">max</span>(<span class="built_in">dot</span>(N, V), <span class="number">0.0</span>), roughness)).rg;</span><br><span class="line">vec3 specular = prefilteredColor * (F * envBRDF.x + envBRDF.y);</span><br></pre></td></tr></table></figure><p>于是，我们得到了反射方程环境光的镜面反射部分。现在，将其预前篇文章中的环境光漫反射部分结合在一起，得到最终的PBRIBL结果：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">vec3 F = <span class="built_in">FresnelSchlickRoughness</span>(<span class="built_in">max</span>(<span class="built_in">dot</span>(N, V), <span class="number">0.0</span>), F0, roughness);</span><br><span class="line"></span><br><span class="line">vec3 kS = F;</span><br><span class="line">vec3 kD = <span class="number">1.0</span> - kS;</span><br><span class="line">kD *= <span class="number">1.0</span> - metallic;</span><br><span class="line"></span><br><span class="line">vec3 irradiance = <span class="built_in">texture</span>(irradianceMap, N).rgb;</span><br><span class="line">vec3 diffuse    = irradiance * albedo;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> <span class="type">float</span> MAX_REFLECTION_LOD = <span class="number">4.0</span>;</span><br><span class="line">vec3 prefilteredColor = <span class="built_in">textureLod</span>(prefilterMap, R,  roughness * MAX_REFLECTION_LOD).rgb;   </span><br><span class="line">vec2 envBRDF  = <span class="built_in">texture</span>(brdfLUT, <span class="built_in">vec2</span>(<span class="built_in">max</span>(<span class="built_in">dot</span>(N, V), <span class="number">0.0</span>), roughness)).rg;</span><br><span class="line">vec3 specular = prefilteredColor * (F * envBRDF.x + envBRDF.y);</span><br><span class="line"></span><br><span class="line">vec3 ambient = (kD * diffuse + specular) * ao; </span><br></pre></td></tr></table></figure><p>现在，在一系列粗糙度和金属度各异的球上运行此代码，我们终于可以在最终的PBR 渲染器中看到其真实颜色：</p><p><img src="/images/LearnOpenGL/3-15.png" /></p>]]></content>
      
      
      
        <tags>
            
            <tag> Shader </tag>
            
            <tag> OpenGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>IBL-Diffuse irradiance</title>
      <link href="/2024/10/22/LearnOpenGL/IBL-Diffuse%20irradiance/"/>
      <url>/2024/10/22/LearnOpenGL/IBL-Diffuse%20irradiance/</url>
      
        <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>IBL，或者称为 image basedlighting，是一系列光照技术的总称，它将周围环境视作一个整体的大光源。IBL通常使用Cubemap（取自现实世界或从3D场景中生成），可以将其中的每个像素视为光源，在渲染方程中直接使用。这种方式可以高效率的获取环境光源，将物体更好的融入环境中。</p><p>由于IBL算法会捕获部分（或整体）环境光源，因此它被认为是一种较为精确的环境光源输入格式，甚至可以算做一种环境光的近似。正是由于此特性，才会在PBR中引入IBL，将环境光照纳入考虑范围可以使光照结果更符合物理规律。</p><h3 id="ibl-in-pbr">IBL in PBR</h3><p>在将IBL引入PBR之前，再来看一眼反射方程： <spanclass="math display">\[L_o(p,\omega_o)=\int_\Omega  (k_d \frac{c}{\pi} + k_s \frac{DFG}{4(w_o\cdot n)(w_i \cdot n)}) L_i(p,\omega_i)n\cdot\omega_id\omega_i\]</span> 正如前篇文章所述，我们的目标是求得入射光线 <spanclass="math inline">\(\omega_i\)</span> 在物体表面法线半球 <spanclass="math inline">\(\Omega\)</span>上的积分，而且要求实时(real-time)求出积分。对于如何获取某一方向 <spanclass="math inline">\(\omega_i\)</span>上的辐照度，正如我们前文所说，可以采用Cubemap来表示场景辐亮度，使用方向向量对其采样即可得到对应的辐亮度。如此对于给定方向向量，获取场景辐照度如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vec3 radiance = <span class="built_in">texture</span>(_cubemapEnvironment, w_i).rgb;</span><br></pre></td></tr></table></figure><p>但是，为了实时渲染，还是需要在fragmentshader中对半球的任何可能方向进行积分，这显然是非常困难的。但还是有一个可行的方法计算半球积分，那就是预处理(pre-process)或者说<strong>预计算(pre-compute)</strong>。</p><p>让我们再回看前文的反射方程，会发现其中的<spanclass="math inline">\(k_d\)</span>项和<spanclass="math inline">\(k_s\)</span>项是相互独立的，可以将diffuse和specular两部分积分分开考虑，<strong>本章节重点在于diffuse部分</strong>。</p><p>将漫反射项中，在积分时的常量部分提出，得到以下公式： <spanclass="math display">\[L_{d}(p,\omega_o)= k_d \frac{c}{\pi}\int_\Omega L_i(p,\omega_i)n\cdot\omega_id\omega_i\]</span> 于是，我们得到了一个只与<spanclass="math inline">\(\omega_i\)</span>有关的积分（假设p在环境贴图的中心，并不影响结果）。然后，我们就可以通过<strong>卷积（convolution）</strong>将每一个采样方向<spanclass="math inline">\(\omega_o\)</span>的积分结果存储于一张新的贴图中。</p><p><img src="/images/LearnOpenGL/3-1.png" /></p><p>这张预计算的环境贴图存储了每个采样方向<spanclass="math inline">\(\omega_o\)</span>积分结果，对于<spanclass="math inline">\(\omega_o\)</span>方向，其值可以当作场景中照射在物体表面的所有环境光的预计算结果。此图被称为辐照度贴图<strong>irradiancemap</strong>，因为经过卷积计算的立方体贴图能让我们从任何方向高效得获取场景（预计算好的）辐照度。</p><blockquote><p>辐射方程也依赖了位置 <spanclass="math inline">\(p\)</span>，不过这里我们假设 <spanclass="math inline">\(p\)</span>位于辐照度图的中心。这就意味着所有漫反射间接光只能来自同一个环境贴图，这样可能会破坏现实感（特别是在室内）。渲染引擎通过在场景中放置多个反射探针(reflectionprobes)来解决此问题，每个反射探针单独预计算其周围环境的辐照度图。这样，位置<span class="math inline">\(p\)</span>处的辐照度（以及辐射度）是取离其最近的反射探针之间的辐照度（辐射度）内插值。目前，我们假设总是从中心采样环境贴图，把反射探针的讨论留给后面的教程。</p></blockquote><p>下面是一个环境立方体贴图及其生成的辐照度图的示例（由 <ahref="http://www.indiedb.com/features/using-image-based-lighting-ibl">Wave引擎</a>提供），每个方向 <span class="math inline">\(\omega_o\)</span>的场景辐射度取平均值。由于立方体贴图每个纹素中存储了（<spanclass="math inline">\(\omega_o\)</span>方向的）卷积结果，辐照度图看起来有点像环境的平均颜色或光照图。使用任何一个向量对立方体贴图进行采样，就可以获取该方向上的场景辐照度。</p><p><img src="/images/LearnOpenGL/3-2.png" /></p><h3 id="pbr-and-hdr">PBR and HDR</h3><p>在之前的章节有提到过：<strong>在PBR管线中使用HDR非常重要。</strong></p><p>由于PBR的大部分输入都基于实际物理属性和测量，因此为入射光线找到其物理等效值很重要。无论是对光线辐射通量的研究性猜测，还是使用它们的直接物理等效值，诸如灯泡和太阳之间的差异不应该被忽视。如果不在HDR环境中进行工作，很难正确处理光源之间的相对强度。</p><p>所以说PBR和HDR是相辅相成的，但是这与IBL有何关系？回想一下前文，我们将环境光强度赋予在环境贴图的颜色值上，而之前使用的Cubemap属于低动态范围LDR，其颜色值介于0.0和1.0之间，不适合用作物理输入参数。于是，我们需要某种方式将光照的高动态范围HDR存储于环境贴图中。</p><h4 id="the-radiance-hdr-file-format">The radiance HDR file format</h4><p>辐射度文件格式（.hdr扩展名）存储了一张完整的立方体贴图，六个面的数据都为浮点数，可以使光线具有更精准的颜色强度。此文件格式使用了一个技巧来存储浮点值：每个通道存储8位，再以alpha通道存放指数。虽然会导致精度损失，但是效率更高。下面是一个HDR环境贴图的png示例：</p><p><img src="/images/LearnOpenGL/colorful_alley_sphere.png" /></p><p>这可能与您期望的完全不同，因为图像非常扭曲，并且没有我们之前看到的环境贴图的六个立方体贴图面。这张环境贴图是从球体投影到平面上，以使我们可以轻松地将环境信息存储到一张等距柱状投影图(EquirectangularMap)中。有一点确实需要说明：水平视角附近分辨率较高，而底部和顶部方向分辨率较低,在大多数情况下，这是一个不错的折衷方案，因为对于几乎所有渲染器来说，大部分有意义的光照和环境信息都在水平视角附近方向。</p><h4 id="hdr-and-stb_image.h">HDR and stb_image.h</h4><p>直接加载HDR图像需要一些文件格式（fileformat）知识，虽然不会很困难但是很麻烦。但幸运的是，我们使用的头文件stb_image.h支持直接将HDR图像加载为一个浮点数组（anarray of floating points values），这正是我们需要的。示例代码如下:</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&quot;stb_image.h&quot;</span></span></span><br><span class="line">[...]</span><br><span class="line"></span><br><span class="line"><span class="built_in">stbi_set_flip_vertically_on_load</span>(<span class="literal">true</span>);</span><br><span class="line"><span class="type">int</span> width, height, nrComponents;</span><br><span class="line"><span class="type">float</span> *data = <span class="built_in">stbi_loadf</span>(<span class="string">&quot;newport_loft.hdr&quot;</span>, &amp;width, &amp;height, &amp;nrComponents, <span class="number">0</span>);</span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> hdrTexture;</span><br><span class="line"><span class="keyword">if</span>(data)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="built_in">glGenTextures</span>(<span class="number">1</span>, &amp;hdrTexture);</span><br><span class="line">    <span class="built_in">glBindTexture</span>(GL_TEXTURE_2D, hdrTexture);</span><br><span class="line">    <span class="built_in">glTexImage2D</span>(GL_TEXTURE_2D, <span class="number">0</span>, GL_RGB16F, width, height, <span class="number">0</span>, GL_RGB, GL_FLOAT, data);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WARP_S, GL_CLAMP_TO_EDGE);</span><br><span class="line">    <span class="built_in">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_WARP_T, GL_CLAMP_TO_EDGE);</span><br><span class="line">    <span class="built_in">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINERAR);</span><br><span class="line">    <span class="built_in">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINERAR);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">stbi_image_free</span>(data);</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">&#123;</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;Failed to load HDR image.&quot;</span> &lt;&lt; std::endl;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>stb_image.h 自动将 HDR 值映射到一个浮点数列表：默认情况下，每个颜色 3个通道，每个通道32位。这就是将等距柱状投影 HDR 环境贴图转存到 2D浮点纹理中要做的全部工作。</p><h4 id="from-equirectangular-to-cubemap">From Equirectangular toCubemap</h4><p>可以直接使用等距柱状投影图来获取环境信息，但是相比之下，直接采样立方体贴图的性能更高。因此，本文先将等距柱状投影图转换为立方体贴图，以备进一步处理。</p><p>要将等距柱状投影图转换为立方体贴图，需要渲染一个立方体，并从内部将等距柱状投影图投影到立方体的每个面，并将立方体的六个面图像构造为立方体贴图。此立方体的顶点着色器按照原样渲染立方体，并将其局部坐标作为3D采样向量传递给片段着色器：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line"><span class="built_in">layout</span> (location = <span class="number">0</span>) in vec3 aPos;</span><br><span class="line"></span><br><span class="line">out vec3 localPos;</span><br><span class="line"></span><br><span class="line">uniform mat4 projection;</span><br><span class="line">uniform mat4 view;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    localPos = aPos;</span><br><span class="line">    gl_Position = projection * view * <span class="built_in">vec4</span>(localPos, <span class="number">1.0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>而在片段着色器中，我们为立方体的每个部分着色，方法类似于将等距柱状投影图整齐地折叠到立方体的每个面一样。为了实现这一点，我们先获取片段的采样方向，这个方向是从立方体的局部坐标进行插值得到的，然后使用此方向向量和一些三角学数学魔法对等距柱状投影图进行采样，如同其就是立方体图本身一样。我们直接将结果存储到立方体每个面的片段中，以下就是我们需要做的：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line">out vec4 FragColor;</span><br><span class="line">in vec3 localPos;</span><br><span class="line"></span><br><span class="line">uniform sampler2D equirectangularMap;</span><br><span class="line"></span><br><span class="line"><span class="type">const</span> vec2 invAtan = <span class="built_in">vec2</span>(<span class="number">0.1591</span>, <span class="number">0.3183</span>)</span><br><span class="line">vec2 <span class="built_in">SampleSphericalMap</span>(vec3 v)</span><br><span class="line">&#123;</span><br><span class="line">    vec2 uv = <span class="built_in">vec2</span>(<span class="built_in">atan</span>(v.z, v.x), <span class="built_in">asin</span>(v.y));</span><br><span class="line">    uv *= invAtan;</span><br><span class="line">    uv += <span class="number">0.5</span>;</span><br><span class="line">    <span class="keyword">return</span> uv;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// make sure to normalize localPos</span></span><br><span class="line">    vec2 uv = <span class="built_in">SampleSphericalMap</span>(<span class="built_in">normalize</span>(localPos));</span><br><span class="line">    vec3 color = <span class="built_in">texture</span>(equirectangularMap, uv).rgb;</span><br><span class="line"></span><br><span class="line">    FragColor = <span class="built_in">vec4</span>(color, <span class="number">1.0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>渲染出来的图像如下：</p><p><img src="/images/LearnOpenGL/3-11.png" /></p><p>图像结果显示我们有效地将等距柱状图映射到了立方体，但是还需要将源HDR图像转换为立方体贴图。为了实现这一点，我们必须渲染同一个立方体六次，每次面对立方体的一个面，并用帧缓冲framebuffer对象记录其结果：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> captureFBO, captureRBO;</span><br><span class="line"><span class="built_in">glGenFramebuffers</span>(<span class="number">1</span>, &amp;captureFBO);</span><br><span class="line"><span class="built_in">glGenRenderbuffers</span>(<span class="number">1</span>, &amp;captureRBO);</span><br><span class="line"></span><br><span class="line"><span class="built_in">glBindFramebuffer</span>(GL_FRAMEBUFFER, captureFBO);</span><br><span class="line"><span class="built_in">glBindRenderbuffer</span>(GL_RENDERBUFFER, captureRBO);</span><br><span class="line"><span class="built_in">glRenderbufferStorage</span>(GL_RENDERBUFFER, GL_DEPTH_COMPONENT24, <span class="number">512</span>, <span class="number">512</span>);</span><br><span class="line"><span class="built_in">glFramebufferRenderbuffer</span>(GL_FRAMEBUFFER, GL_DEPTH_ATTACHMENT, GL_RENDERBUFFER, captureRBO);  </span><br><span class="line"></span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> envCubemap;</span><br><span class="line"><span class="built_in">glGenTextures</span>(<span class="number">1</span>, &amp;envCubemap);</span><br><span class="line"><span class="built_in">glBindTexture</span>(GL_TEXTURE_CUBE_MAP, envCubemap);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">6</span>; ++i)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// note that we store each face with 16 bit floating point values</span></span><br><span class="line">    <span class="built_in">glTexImage2D</span>(GL_TEXTURE_CUBE_MAP_POSITIVE_X + i, <span class="number">0</span>, GL_RGB16F, </span><br><span class="line">                 <span class="number">512</span>, <span class="number">512</span>, <span class="number">0</span>, GL_RGB, GL_FLOAT, <span class="literal">nullptr</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">glTexParameteri</span>(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_WRAP_S, GL_CLAMP_TO_EDGE);</span><br><span class="line"><span class="built_in">glTexParameteri</span>(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_WRAP_T, GL_CLAMP_TO_EDGE);</span><br><span class="line"><span class="built_in">glTexParameteri</span>(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_WRAP_R, GL_CLAMP_TO_EDGE);</span><br><span class="line"><span class="built_in">glTexParameteri</span>(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_MIN_FILTER, GL_LINEAR);</span><br><span class="line"><span class="built_in">glTexParameteri</span>(GL_TEXTURE_CUBE_MAP, GL_TEXTURE_MAG_FILTER, GL_LINEAR);</span><br></pre></td></tr></table></figure><p>那剩下要做的就是将等距柱状 2D 纹理捕捉到立方体贴图的面上。</p><p>之前在<ahref="https://learnopengl-cn.github.io/04%20Advanced%20OpenGL/05%20Framebuffers/">帧缓冲</a>和<ahref="https://learnopengl-cn.github.io/05%20Advanced%20Lighting/03%20Shadows/02%20Point%20Shadows/">点阴影</a>教程中讨论过的代码细节，我就不再次详细说明，实际过程可以概括为：面向立方体六个面设置六个不同的视图矩阵，给定投影矩阵的fov 为 90度以捕捉整个面，并渲染立方体六次，将结果存储在浮点帧缓冲中：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">glm::mat4 captureProjection = glm::<span class="built_in">perspective</span>(glm::<span class="built_in">radians</span>(<span class="number">90.0f</span>), <span class="number">1.0f</span>, <span class="number">0.1f</span>, <span class="number">10.0f</span>);</span><br><span class="line">glm::mat4 captureViews[] = </span><br><span class="line">&#123;</span><br><span class="line">   glm::<span class="built_in">lookAt</span>(glm::<span class="built_in">vec3</span>(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>), glm::<span class="built_in">vec3</span>( <span class="number">1.0f</span>,  <span class="number">0.0f</span>,  <span class="number">0.0f</span>), glm::<span class="built_in">vec3</span>(<span class="number">0.0f</span>, <span class="number">-1.0f</span>,  <span class="number">0.0f</span>)),</span><br><span class="line">   glm::<span class="built_in">lookAt</span>(glm::<span class="built_in">vec3</span>(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>), glm::<span class="built_in">vec3</span>(<span class="number">-1.0f</span>,  <span class="number">0.0f</span>,  <span class="number">0.0f</span>), glm::<span class="built_in">vec3</span>(<span class="number">0.0f</span>, <span class="number">-1.0f</span>,  <span class="number">0.0f</span>)),</span><br><span class="line">   glm::<span class="built_in">lookAt</span>(glm::<span class="built_in">vec3</span>(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>), glm::<span class="built_in">vec3</span>( <span class="number">0.0f</span>,  <span class="number">1.0f</span>,  <span class="number">0.0f</span>), glm::<span class="built_in">vec3</span>(<span class="number">0.0f</span>,  <span class="number">0.0f</span>,  <span class="number">1.0f</span>)),</span><br><span class="line">   glm::<span class="built_in">lookAt</span>(glm::<span class="built_in">vec3</span>(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>), glm::<span class="built_in">vec3</span>( <span class="number">0.0f</span>, <span class="number">-1.0f</span>,  <span class="number">0.0f</span>), glm::<span class="built_in">vec3</span>(<span class="number">0.0f</span>,  <span class="number">0.0f</span>, <span class="number">-1.0f</span>)),</span><br><span class="line">   glm::<span class="built_in">lookAt</span>(glm::<span class="built_in">vec3</span>(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>), glm::<span class="built_in">vec3</span>( <span class="number">0.0f</span>,  <span class="number">0.0f</span>,  <span class="number">1.0f</span>), glm::<span class="built_in">vec3</span>(<span class="number">0.0f</span>, <span class="number">-1.0f</span>,  <span class="number">0.0f</span>)),</span><br><span class="line">   glm::<span class="built_in">lookAt</span>(glm::<span class="built_in">vec3</span>(<span class="number">0.0f</span>, <span class="number">0.0f</span>, <span class="number">0.0f</span>), glm::<span class="built_in">vec3</span>( <span class="number">0.0f</span>,  <span class="number">0.0f</span>, <span class="number">-1.0f</span>), glm::<span class="built_in">vec3</span>(<span class="number">0.0f</span>, <span class="number">-1.0f</span>,  <span class="number">0.0f</span>))</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// convert HDR equirectangular environment map to cubemap equivalent</span></span><br><span class="line">equirectangularToCubemapShader.<span class="built_in">use</span>();</span><br><span class="line">equirectangularToCubemapShader.<span class="built_in">setInt</span>(<span class="string">&quot;equirectangularMap&quot;</span>, <span class="number">0</span>);</span><br><span class="line">equirectangularToCubemapShader.<span class="built_in">setMat4</span>(<span class="string">&quot;projection&quot;</span>, captureProjection);</span><br><span class="line"><span class="built_in">glActiveTexture</span>(GL_TEXTURE0);</span><br><span class="line"><span class="built_in">glBindTexture</span>(GL_TEXTURE_2D, hdrTexture);</span><br><span class="line"></span><br><span class="line"><span class="built_in">glViewport</span>(<span class="number">0</span>, <span class="number">0</span>, <span class="number">512</span>, <span class="number">512</span>); <span class="comment">// don&#x27;t forget to configure the viewport to the capture dimensions.</span></span><br><span class="line"><span class="built_in">glBindFramebuffer</span>(GL_FRAMEBUFFER, captureFBO);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">unsigned</span> <span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">6</span>; ++i)</span><br><span class="line">&#123;</span><br><span class="line">    equirectangularToCubemapShader.<span class="built_in">setMat4</span>(<span class="string">&quot;view&quot;</span>, captureViews[i]);</span><br><span class="line">    <span class="built_in">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, </span><br><span class="line">                           GL_TEXTURE_CUBE_MAP_POSITIVE_X + i, envCubemap, <span class="number">0</span>);</span><br><span class="line">    <span class="built_in">glClear</span>(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);</span><br><span class="line"></span><br><span class="line">    <span class="built_in">renderCube</span>(); <span class="comment">// renders a 1x1 cube</span></span><br><span class="line">&#125;</span><br><span class="line"><span class="built_in">glBindFramebuffer</span>(GL_FRAMEBUFFER, <span class="number">0</span>);  </span><br></pre></td></tr></table></figure><p>采用帧缓冲的colorattachment并围绕立方体贴图的每个面切换纹理目标，直接将场景渲染到立方体贴图的面上，此立方体贴图envCubemap 就应该是源 HDR 图的环境立方体贴图版。</p><p>让我们编写一个非常简单的天空盒着色器来测试立方体贴图，用来显示周围的立方体贴图：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line"><span class="built_in">layout</span> (location = <span class="number">0</span>) in vec3 aPos;</span><br><span class="line"></span><br><span class="line">uniform mat4 projection;</span><br><span class="line">uniform mat4 view;</span><br><span class="line"></span><br><span class="line">out vec3 localPos;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    localPos = aPos;</span><br><span class="line"><span class="comment">// remove translation from the view matrix</span></span><br><span class="line">    mat4 rotView = <span class="built_in">mat4</span>(<span class="built_in">mat3</span>(view)); </span><br><span class="line">    vec4 clipPos = projection * rotView * <span class="built_in">vec4</span>(localPos, <span class="number">1.0</span>);</span><br><span class="line"></span><br><span class="line">    gl_Position = clipPos.xyww;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>片段着色器直接使用立方体的片段局部坐标，对环境立方体贴图采样：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#version 330 core</span></span><br><span class="line">out vec4 FragColor;</span><br><span class="line"></span><br><span class="line">in vec3 localPos;</span><br><span class="line"></span><br><span class="line">uniform samplerCube environmentMap;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    vec3 envColor = <span class="built_in">texture</span>(environmentMap, localPos).rgb;</span><br><span class="line"></span><br><span class="line">    envColor = envColor / (envColor + <span class="built_in">vec3</span>(<span class="number">1.0</span>));</span><br><span class="line">    envColor = <span class="built_in">pow</span>(envColor, <span class="built_in">vec3</span>(<span class="number">1.0</span>/<span class="number">2.2</span>)); </span><br><span class="line"></span><br><span class="line">    FragColor = <span class="built_in">vec4</span>(envColor, <span class="number">1.0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>现在，在之前渲染的球体上渲染环境贴图，效果应该如下图：</p><p><img src="/images/LearnOpenGL/3-12.png" /></p><h3 id="cubemap-convolution">Cubemap convolution</h3><p>在半球<spanclass="math inline">\(\Omega\)</span>任意可能方向上采样环境光在计算方面还是不具有可行性的，因为可能方向的个数理论上是无限大的。但是，可以通过尽量多的有限采样来近似上述结果，更有效率的求出半球上的积分，例如均匀的、或随机的在半球面上采样方向。在渲染时，采用半球的方向则通过表面法线normal来确定。</p><p>有很多方法可以对环境贴图进行卷积，本文使用的方法是：<strong>对于立方体贴图的每个纹素，在纹素所代表的方向半球<spanclass="math inline">\(\Omega\)</span>内生成固定数量的采样向量，并对采样结果取平均值。</strong>数量固定的采样向量均匀分布在半球内部。</p><p>从前文的公式中可以看到，反射方程是对立体角 <spanclass="math inline">\(dw\)</span> 的积分<spanclass="math inline">\(\int\)</span>，处理起来有些麻烦。为此，我们使用球坐标系的<span class="math inline">\(\theta\)</span> 和 <spanclass="math inline">\(\phi\)</span> 来等效替代立体角 <spanclass="math inline">\(dw\)</span>。</p><p><img src="/images/LearnOpenGL/3-3.png" /></p><p>对于围绕半圆的航向角 <span class="math inline">\(\phi\)</span> ，在<span class="math inline">\(0\)</span> 到 <spanclass="math inline">\(2\pi\)</span> 采样；对于从半球顶点出发的倾斜角<span class="math inline">\(\theta\)</span>， 在 <spanclass="math inline">\(0\)</span> 到 <spanclass="math inline">\(\frac{1}{2}\pi\)</span>采样。于是反射积分方程可以写为： <spanclass="math display">\[L_o(p,\phi_o, \theta_o) = k_d \frac{c}{\pi}\int_{\phi=0}^{2\pi} \int_{\theta=0}^{\frac{1}{2} \pi}L_i(p,\phi_i,\theta_i) cos(\theta) sin(\theta) d\phi d\theta\]</span> 求解此积分需要在半球<spanclass="math inline">\(\Omega\)</span>中采样固定此书，然后求平均值。分别给每个球坐标指定离散样本数量<spanclass="math inline">\(n_1\)</span> 和 <spanclass="math inline">\(n_2\)</span> 以求其黎曼和，积分转换为离散版本为：<span class="math display">\[L_o(p,\phi_o, \theta_o) = k_d \frac{c\pi}{n_1 n_2}\sum\limits_{\phi=0}^{n_1} \sum\limits_{\theta=0}^{n_2}L_i(p,\phi_i,\theta_i) cos(\theta) sin(\theta)\]</span>当我们离散地对两个球坐标轴进行采样时，每个采样近似代表了半球上的一小块区域。由于球的一般特性，当采样区域靠近中心顶部时，天顶角<spanclass="math inline">\(\theta\)</span>变小，采样区域也会变小。为了平衡较小区域的贡献度，使用<spanclass="math inline">\(sin\theta\)</span>来作为权重。</p><p>对半球离散采样代码如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">vec3 irradiance = <span class="built_in">vec3</span>(<span class="number">0.0</span>);  </span><br><span class="line"></span><br><span class="line">vec3 up    = <span class="built_in">vec3</span>(<span class="number">0.0</span>, <span class="number">1.0</span>, <span class="number">0.0</span>);</span><br><span class="line">vec3 right = <span class="built_in">normalize</span>(<span class="built_in">cross</span>(up, normal));</span><br><span class="line">up         = <span class="built_in">normalize</span>(<span class="built_in">cross</span>(normal, right));</span><br><span class="line"></span><br><span class="line"><span class="type">float</span> sampleDelta = <span class="number">0.025</span>;</span><br><span class="line"><span class="type">float</span> nrSamples = <span class="number">0.0</span>; </span><br><span class="line"><span class="keyword">for</span>(<span class="type">float</span> phi = <span class="number">0.0</span>; phi &lt; <span class="number">2.0</span> * PI; phi += sampleDelta)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">float</span> theta = <span class="number">0.0</span>; theta &lt; <span class="number">0.5</span> * PI; theta += sampleDelta)</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment">// spherical to cartesian (in tangent space)</span></span><br><span class="line">        vec3 tangentSample = <span class="built_in">vec3</span>(<span class="built_in">sin</span>(theta) * <span class="built_in">cos</span>(phi),  <span class="built_in">sin</span>(theta) * <span class="built_in">sin</span>(phi), <span class="built_in">cos</span>(theta));</span><br><span class="line">        <span class="comment">// tangent space to world</span></span><br><span class="line">        vec3 sampleVec = tangentSample.x * right + tangentSample.y * up + tangentSample.z * N; </span><br><span class="line"></span><br><span class="line">        irradiance += <span class="built_in">texture</span>(environmentMap, sampleVec).rgb * <span class="built_in">cos</span>(theta) * <span class="built_in">sin</span>(theta);</span><br><span class="line">        nrSamples++;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line">irradiance = PI * irradiance * (<span class="number">1.0</span> / <span class="built_in">float</span>(nrSamples));</span><br></pre></td></tr></table></figure><p>我们指定了一个固定值sampleDelta作为差值去采样整个半球，增大或减小这个值会影响近似的精度。在循环中，首先获取一个球面坐标并将它转换为3D直角坐标向量，然后将该向量从切线空间装换到世界空间，并使用此向量采样HDR环境贴图，最后取平均值作为辐照度。代码中的<spanclass="math inline">\(cos\theta\)</span>之前提到过，与表面夹角越大的光线对表面的贡献越小，而<spanclass="math inline">\(sin\theta\)</span>正式刚刚提到的贡献区域大小问题。</p><p>于是最后终于得到了一张预计算好的辐射度贴图，可以直接用于IBL。</p><h3 id="pbr-and-indirect-irradiance-lighting">PBR and indirectirradiance lighting</h3><p>irradiancemap代表了环境光反射积分中的漫反射部分。我们将除去重要光源外的所有周围光源当作环境光，取代原本PBR中的环境光常量。</p><p>有了上文的铺垫，如今只需要采样irradiancemap即可得到环境光的漫反射部分：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">uniform samplerCube irradianceMap;</span><br><span class="line"></span><br><span class="line"><span class="comment">// vec3 ambient = vec3(0.03);</span></span><br><span class="line">vec3 ambient = <span class="built_in">texture</span>(irradianceMap, N).rgb;</span><br></pre></td></tr></table></figure><p>由于环境光也包括漫反射和镜面反射两部分，正如我们从分割版的反射方程中看到的那样，需要对漫反射部分进行加权处理。此处，使用菲涅尔公式来计算表面的间接反射率，我们从中得出折射率或称漫反射率：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vec3 kS = <span class="built_in">fresnelSchlick</span>(<span class="built_in">max</span>(<span class="built_in">dot</span>(N, V), <span class="number">0.0</span>), F0);</span><br><span class="line">vec3 kD = <span class="number">1.0</span> - kS;</span><br><span class="line">vec3 irradiance = <span class="built_in">texture</span>(irradianceMap, N).rgb;</span><br><span class="line">vec3 diffuse = irradiance * albedo;</span><br><span class="line">vec3 ambient = (kD * diffuse) * ao;</span><br></pre></td></tr></table></figure><p>由于环境光来自于法线 <span class="math inline">\(N\)</span>所在半球的所有方向，因此没有一个确切的半程向量来计算菲涅尔效应。为了任然能够模拟菲涅尔公式的输入，我们目前没有考虑粗糙度问题，表面反射率相对较高。环境光与直接光应具有相同属性，我们希望粗糙表面的边缘具有较弱的反射率。由于没有考虑到上述问题，间接菲涅尔反射在粗糙度非金属表面看起来有些过强（为了演示略微夸大）：</p><p><img src="/images/LearnOpenGL/3-13.png" /></p><p>我们可以通过在 <ahref="https://seblagarde.wordpress.com/2011/08/17/hello-world/">SébastienLagarde</a> 提出的 Fresnel-Schlick方程中加入粗糙度项来缓解这个问题：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">vec3 <span class="title">fresnelSchlickRoughness</span><span class="params">(<span class="type">float</span> cosTheta, vec3 F0, <span class="type">float</span> roughness)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">return</span> F0 + (<span class="built_in">max</span>(<span class="built_in">vec3</span>(<span class="number">1.0</span> - roughness), F0) - F0) * <span class="built_in">pow</span>(<span class="number">1.0</span> - cosTheta, <span class="number">5.0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在计算菲涅耳效应时纳入表面粗糙度，环境光代码最终确定为：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">vec3 kS = <span class="built_in">fresnelSchlickRoughness</span>(<span class="built_in">max</span>(<span class="built_in">dot</span>(N, V), <span class="number">0.0</span>), F0, roughness); </span><br><span class="line">vec3 kD = <span class="number">1.0</span> - kS;</span><br><span class="line">vec3 irradiance = <span class="built_in">texture</span>(irradianceMap, N).rgb;</span><br><span class="line">vec3 diffuse    = irradiance * albedo;</span><br><span class="line">vec3 ambient    = (kD * diffuse) * ao;</span><br></pre></td></tr></table></figure><p>如你所见，实际PBR中IBL计算相当简单，只需要采样一张立方体贴图，大部分工作都通过预计算来完成。在场景中排列金属度沿垂直方向递增、粗糙度沿水平方向递增的金属球体，在添加IBL的漫反射部分后，其看起来如下：</p><p><img src="/images/LearnOpenGL/3-14.png" /></p>]]></content>
      
      
      
        <tags>
            
            <tag> Shader </tag>
            
            <tag> OpenGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Equirectangular projection</title>
      <link href="/2024/10/03/Equirectangular%20Projection/"/>
      <url>/2024/10/03/Equirectangular%20Projection/</url>
      
        <content type="html"><![CDATA[<p>equirectangular projection 又称 equidistant cylindricalprojection（其实是它的一种特例），国内主要翻译为<strong>等距柱状投影</strong>，是一种简单的地图投影，用于将球体表面投影到圆柱体竖直表面（等效于平面）。一般认为是马里纳斯（Marinusof Tyre）最早用于绘制海图。</p><p>这种投影将<strong>经线</strong>映射为<strong>等距的垂直直线</strong>，将<strong>纬线</strong>映射为<strong>等距的水平直线</strong>。投影的面积和角度都会改变，会扭曲球体表面内容，多用于地图测绘。</p><figure><img src="/images/3DMath/equirectangular-projection-1.png"alt="distortions -- wiki" /><figcaption aria-hidden="true">distortions -- wiki</figcaption></figure><p>其属于Cylindrical projection的一种，相关的其他技术就不细说了。</p><p><img src="/images/3DMath/equirectangular-projection-2.png" /></p><h3 id="definition">Definition</h3><p>正投影（forward projection）会将球面坐标变为平面坐标，反投影（reverseprojection）会将平面重新变为球体。使用以下定义描述球体：</p><ul><li><span class="math inline">\(\lambda\)</span>是被投影位置的经度；</li><li><span class="math inline">\(\varphi\)</span>是被投影位置的维度；</li><li><span class="math inline">\(\varphi _1\)</span> 是基准线(standardline)的维度，在投影前后其长度并未被扭曲；</li><li><span class="math inline">\(\varphi _0\)</span> 是地图的centralparallel；</li><li><span class="math inline">\(\lambda _0\)</span> 是地图的centralmeridian；</li><li><span class="math inline">\(x\)</span> 是投影之后的水平坐标；</li><li><span class="math inline">\(y\)</span> 是投影之后的垂直坐标；</li><li><span class="math inline">\(R\)</span> 是球体半径；</li></ul><p><img src="/images/3DMath/equirectangular-projection-1.png" /></p><h4 id="forward">forward</h4><p><span class="math display">\[\begin{aligned}x&amp;=R(\lambda -\lambda _{0})\cos \varphi _{1}\\y&amp;=R(\varphi -\varphi _{0})\end{aligned}\]</span></p><p>当 <span class="math inline">\(\varphi_1 = 0\)</span>时，基准线位于赤道，只有赤道长度没有被扭曲。此投影将经度值映射为 <spanclass="math inline">\(x\)</span>，将纬度值映射为 <spanclass="math inline">\(y\)</span>，所以有时也被称作 latitude/longitude 或lat/lon(g) 投影。</p><p>当 <span class="math inline">\(\varphi_1 \neq 0\)</span> 时，例如<span class="math inline">\(\varphi_1 = 36°\)</span> 或者 <spanclass="math inline">\(\varphi_1 =37.5°\)</span>时，只有对应部分纬线长度没有被扭曲，其他部分仍然被扭曲了。放几个图就容易理解了：</p><figure><img src="/images/3DMath/equirectangular-projection-4.png"alt="Equirectangular Projection" /><figcaption aria-hidden="true">Equirectangular Projection</figcaption></figure><figure><img src="/images/3DMath/equirectangular-projection-5.png"alt="Miller Equidistant Projection" /><figcaption aria-hidden="true">Miller EquidistantProjection</figcaption></figure><h4 id="reverse">Reverse</h4><p><span class="math display">\[\begin{aligned}\lambda &amp;={\frac {x}{R\cos \varphi _{1}}}+\lambda _{0}\\\varphi &amp;={\frac {y}{R}}+\varphi _{0}\end{aligned}\]</span></p><p>注：有时 <span class="math inline">\(\lambda\)</span>也被称作"yaw"，<span class="math inline">\(\varphi\)</span> 被称作"pitch"。无论名称如何，都代表角度。</p><h3 id="参考文献">参考文献</h3><p>https://en.wikipedia.org/wiki/Equirectangular_projection</p><p>https://www.youtube.com/watch?v=EnW7TPnDN8Y</p><p>https://mathworld.wolfram.com/EquirectangularProjection.html</p>]]></content>
      
      
      <categories>
          
          <category> ink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D Math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnOpenGL - Framebuffers</title>
      <link href="/2024/10/02/LearnOpenGL/Framebuffers/"/>
      <url>/2024/10/02/LearnOpenGL/Framebuffers/</url>
      
        <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>渲染中我们会用到很多类型的屏幕缓存：</p><ul><li>color buffer 颜色缓存，用于存储颜色值</li><li>depth buffer 深度缓存，用于存储深度和进行深度测试</li><li>stencil buffer模板缓存，用于进行模板测试，通过条件决定是否丢弃像素值</li></ul><p>这一系列缓存都存储于GPU中被称作<strong>framebuffer（帧缓存）</strong>的地方，OpenGL允许我们创建帧缓存用于存储渲染过程中的中间产物。这些帧缓存可以用来实现许多更复杂的效果，如镜像、后处理特效等。</p><h3 id="opengl中的framebuffer">OpenGL中的framebuffer</h3><p>像是openGL中的其他object一样，可以通过这样一系列过程创建和使用framebuffer：先创建一个framebufferobject，将其 bind 到GL_FRAMEBUFFER，做一些operations，最后再unbind。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> fbo;</span><br><span class="line"><span class="built_in">glGenFramebuffers</span>(<span class="number">1</span>, &amp;fbo);</span><br><span class="line"><span class="built_in">glBindFramebuffer</span>(GL_FRAMEBUFFER, fbo);</span><br></pre></td></tr></table></figure><p>只进行如上操作，framebuffer依然是不可用的。一个framebuffer被完整创建，还需要满足以下条件：</p><ul><li>需要为其链接（attach）至少一个缓存（color、depth 或 stencil）；</li><li>应该至少有一个 color attachment；</li><li>所有 attachments 也应该已经创建完成（reserved memory)；</li><li>每一个缓存都拥有相同的 samples；</li></ul><p>不懂什么是 samples 没关系，Anti-Aliasing章节里面会讲到。</p><p>随后使用函数<code>glCheckFramebufferStatus</code>检查帧缓存状态，当状态为<code>GL_FRAMEBUFFER_COMPLETE</code>时，代表帧缓存可以正式使用。</p><p>这里解释一个名词<strong>attachment</strong>，其代表一个存储位置，可以做为帧缓存的一个buffer使用，类似于一个图像。在创建attachment 时有两种选择：textures, renderbuffer objects。</p><h4 id="texture-attachments">Texture attachments</h4><p>当使用 texture 作为帧缓存的 attachments时，就和普通的颜色、深度或者模板缓存一样，所有的渲染指令都会写入到这张texture 中。其优点是，可以轻松得将 texture 在 shader中使用。使用如下函数将 texture链接给帧缓存的颜色缓冲，对于深度缓冲和模板缓冲的链接具体方式请参考learnopengl文章：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, texture, <span class="number">0</span>);</span><br></pre></td></tr></table></figure><h4 id="renderbuffer-object-attachments">Renderbuffer objectattachments</h4><p>renderbuffer object 是一个真正的 buffer，像是一组btyes，integers，pixels 等。render buffer object不能被直接读取，于是相比于 texture，openGL可以对 renderbuffer object做一些内存优化使其获得离屏渲染的性能优势。</p><p>由于 renderbuffer object 是只写的，因此经常被用作 depth/stencilattachments，因为大多数情况下我们并不需要从中读取数据，只关心深度测试和模板测试。我们需要深度和模板值用于测试，但是不需要（sample）采样这些值。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">glRenderbufferStorage</span>(GL_RENDERBUFFER, GL_DEPTH24_STENCIL8, <span class="number">800</span>, <span class="number">600</span>);</span><br><span class="line"><span class="built_in">glFramebufferRenderbuffer</span>(GL_FRAMEBUFFER, GL_DEPTH_STENCIL_ATTACHMENT, GL_RENDERBUFFER, rbo);</span><br></pre></td></tr></table></figure><h3 id="post-processing">Post-processing</h3><p>在了解了framebuffer如何工作之后，就是好好利用它做一些有趣的事情了。我们打算将一个场景渲染到创建的帧缓存中，然后将其colorattachment渲染到一个quad上，随后我们就可以使用这张 texture做后处理效果了。</p><h4 id="rendering-to-a-texture">Rendering to a texture</h4><p>先来看准备工作，这里只列出一些核心代码。</p><p>根据上文中提到的一系列操作，先创建出一个待使用的framebuffer：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">unsigned</span> <span class="type">int</span> framebuffer;</span><br><span class="line"><span class="built_in">glGenFramebuffers</span>(<span class="number">1</span>, &amp;framebuffer);</span><br><span class="line"><span class="built_in">glBindFramebuffer</span>(GL_FRAMEBUFFER, framebuffer);</span><br><span class="line"><span class="comment">// generate texture</span></span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> textureColorbuffer;</span><br><span class="line"><span class="built_in">glGenTextures</span>(<span class="number">1</span>, &amp;textureColorbuffer);</span><br><span class="line"><span class="built_in">glTexImage2D</span>(GL_TEXTURE_2D, textureColorbuffer);</span><br><span class="line"><span class="built_in">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MIN_FILTER, GL_LINEAR);</span><br><span class="line"><span class="built_in">glTexParameteri</span>(GL_TEXTURE_2D, GL_TEXTURE_MAG_FILTER, GL_LINEAR);</span><br><span class="line"><span class="built_in">glBindTexture</span>(GL_TEXTURE_2D, <span class="number">0</span>);</span><br><span class="line"><span class="comment">// attach</span></span><br><span class="line"><span class="built_in">glFramebufferTexture2D</span>(GL_FRAMEBUFFER, GL_COLOR_ATTACHMENT0, GL_TEXTURE_2D, textureColorbuffer, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line"><span class="comment">// generate renderbuffer object</span></span><br><span class="line"><span class="type">unsigned</span> <span class="type">int</span> rbo;</span><br><span class="line"><span class="built_in">glGenRenderbuffers</span>(<span class="number">1</span>, &amp;rbo);</span><br><span class="line"><span class="built_in">glBindRenderbuffer</span>(GL_RENDERBUFFER, rbo);</span><br><span class="line"><span class="built_in">glRenderbufferStorage</span>(GL_RENDERBUFFER, GL_DEPTH24_STENCIL8, <span class="number">800</span>, <span class="number">600</span>);</span><br><span class="line"><span class="built_in">glBindRenderbuffer</span>(GL_RENDERBUFFER, <span class="number">0</span>);</span><br><span class="line"><span class="comment">// attach</span></span><br><span class="line"><span class="built_in">glFramebufferRenderbuffer</span>(GL_FRAMEBUFFER, GL_DEPTH_STENCIL_ATTACHMENT, GLRENDERBUFFER, rbo);</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span>(<span class="built_in">glCheckFramebufferStatus</span>(GL_FRAMEBUFFER) != GL_FRAMEBUFFER_COMPLETE)</span><br><span class="line">    std::cout &lt;&lt; <span class="string">&quot;ERROR::FRAMEBUFFER::Framebuffer is not complete!&quot;</span> &lt;&lt; std::endl;</span><br><span class="line"><span class="built_in">glBindFramebuffer</span>(GL_FRAMEBUFFER, <span class="number">0</span>);</span><br></pre></td></tr></table></figure><p>随后，将场景渲染到一个texture需要以下几步：</p><ol type="1"><li>使用新创建的 framebuffer 正常渲染场景；</li><li>切换回默认framebuffer；</li><li>渲染一个覆盖整个屏幕的quad，使用之前framebuffer的colorattachment用作贴图；</li></ol><p>完成上述步骤后得到如下结果：</p><p><img src="/images/LearnOpenGL/Advanced%20OpenGL/1-1.png" /></p><p>然后，我们就可以以此为基础实现很多后处理效果，下面是一些效果的简单说明。</p><h4 id="inversion">Inversion</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    FragColor = <span class="built_in">vec4</span>(<span class="built_in">vec3</span>(<span class="number">1.0</span> - <span class="built_in">texture</span>(screenTexture, TexCoords)), <span class="number">1.0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="grayscale">Grayscale</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">void</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    FragColor = <span class="built_in">texture</span>(screenTexture, TexCoords);</span><br><span class="line">    <span class="type">float</span> average = (FragColor.r + FragColor.g + FragColor.b) / <span class="number">3.0</span>;</span><br><span class="line">    FragColor = <span class="built_in">vec4</span>(average, average, average, <span class="number">1.0</span>);</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// the human eye tends to be more sensitive to green colors and the least to blue</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">main</span><span class="params">()</span></span>&#123;</span><br><span class="line">    FragColor = <span class="built_in">texture</span>(screenTexture, TexCoords);</span><br><span class="line">    <span class="type">float</span> average = <span class="number">0.2126</span> * FragColor.r + <span class="number">0.7152</span> * FragColor.g + <span class="number">0.0722</span> * FragColor.b;</span><br><span class="line">    FragColor = <span class="built_in">vec4</span>(average, average, average, <span class="number">1.0</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><img src="/images/LearnOpenGL/Advanced%20OpenGL/1-3.png" /></p><h4 id="kernel-effects">Kernel effects</h4><p>kernels 是后处理中一种非常有用的工具，本案例使用的是一个3x3 的sharpen kernel，可以锐化原图像。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">const</span> <span class="type">float</span> offset = <span class="number">1.0</span> / <span class="number">300.0</span>;  </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    vec2 offsets[<span class="number">9</span>] = vec2[](</span><br><span class="line">        <span class="built_in">vec2</span>(-offset,  offset), <span class="comment">// top-left</span></span><br><span class="line">        <span class="built_in">vec2</span>( <span class="number">0.0f</span>,    offset), <span class="comment">// top-center</span></span><br><span class="line">        <span class="built_in">vec2</span>( offset,  offset), <span class="comment">// top-right</span></span><br><span class="line">        <span class="built_in">vec2</span>(-offset,  <span class="number">0.0f</span>),   <span class="comment">// center-left</span></span><br><span class="line">        <span class="built_in">vec2</span>( <span class="number">0.0f</span>,    <span class="number">0.0f</span>),   <span class="comment">// center-center</span></span><br><span class="line">        <span class="built_in">vec2</span>( offset,  <span class="number">0.0f</span>),   <span class="comment">// center-right</span></span><br><span class="line">        <span class="built_in">vec2</span>(-offset, -offset), <span class="comment">// bottom-left</span></span><br><span class="line">        <span class="built_in">vec2</span>( <span class="number">0.0f</span>,   -offset), <span class="comment">// bottom-center</span></span><br><span class="line">        <span class="built_in">vec2</span>( offset, -offset)  <span class="comment">// bottom-right    </span></span><br><span class="line">    );</span><br><span class="line"></span><br><span class="line">    <span class="type">float</span> kernel[<span class="number">9</span>] = <span class="type">float</span>[](</span><br><span class="line">        <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span>,</span><br><span class="line">        <span class="number">-1</span>,  <span class="number">9</span>, <span class="number">-1</span>,</span><br><span class="line">        <span class="number">-1</span>, <span class="number">-1</span>, <span class="number">-1</span></span><br><span class="line">    );</span><br><span class="line">    </span><br><span class="line">    vec3 sampleTex[<span class="number">9</span>];</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">9</span>; i++)</span><br><span class="line">    &#123;</span><br><span class="line">        sampleTex[i] = <span class="built_in">vec3</span>(<span class="built_in">texture</span>(screenTexture, TexCoords.st + offsets[i]));</span><br><span class="line">    &#125;</span><br><span class="line">    vec3 col = <span class="built_in">vec3</span>(<span class="number">0.0</span>);</span><br><span class="line">    <span class="keyword">for</span>(<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="number">9</span>; i++)</span><br><span class="line">        col += sampleTex[i] * kernel[i];</span><br><span class="line">    </span><br><span class="line">    FragColor = <span class="built_in">vec4</span>(col, <span class="number">1.0</span>);</span><br><span class="line">&#125;  </span><br></pre></td></tr></table></figure><p><img src="/images/LearnOpenGL/Advanced%20OpenGL/1-4.png" /></p><h4 id="blur">Blur</h4><p>blur是一种特殊的核函数，用于模糊图像。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> kernel[<span class="number">9</span>] = <span class="type">float</span>[](</span><br><span class="line">    <span class="number">1.0</span> / <span class="number">16</span>, <span class="number">2.0</span> / <span class="number">16</span>, <span class="number">1.0</span> / <span class="number">16</span>,</span><br><span class="line">    <span class="number">2.0</span> / <span class="number">16</span>, <span class="number">4.0</span> / <span class="number">16</span>, <span class="number">2.0</span> / <span class="number">16</span>,</span><br><span class="line">    <span class="number">1.0</span> / <span class="number">16</span>, <span class="number">2.0</span> / <span class="number">16</span>, <span class="number">1.0</span> / <span class="number">16</span>  </span><br><span class="line">);</span><br></pre></td></tr></table></figure><p><img src="/images/LearnOpenGL/Advanced%20OpenGL/1-5.png" /></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shader </tag>
            
            <tag> OpenGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unity使用中的常见问题</title>
      <link href="/2024/10/01/Unity/UnityEngine/Unity%E4%BD%BF%E7%94%A8%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/"/>
      <url>/2024/10/01/Unity/UnityEngine/Unity%E4%BD%BF%E7%94%A8%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> Unity </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
            <tag> Game Development </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnOpenGL - Parallax Mapping</title>
      <link href="/2024/09/30/LearnOpenGL/Parallax%20Mapping/"/>
      <url>/2024/09/30/LearnOpenGL/Parallax%20Mapping/</url>
      
        <content type="html"><![CDATA[<h3 id="技术来源">技术来源</h3><p><strong>Parallax mapping</strong> 与 Normal mapping相似，但是原理不同。Normalmapping通过影响光照等其他因素来给予模型细节和深度感，Parallax mapping则通过另外一种方式模拟更真实的深度感，两者结合可以产生难以置信的逼真效果。</p><p>Parallax mapping 与 <strong>displacementmapping</strong>技术群联系密切。displacementmapping基于存储在贴图中的几何信息偏移顶点，使用一张被称为<strong>heightmap</strong>的贴图存储高度信息。一面砖墙的高度图如下：</p><p><img src="/images/LearnOpenGL/2-1.png" /></p><p>其模拟效果依赖于顶点数目，想要的得出较好的渲染效果需要模型具有大量的顶点。而Parallaxmapping便可以不依赖于顶点数量，通过<strong>偏移采样uv</strong>的trick来模拟深度。</p><h3 id="实现原理">实现原理</h3><p>Parallax mapping 基于<strong>视线方向（viewdirecion)</strong>和<strong>高度图（heightmap）</strong>来改变纹理坐标，使物体的一部分看起来比另一部分更高或更低。为了方便理解，下面是一张砖墙的表面示意图：</p><p><img src="/images/LearnOpenGL/2-2.png" /></p><p>图中红线是高度图中的值，代表了砖墙的几何表面；$ V $是视线方向，viewdirecion;</p><p>图中这种情况，对于渲染点<spanclass="math inline">\(A\)</span>，由于模型几何信息的缺失，如果以高度图的值表示真实墙面几何，那么实际上看到的应该是点<spanclass="math inline">\(B\)</span>。<strong>Parallaxmapping的目标就是，将点<spanclass="math inline">\(A\)</span>的纹理坐标通过某一种方法偏移到点<spanclass="math inline">\(B\)</span>的纹理坐标处。</strong>当我们在渲染点<spanclass="math inline">\(A\)</span>时使用点<spanclass="math inline">\(B\)</span>处的纹理坐标对其他贴图进行采样，就相当于看到了点<spanclass="math inline">\(B\)</span>。</p><p>Parallax mapping 具体做的事情就是，将向量<spanclass="math inline">\(V\)</span>以点<spanclass="math inline">\(A\)</span>处的高度值<spanclass="math inline">\(H(A)\)</span>为长度进行缩放。下示例图展示了缩放后的向量<spanclass="math inline">\(P\)</span>：</p><p><img src="/images/LearnOpenGL/2-3.png" /></p><p>在得到向量<spanclass="math inline">\(P\)</span>后，使用其投影在模型表面上的坐标值作为纹理偏移offset。这里就又用到了切线空间，切线空间的tangent和bitangent和模型纹理坐标方向相同，所以只需要将<spanclass="math inline">\(P\)</span>转换到切线空间即可，其x，y坐标就是纹理坐标的偏移量offset。</p><p>当然，Parallaxmapping并不是完美的，在大多数情况下其模拟效果都不错。但是，当高度发生剧烈变化时，实际交点<spanclass="math inline">\(B\)</span>和计算得出的交点有所差距，如下示例图：</p><p><img src="/images/LearnOpenGL/2-4.png" /></p><h3 id="具体实现">具体实现</h3><h4 id="parallax-mapping">Parallax mapping</h4><p>具体请参考LearnOpenGL：https://learnopengl.com/Advanced-Lighting/Parallax-Mapping，这里仅列出部分内容。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">vec2 <span class="title">ParallaxMapping</span><span class="params">(vec2 texCoords, vec3 viewDir)</span></span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="type">float</span> height =  <span class="built_in">texture</span>(depthMap, texCoords).r;    </span><br><span class="line">    vec2 p = viewDir.xy / viewDir.z * (height * height_scale);</span><br><span class="line">    <span class="keyword">return</span> texCoords - p;    </span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><p>这个函数便是纹理坐标偏移的核心函数，很简单的一个函数。先通过原始纹理坐标texCoords取到height值，然后用它计算向量<spanclass="math inline">\(P\)</span>。这段代码值得一提的一点就是，<strong>viewDir除以了z值</strong>。我们知道viewDir是一个单位化的向量，其z值位于[0,1]。当视线viewDir接近平行于表面时，z更接近于0，除法会返回一个比垂直于表面时更大向量<spanclass="math inline">\(P\)</span>，即获得了更大的纹理坐标偏移offset。</p><p>在具体应用中，除以viewDir.z可能会产生不理想的结果，所以有时会将等式的除法移除。这种做法被称为<strong>ParallaxMapping with OffsetLimiting</strong>，具体要不要使用除法就要看实际情况了。</p><p>Parallaxmapping比较明显的artifact通常出现在边缘，因为此时纹理坐标经过偏移值之后已经超出[0,1]范围，其具体表现因贴图的wrappingmodel设置不同而有所差异。解决这个artifact比较好的方法是将超出范围的fragment舍弃掉：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">texCoords = <span class="built_in">ParallaxMapping</span>(fs_in.TexCoords,  viewDir);</span><br><span class="line"><span class="keyword">if</span>(texCoords.x &gt; <span class="number">1.0</span> || texCoords.y &gt; <span class="number">1.0</span> || texCoords.x &lt; <span class="number">0.0</span> || texCoords.y &lt; <span class="number">0.0</span>)</span><br><span class="line">    discard;</span><br></pre></td></tr></table></figure><p><strong>Parallaxmapping最好的应用情景还是渲染平面</strong>。当然其在特定角度下和高度变化剧烈时可能会出现模拟不正确的情况，但整体而言模拟效果还是比较好的。</p><p><img src="/images/LearnOpenGL/2-5.png" /></p><h4 id="steep-parallax-mapping">Steep Parallax Mapping</h4><p>Steep Parallax Mapping是普通视察贴图技术的改进版，两个的基本原理还是一样的，但是Steep ParallaxMapping使用了多次采样来更准确的确定 向量<spanclass="math inline">\(P\)</span> 和 点<spanclass="math inline">\(B\)</span>，采样的此数越多，模拟的越准确。请看如下图示：</p><p><img src="/images/LearnOpenGL/2-6.png" /></p><p>Steep Parallax Mapping的做法是将整个深度分为很多个<strong>layers</strong>，每个layers之前间隔相同。对于每个layer，都会沿着向量<spanclass="math inline">\(P\)</span>的方向偏移纹理坐标并采样深度图，直到采样的深度小于当前layer的深度。上图中，直到D(3)= 0.37时，其深度小于layer的深度，因此我们可以推断此时向量<spanclass="math inline">\(P\)</span>最接近于模型的实际表面，可在此时偏移纹理坐标。下面看下相关代码：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">vec2 <span class="title">ParallaxMapping</span><span class="params">(vec2 texCoords, vec3 viewDir)</span></span></span><br><span class="line"><span class="function"></span>&#123; </span><br><span class="line">    <span class="comment">// number of depth layers</span></span><br><span class="line">    <span class="comment">// const float numLayers = 10;</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment">// taking less samples when lookning straight at a surface and more samples when looking at an angle</span></span><br><span class="line">    <span class="type">const</span> <span class="type">float</span> minLayers = <span class="number">8.0</span>;</span><br><span class="line"><span class="type">const</span> <span class="type">float</span> maxLayers = <span class="number">32.0</span>;</span><br><span class="line"><span class="type">float</span> numLayers = <span class="built_in">mix</span>(maxLayers, minLayers, <span class="built_in">max</span>(<span class="built_in">dot</span>(<span class="built_in">vec3</span>(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>), viewDir), <span class="number">0.0</span>));</span><br><span class="line">    </span><br><span class="line">    <span class="comment">// calculate the size of each layer</span></span><br><span class="line">    <span class="type">float</span> layerDepth = <span class="number">1.0</span> / numLayers;</span><br><span class="line">    <span class="comment">// depth of current layer</span></span><br><span class="line">    <span class="type">float</span> currentLayerDepth = <span class="number">0.0</span>;</span><br><span class="line">    <span class="comment">// the amount to shift the texture coordinates per layer (from vector P)</span></span><br><span class="line">    vec2 P = viewDir.xy * height_scale; </span><br><span class="line">    vec2 deltaTexCoords = P / numLayers;</span><br><span class="line">  </span><br><span class="line">    <span class="comment">// get initial values</span></span><br><span class="line">vec2  currentTexCoords     = texCoords;</span><br><span class="line"><span class="type">float</span> currentDepthMapValue = <span class="built_in">texture</span>(depthMap, currentTexCoords).r;</span><br><span class="line">  </span><br><span class="line"><span class="keyword">while</span>(currentLayerDepth &lt; currentDepthMapValue)</span><br><span class="line">&#123;</span><br><span class="line">    <span class="comment">// shift texture coordinates along direction of P</span></span><br><span class="line">    currentTexCoords -= deltaTexCoords;</span><br><span class="line">    <span class="comment">// get depthmap value at current texture coordinates</span></span><br><span class="line">    currentDepthMapValue = <span class="built_in">texture</span>(depthMap, currentTexCoords).r;  </span><br><span class="line">    <span class="comment">// get depth of next layer</span></span><br><span class="line">    currentLayerDepth += layerDepth;  </span><br><span class="line">&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> currentTexCoords;</span><br><span class="line">&#125; </span><br></pre></td></tr></table></figure><p>但是这种类步进式的采用通常会<strong>产生分层</strong>的问题，如下图：</p><p><img src="/images/LearnOpenGL/2-7.png" /></p><p>解决分层问题最暴力的方法就是增大采样次数，但这样会产生性能问题。有两种比较流行的方式可以缓解此问题，<strong>ReliefParallax Mapping</strong> 和 Parallax Occlusion Mapping。其中ReliefParallax Mapping可以产生更精确的效果，但会比 Parallax Occlusion Mapping性能消耗高。Parallax Occlusion Mapping 可以更efficient地模拟出和ReliefParallax Mapping近乎相同的结果，所以通常会选择它来用作解决方案。</p><h4 id="parallax-occlusion-mapping">Parallax Occlusion Mapping</h4><p>相比于 Steep Parallax Mapping，Parallax Occlusion Mapping会使用collision发生前后的layer深度做<strong>线性插值interpolate</strong>。线性插值的权重取自<strong>layer深度和对应heightmap深度的差值</strong>，请看下图：</p><p><img src="/images/LearnOpenGL/2-8.png" /></p><p>两种技术非常相似，Parallax Occlusion Mapping 更像是 Steep ParallaxMapping后面增加了一个线性插值的步骤。所以代码只需在之前的基础上增加如下几行：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[...] <span class="comment">// steep parallax mapping code here</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">// get texture coordinates before collision (reverse operations)</span></span><br><span class="line">vec2 prevTexCoords = currentTexCoords + deltaTexCoords;</span><br><span class="line"></span><br><span class="line"><span class="comment">// get depth after and before collision for linear interpolation</span></span><br><span class="line"><span class="type">float</span> afterDepth  = currentDepthMapValue - currentLayerDepth;</span><br><span class="line"><span class="type">float</span> beforeDepth = <span class="built_in">texture</span>(depthMap, prevTexCoords).r - currentLayerDepth + layerDepth;</span><br><span class="line"> </span><br><span class="line"><span class="comment">// interpolation of texture coordinates</span></span><br><span class="line"><span class="type">float</span> weight = afterDepth / (afterDepth - beforeDepth);</span><br><span class="line">vec2 finalTexCoords = prevTexCoords * weight + currentTexCoords * (<span class="number">1.0</span> - weight);</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> finalTexCoords; </span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shader </tag>
            
            <tag> OpenGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《超凡动物奇观》随笔</title>
      <link href="/2024/09/30/%E9%9A%8F%E7%AC%94/%E3%80%8A%E8%B6%85%E5%87%A1%E5%8A%A8%E7%89%A9%E5%A5%87%E8%A7%82%E3%80%8B%E9%9A%8F%E7%AC%94%E4%B8%80/"/>
      <url>/2024/09/30/%E9%9A%8F%E7%AC%94/%E3%80%8A%E8%B6%85%E5%87%A1%E5%8A%A8%E7%89%A9%E5%A5%87%E8%A7%82%E3%80%8B%E9%9A%8F%E7%AC%94%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<p>一集四十分钟，当下饭视频有点太长了，但还是被吸引着看完了。观感很不错，特效加的位置和时机都堪称完美。第一集讲的主要是物种间关系，找了几个比较可爱的物种拍的，导演是懂人们爱看什么的。第一集标题是“奇特的关系”，挺正常的，但有弹幕在吐槽标题，顿时觉得不简单，于是看看评论区，好家伙。“长颈鹿：救命！蚂蚁在我的舌头上集体喷射”“连虫虫都知道的好孕小技巧！难道你还不知道吗？”我只能说，B站真有你的。</p><h3 id="北美飞鼠">北美飞鼠</h3><p>有一个从腕部一直延伸到脚踝的飞膜，印象中好多游戏中的怪物设计借鉴了这种结构，也确实是一个比较容易融入二次创作的东西。北美飞鼠会用飞翼在森林间穿梭，在夜晚为寻找食物往往会移动到远离洞穴的位置，就算飞鼠有较好的夜间视力，想找到回家的路也有些困难。这种情况下，飞鼠会发出捕食者难以察觉的超声波紧急信号，同类便会会以信号以示巢穴方向。比较新颖的一点是，北美飞鼠被发现会发光。在紫外光下，北美飞鼠腹部会发出粉红色的微光（更可爱了）。“Absorbingmoonlight in one colour, and emitting it in another."研究人员认为，其黑色腹部发出的光，就如灯塔一样，使其在夜晚更容易看到同伴。</p><p><img src="/images/Animals/Northern%20flying%20squirrels.png" /></p><h3 id="穴小鸮">穴小鸮</h3><p>穴小鸮，分布于美洲的鸮形目鸟类，是唯一群居的猫头鹰，就算你们没挺过这个名字也应该见过它们的表情包。不得不说鸮形目的脖子真的灵活，上下左右都能近乎180度旋转，可用通过旋转凹陷的脸来将声音导向耳朵，拥有非常灵敏的听觉。会吃大型节肢动物，和小型哺乳动物，然后从口中吐出颗粒状呕吐物。多说无益，快去看穴小鸮的可爱纪录片。</p><p><img src="/images/Animals/Burrowing%20owls-1.png" /></p><p><img src="/images/Animals/Burrowing%20owls-2.png" /></p><p><img src="/images/Animals/Burrowing%20owls-3.png" /></p><h3 id="萤火虫">萤火虫</h3><p>记录片中倒是没具体到哪个种，主要还是想记一下他们的集体发光行为。世界上有2000多种萤火虫，只有大约12种会同步发光。</p><p>萤火虫会将氧气注入腹部，氧气会和荧光素、荧光素酶发生反应，发出生物荧光。在墨西哥森林的夜晚，有时会见到成千上万的雄性萤火虫发光慢慢同步，数不清的荧光点在黑暗中如呼吸般亮起与熄灭，形成一道道荧光涟漪。</p><p><img src="/images/Animals/firefly.png" /></p><p>研究员认为，可能是雌性萤火虫的择偶倾向诱使部分种的萤火虫出现集体发光行为。雌性萤火虫青睐炫目的光亮，雄性萤火虫聚集产生的光亮更容易吸引雌性萤火虫的到来。</p><h3 id="哨金合欢树-举腹蚁">哨金合欢树 &amp; 举腹蚁</h3><p>纪录片中讲述的哨金合欢树分布于非洲大草原，我在中文互联网上没有查到这个种的详细信息，由于无关紧要，暂时就不深入了解这个种的信息了。</p><p>草原上的大象每天要吃掉非常多的植物来满足生存需求，而哨金合欢树会发出低频声波屏障，听起来像一群愤怒的蜜蜂，让大象望而却步，这也是此种为什么以“哨”命名。其声音来源于膨大树枝的空洞，当风经过时，便会发出声音。这种空洞并非哨金合欢于生具有的，而是由举腹蚁通过触角测量以特定角度啃出来的。</p><p><img src="/images/Animals/whistling%20acacia-2.png" /></p><p><img src="/images/Animals/whistling%20acacia-3.png" /></p><p>哨合金欢树与举腹蚁是自然界中常见的互利共生关系。举腹蚁以上述空洞为入口生活在哨合金欢树上，帮助其驱赶长颈鹿（也算是保卫家园）；哨合金欢树会分泌甜美花蜜吸引举腹蚁；</p><p>举腹蚁会利用震动确定入侵者的方位:</p><blockquote><p>“If the front right detects motion a fraction of a second before backleft, it means enemy ahead, to the right. ”</p></blockquote><p>在到达入侵者体表时，会通过刺咬和喷射有毒的酸液来使其产生刺痛感，达到驱使入侵者的目的。举腹蚁在喷射时会抬起腹部，这正是其名称的由来。</p><p><img src="/images/Animals/Cocktail%20ants.png" /></p><h3 id="疣猪-狐獴">疣猪 &amp; 狐獴</h3><p>疣猪喜欢用泥巴浴来清洁体表，但是无法清除体表的寄生虫如吸血蜱虫和虱子。</p><p>狐獴善于交流，掌握着最复杂的动物语言之一。</p><p>在乌干达一个村庄的附近，生活着这两种动物。疣猪警惕性高，通常处于战备状态，但在这里，它们需要学会表现的友善一些。在这里，疣猪会通过肢体语言与狐獴进行交流，其会在地上翻滚邀请狐獴进食，清楚体表寄生虫。狐獴有时会举群前往村庄寻找食物，其拥有灵敏的嗅觉，可以闻到细微的血腥味，以此抓出富含蛋白质的寄生虫</p><p><img src="/images/Animals/weighing%20&amp;%20mongoose.png" /></p><p>这种奇特关系只存在于地球上的两个地方，两个人类的居住地，那里的捕食者较少，所以猎物可以放松一些陪伴新朋友。</p><h3 id="木维网-wood-wide-web">“木维网” “Wood Wide Web”</h3><p>在地底的树根处，真菌形成了发丝状网络从树根中提取糖分，也会从土壤中吸取矿物质为树木提供养分。当然还有更奇妙的事情，当树木通过真菌网络连接起来时，便可以进行信息交流，相互发出干旱和疾病警告。</p><p><img src="/images/Animals/wood%20wide%20web.png" /></p>]]></content>
      
      
      <categories>
          
          <category> ink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Animals </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LearnOpenGL - Normal Mapping</title>
      <link href="/2024/09/29/LearnOpenGL/Normal%20Mapping/"/>
      <url>/2024/09/29/LearnOpenGL/Normal%20Mapping/</url>
      
        <content type="html"><![CDATA[<h3 id="技术来源">技术来源</h3><p>为了展示更多的光照细节，需要更精细的网格模型，但实时渲染中网格面数越大对渲染速度的影响越大。为了解决此矛盾，使渲染质量在线的同时保持较快渲染速度，可以将模型细节信息记录在贴图中，在渲染时使用像素法线（Per-FragmentNormal）代替面法线(Per-Surface Normal)，此技术被称为<strong>normalMapping</strong>或者<strong>bump mapping</strong>。</p><p><img src="/images/LearnOpenGL/1-1.png" /></p><p><img src="/images/LearnOpenGL/1-2.png" /></p><h3 id="实现原理">实现原理</h3><p>要实现像素法线，可以像漫发射贴图一样将信息存储在一张贴图中，在渲染时逐像素读取。由于法线向量是一个几何实体而贴图存储颜色信息，因此法线向量x,y,z需要经过变换存储于r,g,b通道中。</p><p>法线向量范围为[-1,1]，需要映射到[0,1]。存储后的法线贴图通常如下颜色：</p><p><img src="/images/LearnOpenGL/1-3.png" /></p><p>但是，单单只是将法线向量映射存储可不行。我们来思考一下，法线向量应该以何坐标系为基准进行存储呢，世界坐标系？如果以世界坐标存储法线，那么法线在其中的值便是固定的了，当模型发生旋转时，其法线信息便会错误，发生如下情况：</p><p><img src="/images/LearnOpenGL/1-4.png" /></p><p>可能有人会说让法线和模型做同样的旋转就好了？针对上示图例模型确实是这样。如果是一个更复杂的模型，然后让其在某一方向拉伸变形呢？恐怕这是世界坐标法线或者物体坐标法线无法解决的问题。为了解决此问题，让法线贴图使用起来更方便，便存储法线在<strong>切线空间</strong>下的坐标于贴图中。</p><h4 id="切线空间">切线空间</h4><p><strong>切线空间位于模型表面，是模型三角形面的局部空间，该局部空间使用该三角面的法线当作+z轴。</strong>烘焙时便是使用该局部空间坐标记录更精细模型的法线，使用时再将其通过<strong>TBN矩阵</strong>转换成世界坐标。TBN是Tangent，Bitangent和Normal的缩写，可以当作切线空间的坐标轴，通过这三个向量就可以构建TBN矩阵。</p><p><img src="/images/LearnOpenGL/1-5.png" /></p><p>三个向量中，Normal是已知的，只需要计算出剩余两个即可。因为Tangent和Bitangent都位于模型表面，所以可以利用贴图坐标，即uv进行计算。请看下图：</p><p><img src="/images/LearnOpenGL/1-6.png" /></p><p>根据上图，我们可以列出以下方程： <span class="math display">\[\begin{align}E_1 = \Delta U_1 T + \Delta V_1 B \\E_2 = \Delta U_2 T + \Delta V_2 B\end{align}\]</span> 或者写为下式： <span class="math display">\[\begin{align}(E_{1x}, E_{1y}, E_{1z}) = \Delta U_1 (T_x, T_y, T_z) + \Delta V_1(B_x,B_y, B_z) \\(E_{2x}, E_{2y}, E_{2z}) = \Delta U_2 (T_x, T_y, T_z) + \Delta V_2(B_x,B_y, B_z)\end{align}\]</span> 其中 <span class="math inline">\(E\)</span>可以通过三角形顶点坐标计算出，<span class="math inline">\(\DeltaU\)</span>和<span class="math inline">\(\DeltaV\)</span>可以通过贴图坐标计算出。于是，剩下的Tangent和Bitangent就是一个二元方程。把上式按照矩阵形式来写，相当于矩阵乘法：<span class="math display">\[\begin{bmatrix}E_{1x} &amp; E_{1y} &amp; E_{1z} \\E_{2x} &amp; E_{2y} &amp; E_{2z} \\\end{bmatrix} = \begin{bmatrix}\Delta U_1 &amp; \Delta V_1 \\\Delta U_2 &amp; \Delta V_2 \\\end{bmatrix} \begin{bmatrix}T_x &amp; T_y &amp; T_z \\B_x &amp; B_y &amp; B_z \\\end{bmatrix}\]</span> 将上述矩阵同时左乘<span class="math inline">\(\Delta U \DeltaV\)</span>的逆矩阵，随后求解逆矩阵，然后等式左右互换位置后得到如下等式：<span class="math display">\[\begin{bmatrix}T_x &amp; T_y &amp; T_z \\B_x &amp; B_y &amp; B_z \\\end{bmatrix} = \frac{1}{\Delta U_1 \Delta V_2 - \Delta U_2 \Delta V_1}\begin{bmatrix}\Delta V_2 &amp; -\Delta V_1 \\-\Delta U_2 &amp; \Delta U_1 \\\end{bmatrix}\begin{bmatrix}E_{1x} &amp; E_{1y} &amp; E_{1z} \\E_{2x} &amp; E_{2y} &amp; E_{2z} \\\end{bmatrix}\]</span>于是，上述等式告诉了我们如何通过三角形的<strong>顶点坐标</strong>和<strong>UV坐标</strong>求解tangent和bitangent。</p><p>通常情况下，三角形顶点坐标和UV坐标都是已知的。</p><h4 id="示例代码">示例代码</h4><p>简单画一个plane</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// positions</span></span><br><span class="line"><span class="function">glm::vec3 <span class="title">pos1</span><span class="params">(<span class="number">-1.0</span>,  <span class="number">1.0</span>, <span class="number">0.0</span>)</span></span>;</span><br><span class="line"><span class="function">glm::vec3 <span class="title">pos2</span><span class="params">(<span class="number">-1.0</span>, <span class="number">-1.0</span>, <span class="number">0.0</span>)</span></span>;</span><br><span class="line"><span class="function">glm::vec3 <span class="title">pos3</span><span class="params">( <span class="number">1.0</span>, <span class="number">-1.0</span>, <span class="number">0.0</span>)</span></span>;</span><br><span class="line"><span class="function">glm::vec3 <span class="title">pos4</span><span class="params">( <span class="number">1.0</span>,  <span class="number">1.0</span>, <span class="number">0.0</span>)</span></span>;</span><br><span class="line"><span class="comment">// texture coordinates</span></span><br><span class="line"><span class="function">glm::vec2 <span class="title">uv1</span><span class="params">(<span class="number">0.0</span>, <span class="number">1.0</span>)</span></span>;</span><br><span class="line"><span class="function">glm::vec2 <span class="title">uv2</span><span class="params">(<span class="number">0.0</span>, <span class="number">0.0</span>)</span></span>;</span><br><span class="line"><span class="function">glm::vec2 <span class="title">uv3</span><span class="params">(<span class="number">1.0</span>, <span class="number">0.0</span>)</span></span>;</span><br><span class="line"><span class="function">glm::vec2 <span class="title">uv4</span><span class="params">(<span class="number">1.0</span>, <span class="number">1.0</span>)</span></span>;</span><br><span class="line"><span class="comment">// normal vector</span></span><br><span class="line"><span class="function">glm::vec3 <span class="title">nm</span><span class="params">(<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">1.0</span>)</span></span>;  </span><br></pre></td></tr></table></figure><p>首先计算edges和<span class="math inline">\(\Delta UV\)</span></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">glm::vec3 edge1 = pos2 - pos1;</span><br><span class="line">glm::vec3 edge2 = pos3 - pos1;</span><br><span class="line">glm::vec2 deltaUV1 = uv2 - uv1;</span><br><span class="line">glm::vec2 deltaUV2 = uv3 - uv1;  </span><br></pre></td></tr></table></figure><p>有个必要数据就可以用上节的公式计算tangent和bitangent</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="type">float</span> f = <span class="number">1.0f</span> / (deltaUV1.x * deltaUV2.y - deltaUV2.x * deltaUV1.y);</span><br><span class="line"></span><br><span class="line">tangent1.x = f * (deltaUV2.y * edge1.x - deltaUV1.y * edge2.x);</span><br><span class="line">tangent1.y = f * (deltaUV2.y * edge1.y - deltaUV1.y * edge2.y);</span><br><span class="line">tangent1.z = f * (deltaUV2.y * edge1.z - deltaUV1.y * edge2.z);</span><br><span class="line"></span><br><span class="line">bitangent1.x = f * (-deltaUV2.x * edge1.x + deltaUV1.x * edge2.x);</span><br><span class="line">bitangent1.y = f * (-deltaUV2.x * edge1.y + deltaUV1.x * edge2.y);</span><br><span class="line">bitangent1.z = f * (-deltaUV2.x * edge1.z + deltaUV1.x * edge2.z);</span><br><span class="line">  </span><br><span class="line">[...] <span class="comment">// similar procedure for calculating tangent/bitangent for plane&#x27;s second triangl</span></span><br></pre></td></tr></table></figure><p>将得到的TBN都可视化之后如下图：</p><p><img src="/images/LearnOpenGL/1-7.png" /></p><h3 id="资料链接">资料链接</h3><p><ahref="https://learnopengl.com/Advanced-Lighting/Normal-Mapping">NormalMap</a></p><p><a href="https://www.cnblogs.com/lookof/p/3509970.html">NormalMap中的值， Tangent Space， 求算 Tangent 与 Binormal 与 TBNMatrix</a></p><p><a href="https://www.bilibili.com/video/BV1YV411Y7Qx">Shader for游戏开发 [3]——法线贴图，切线空间Normal Maps, Tangent Space &amp;IBL</a></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Shader </tag>
            
            <tag> OpenGL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SubstanceDesigner学习笔记</title>
      <link href="/2024/09/27/Game%20Development/SubstanceDesigner%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/09/27/Game%20Development/SubstanceDesigner%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<p>这篇文章还是偏向于个人记录，系统的SubstanceDesigner教程，可以观看JavierPerez的教程B站上可以搜到，随便贴一个https://www.bilibili.com/video/BV1kV4y1C7q3?p=1&amp;vd_source=a6d4de83b46a19b70d53fc1e9adb6574。</p><h3 id="前言">前言</h3><p>想要成为TA避免不了学习各种工具软件，SubstanceDesigner可以通过节点生成各种材质，顺便用来生成需要的纹理。就个人而言，这种工具软件除非专精，不然只需要入门，可以跟着资料搞出自己需要的素材即可。</p><p>这篇文章就记录一下SD的基本操作，方便后续再使用到时快速回想起来。</p><p>在有其他DCC软件的使用经验下，SD并不难上手，但是想要用的得心应手还需多加练习。要用好SD需要的是分析和构建能力，其提供了丰富的程序化生成素材，可以满足绝大部分的需求。</p><p>官方文档：https://helpx.adobe.com/substance-3d-designer/glossary.html</p><p>graph node refrence : Substance graphs -- Nodes reference forSubstance graphs</p><h3 id="基础操作">基础操作</h3><p>新建工作文件 File -- New -- Substance Graph 或者 Ctrl+N</p><h4 id="graph区">Graph区</h4><p>建立组：框选相应节点 -- 上方工具栏选择Frame</p><p>space 新建节点</p><p>mouse hit 显示节点属性</p><p>mouse double hit 生成节点2D视图</p><p>调整节点尺寸 Properties窗内调整Base Parameters中的Output Size</p><p>对于material 节点查看效果：右键单击 -- View in 3D View</p><p>对常用节点做个整理。</p><h5 id="blend">Blend</h5><p>Blend节点有几种混合模式，有的还比较复杂，贴个表格，A前景色，B背景色，alpha针对前景的遮罩</p><figure><imgsrc="https://picx.zhimg.com/80/v2-653562808e7c3f6a081ded6ffc80651b_720w.webp"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><h4 id="warp">Warp</h4><p><strong>Directional Warp</strong></p><p>具有方向的扭曲，根据Intensity map使图像向设定的方向WarpAngle扭曲。</p><p><strong>Warp</strong></p><p>根据输入的Gradient map中的斜率使图形扭曲</p><p><strong>Vector Warp</strong></p><p>颜色map驱动的扭曲</p><p><strong>Multi_directional_warp_grayscale</strong></p><p>多次应用Directional Warp从而达到多方向扭曲的效果。</p><h3 id="案例">案例</h3><p>使用SubstanceDesigner做一个冰面的效果，尝试可以快速上手SD。此案例是参考B站案例做的，如果想制作出这种效果直接跳链接：https://www.bilibili.com/video/BV1mk4y157az。</p><p><strong>素材搜集及分析</strong></p><p><img src="/images/Creators/ice_material/1.png" /></p><p>可以去网络上搜集一些冰面的图片，也可以去一些网站参考其他创作者的作品，我们需要做的就是提炼出冰面的特点，明确要做的东西。根据上述图片我们对冰面做一下分析：</p><p>大多为白色与蓝色，白蓝着色会使人下意识认为这是冰；根据需要也可更改为其他颜色；</p><p>经常性有裂纹出现，裂纹可以给冰面更多的立体感，如延伸向水中的裂缝、由于挤压翘起的裂缝等；裂纹周围颜色较浅，而内部颜色偏深；</p><p>除去大的裂纹之外，冰面具有较多更多细节，如冰中气泡或其他原因产生的白色颗粒状物体，更加微小的裂纹等；</p><p><strong>材质制作</strong></p><p>在分析完成后，便可以正式开始制作冰面。</p><p>首先，制作冰面最明显的特征——裂纹。SD可以直接使用绝大部分噪声，其中细胞噪声最适合用来制作裂纹。</p><p><img src="\images\Creators\ice_material\2.png" /></p><p>再使用平铺为冰面增加更多的凹凸细节，以及使用同样的方式为高位的冰面，增加更多的裂纹。</p><figure><img src="\images\Creators\ice_material\5.png" alt="5" /><figcaption aria-hidden="true">5</figcaption></figure><p>现在基本的形已经塑造完成，接下来进行着色，为冰面赋予灵魂，这里使用的颜色通常比显示冰面要更蓝以达到更好的视觉效果。</p><figure><img src="\images\Creators\ice_material\7.png" alt="7" /><figcaption aria-hidden="true">7</figcaption></figure><p>看起来好像蓝过头了，没关系，根据法线图在平整的位置加一层白霜就好了。最后在用噪音图点缀一些白色的颗粒，冰面就完成了。</p><figure><img src="\images\Creators\ice_material\8.png" alt="8" /><figcaption aria-hidden="true">8</figcaption></figure><p><strong>小结</strong></p><p>上面总结的点基本都在案例里面实现了，除了向水中延伸的裂缝，整个流程顺下来并不复杂，当然有一部分原因是有教程跟着走。这种冰面在游戏中并不常见，就不细研究了，游戏中出现的冰面大多会用视差实现刚才说的那个特点，不需要很多的冰面细节而且视觉效果也不错，打算之后在引擎里面做一做。</p>]]></content>
      
      
      <categories>
          
          <category> DDC </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Substance Designer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSH技术记录</title>
      <link href="/2024/09/27/Computer%20Science/SSH%E6%8A%80%E6%9C%AF%E8%AE%B0%E5%BD%95/"/>
      <url>/2024/09/27/Computer%20Science/SSH%E6%8A%80%E6%9C%AF%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<p>一个线上文档：https://manpages.debian.org/bookworm/openssh-client/</p><h4 id="复数ssh链接">复数ssh链接</h4><p><strong>应提前注意：如此设置之后，电脑上已存在的github仓库可能会无法使用，需要重新clone。</strong></p><p>以同一台主机使用不同账户链接github服务器为例，操作方法如下。</p><p>1.分别生成sshkey，并导入github。记得标记key，以免后续混淆。</p><p>2.将sshkey加入ssh-agent中，需要时会自动验证sshkey，记得启动ssh-agent服务。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ssh-add .ssh/id_ras_xxxx</span><br></pre></td></tr></table></figure><p>3.配置ssh的config文件，win下一般为User/.ssh/config，内容如下</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Host hostA</span><br><span class="line">    HostName github.com</span><br><span class="line">    User git</span><br><span class="line">    PreferredAuthentications publickey</span><br><span class="line">    IdentityFile ~/.ssh/id_rsa_xxxx</span><br><span class="line"></span><br><span class="line">Host hostB</span><br><span class="line">    HostName github.com</span><br><span class="line">    User git</span><br><span class="line">    PreferredAuthentications publickey</span><br><span class="line">    IdentityFile ~/.ssh/id_rsa_xxxx</span><br></pre></td></tr></table></figure><p>其中IdentityFile设置为前几步生成的sshkey文件路径，随后使用ssh -ThostA和ssh -T hostB验证是否成功。</p><p><img src="/images/13.png" /></p><p>如此设置之后，使用github还需要注意几点。</p><p>首先github仓库提供的SSH链接不可以直接用来clone，会链接超时。应该做如下改变：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">原本：git@github.com:userName/projectName.git</span><br><span class="line">变为：hostA:userName/projectName.git</span><br></pre></td></tr></table></figure><p>这是由于SSH验证时，会取config里面找设置，如果没有对应Host设置，便会自动尝试默认key名称，直到找到key文件，否则链接超时。上面config里面没有Hostgithub.com，.ssh中也没有默认名称的key文件id_rsa等，所以默认无法通过验证，可以使用ssh -Tv git@github.com 命令来查看验证的具体过程.</p>]]></content>
      
      
      <categories>
          
          <category> Computer Science </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SSH </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title></title>
      <link href="/2024/09/26/Game%20Development/Real-time%20Hair%20Rendering/"/>
      <url>/2024/09/26/Game%20Development/Real-time%20Hair%20Rendering/</url>
      
        <content type="html"><![CDATA[<h3 id="资料">资料</h3><p>毛发渲染总结 https://zhuanlan.zhihu.com/p/378015611</p><p>Shell Texturing https://www.youtube.com/watch?v=9dr-tRQzij4</p><p>Real-time Hair For Games workflowhttps://www.youtube.com/watch?v=PoVmPVAO2m0</p>]]></content>
      
      
      
    </entry>
    
    
    
    <entry>
      <title>PBR简述（二）工作流</title>
      <link href="/2024/07/31/Game%20Development/PBR%E7%AE%80%E8%BF%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E5%B7%A5%E4%BD%9C%E6%B5%81/"/>
      <url>/2024/07/31/Game%20Development/PBR%E7%AE%80%E8%BF%B0%EF%BC%88%E4%BA%8C%EF%BC%89%E5%B7%A5%E4%BD%9C%E6%B5%81/</url>
      
        <content type="html"><![CDATA[<p>本文中所引用的图示均标识出处，参考文章将在文末给出。</p><p>上篇文章对PBR相关的理论进行了简要阐述，本文主要对生产中应用的<strong>金属/粗糙度工作流</strong>和<strong>高光/光泽度工作流</strong>进行简要阐述。由于工作流涉及到贴图相关内容，有必要在开始之前对以下概念做简单介绍。</p><h2 id="gamma-correction-伽马矫正"><strong>Gamma Correction伽马矫正</strong></h2><p><strong>PBR的计算需要在线性空间下进行，以确保渲染结果的物理准确性。</strong></p><p>有的游戏引擎会提供“Linear Color Space”与“Gamma ColorSpace”供使用者选择，以Unity为例，其默认使用Gamma ColorSpace。那么什么是Gamma呢？为什么Gamma总是和Linear一起出现？这三篇资料/视频可能会解答你的疑惑：<ahref="https://www.zhihu.com/question/27467127/answer/37555901">《Gamma是什么》</a>《<ahref="https://www.bilibili.com/video/BV1iQ4y1g7oX">视频图像中的Gamma校正有什么作用</a>》<ahref="https://www.bilibili.com/video/BV15t411Y7cf">《Gamma校正与线性工作流入门讲解》</a></p><p>下面做一些简单介绍：</p><p>在图像领域中，Gamma通常指<strong>伽马矫正（GammaCorrection）</strong>。用wiki上的说法，伽马矫正是用于编码/解码视频或图像中的亮度值/三色值的<strong>非线性运算</strong>，其简化的表达式如下：<span class="math display">\[V_{out}=AV_{in}^ \gamma\]</span> 通常情况下A=1， 输入与输出范围都为[0,1]。 <spanclass="math inline">\(\gamma &lt; 1\)</span>时为<strong>编码伽马(encoding gamma)</strong>，<spanclass="math inline">\(\gamma &gt; 1\)</span>时为<strong>解码伽马(decoding gamma)</strong>。</p><p>伽马矫正源于早期CRT设备的图像显示，由于早期显示器多为CRT设备，其电压值与显示亮度是非线性的（电压增大两倍时显示亮度增加小于两倍）。</p><figure><img src="/images/PBR/2-1.png"alt="CRT设备输入电压与亮度函数曲线 引自《视频图像中的Gamma校正有什么作用》" /><figcaption aria-hidden="true">CRT设备输入电压与亮度函数曲线引自《视频图像中的Gamma校正有什么作用》</figcaption></figure><p>为了让CRT设备正确显示图像，对输入的图像值做预处理使其亮度增大以抵消CRT设备亮度的弱化效果。当时人们通过目测实验取伽马值<span class="math inline">\(\gamma\)</span>为2.5，于是输入CRT设备图像值需要进行 <spanclass="math inline">\(y=x^{\frac{1}{2.5}}\)</span>运算。这样CRT设备显示时相当于对图象值进行了一次 <spanclass="math inline">\(y=x^{2.5}\)</span> 运算，整个系统的伽马值 <spanclass="math inline">\(\gamma\)</span>为1，可以尽量还原其他设备记录的原始图像亮度。这段是对CRT设备使用伽马矫正的简要描述，实际情况更加复杂一些，可见上述视频《视频图像中的Gamma校正有什么作用》。</p><p>后面随着技术发展，显示器基本可以做到线性显示图像亮度，为何伽马矫正依然存在？兼容CRT设备是一方面，但这不是伽马矫正在现代图形系统的主要目的。现代图形系统中伽马矫正存在的主要意义是，<strong>合理利用存储空间以提高图像质量</strong>。原因有二，<strong>其一，人眼对亮度的感知是非线性的；其二，图像存储空间/传输带宽有限。</strong></p><figure><img src="/images/PBR/2-2.png"alt="感知亮度与物理亮度 引自《视频图像中的Gamma校正有什么作用》" /><figcaption aria-hidden="true">感知亮度与物理亮度引自《视频图像中的Gamma校正有什么作用》</figcaption></figure><p>相关实验表明，人眼对黑暗的变化更加敏感。如果使用[0,1]范围表示显示器的亮度，人眼感觉的中灰度的实际亮度大约是0.218。因此，对于图像存储，如果使用8bit均匀存储[0,1]亮度（下图左），图像中暗色部分会出现明显色阶断层。而如果对图像进行一次编码伽马存储，将人眼中灰度的亮度0.218映射到0.5（下图右），使用更多空间存储图像暗部，在显示图像时解码伽马，色阶断层现象会改善很多。（原视频会更明显，这里有些失真）</p><figure><img src="/images/PBR/2-3.png"alt="引自《Gamma校正与线性工作流入门讲解》" /><figcaptionaria-hidden="true">引自《Gamma校正与线性工作流入门讲解》</figcaption></figure><p>因为伽马矫正的存在，微软联合爱普生、惠普提供了sRGB颜色空间标准，推荐显示器的显示伽马值为2.2，并配合0.45的编码伽马就可以保证最后伽马曲线之间可以相互抵消（因为2.2×0.45≈1）。所以，一些制图软件输出的材质贴图可能就使用了sRGB标准，也就是说这些材质贴图已经进行了<strong>编码伽马</strong>，其数值是非线性的，如果直接使用这些材质贴图的图像值进行PBR的物理运算会得到错误的结果。这就又回到了开始提到的线性颜色空间与伽马颜色空间，此处还以Unity为例。</p><figure><img src="/images/PBR/2-4.png" alt="Unity中Color Space的设置" /><figcaption aria-hidden="true">Unity中Color Space的设置</figcaption></figure><figure><img src="/images/PBR/2-5.png" alt="Unity中导入sRGB图像的设置" /><figcaption aria-hidden="true">Unity中导入sRGB图像的设置</figcaption></figure><p>当处于<strong>伽马颜色空间</strong>时，Unity不会对着色器的输入和输出做任何处理，这意味着输出的亮度经过显示器的显示伽马（解码伽马）后可能得到非预期的亮度。当处于<strong>线性颜色空间</strong>下时，Unity在对设置为sRGB模式的材质贴图采样时会自动将其进行解码伽马转化到线性空间下，而且着色器在写入颜色缓冲前自动进行编码伽马。</p><p>总的来说就是，因为伽马矫正的存在，我们需要注意材质贴图是否是sRGB，保证PBR运算是在线性空间下进行的，线性空间和伽马空间本质上来说都是帮助我们正确输出图像亮度的辅助工具。</p><h2 id="high-dynamic-range-高动态范围"><strong>High Dynamic Range高动态范围</strong></h2><p>高动态范围，简称HDR。通常显示设备使用的颜色缓冲每通道精度为8位，意味着只能使用256种不同亮度表示真实世界的亮度。而真实世界的亮度范围可以说是非常之大，用256个亮度表示肯定会丢失精度，而HDR可以使用远高于8位的精度记录亮度信息，更加准确的反应真实的光照环境。</p><p>HDR经常和PBR配合使用，碍于本文重点在于PBR的工作流，这里就不在更多展开HDR相关内容，只是提一嘴，之后再写相关的文章仔细讲讲。</p><p>在对上述概念有所了解之后，我们正式进入工作流部分。</p><p>在PBR工作流中，输入着色器的或者说artist输出的材质贴图的每个通道都有明确定义。工作流中材质贴图可能是sRGB标准下的，也可能是线性的，这就是为什么前面要了解伽马矫正。下文首先阐述每个工作流的特点，然后再讲两个工作流的共同点，以及两者相对的优缺点。由于部分贴图的有多个名称，为避免混淆会使用某一特定名称。</p><h2 id="metalroughness-workflow-金属粗糙度工作流">Metal/RoughnessWorkflow 金属/粗糙度工作流</h2><p>以下简称为金属流。在金属工作流中有三张特定贴图：<strong>基础色贴图(basecolor)、金属度贴图(metallic)和粗糙度贴图(roughness)。</strong></p><figure><img src="/images/PBR/2-6.png"alt="金属工作流特有贴图 引自《The PBR Guide - Part 2》" /><figcaption aria-hidden="true">金属工作流特有贴图 引自《The PBR Guide -Part 2》</figcaption></figure><p><strong>Base Color (RGB - sRGB)</strong></p><figure><img src="/images/PBR/2-7.png"alt="基础色贴图 引自《The PBR Guide - Part 2》" /><figcaption aria-hidden="true">基础色贴图 引自《The PBR Guide - Part2》</figcaption></figure><p>在金属流中，基础色贴图存储了<strong>电介质的漫反射颜色</strong>和<strong>金属的反射率。</strong>正如上篇文章所述，电介质表面颜色大部分来源于漫反射，金属表面颜色来源于高光反射。于是，金属流就将决定两者表面颜色的数据都整合在基础色贴图中：存储电介质的漫反射颜色，在着色中使用0.04作为其基础反射率F0 的默认值；存储金属的反射率，因为金属没有漫反射。</p><p><strong>Metallic (Grayscale - Linear)</strong></p><figure><img src="/images/PBR/2-8.png"alt="金属度贴图 引自《The PBR Guide - Part 2》" /><figcaption aria-hidden="true">金属度贴图 引自《The PBR Guide - Part2》</figcaption></figure><p>金属度贴图用于确定物体表面是否为金属，其作用类似于mask，决定了着色器如何处理基础色贴图中的数据。金属度贴图是线性灰度图，0.0（black- 0 sRGB）表示表面为电介质，1.0（white -255sRGB）表示表面为纯金属。为区分表面是否为金属，看起来金属度贴图应该是二元的黑白图。但实际上，大部分渲染器都允许金属图包含0-1之间的灰度值用来模拟生锈的金属或者镀层，此时可能需要手动调整基础色贴图中的颜色值以获得期望的渲染结果。物体的基础反射率F0计算公式为：</p><p>F0=lerp(F0,baseColor,metallic)</p><p><strong>Roughness （Grayscale - Linear）</strong></p><figure><img src="/images/PBR/2-9.png"alt="粗糙度贴图 引自《The PBR Guide - Part 2》" /><figcaption aria-hidden="true">粗糙度贴图 引自《The PBR Guide - Part2》</figcaption></figure><p>正如上篇文章所述，粗糙度贴图描述了物体表面的不规则度（SurfaceIrregularities）。粗糙度贴图也是线性灰度图，0.0（black）表示物体表面光滑，1.0（white）表示物体表面粗糙。</p><h2 id="specularglossiness-workflow-高光光泽度贴图">Specular/GlossinessWorkflow 高光/光泽度贴图</h2><p>以下简称为高光流。在高光工作流中也有三张特定贴图：<strong>漫反射贴图(diffuse)、反射率贴图(specular)和光滑度贴图(glossiness)。</strong>高光工作流与金属工作流最明显的区别在于，高光工作流中的电介质基础反射率F0 不在默认取值为0.04，而是和金属的反射率一同存储于反射率贴图中。</p><figure><img src="/images/PBR/2-10.png"alt="高光工作流特有贴图 引自《The PBR Guide - Part 2》" /><figcaption aria-hidden="true">高光工作流特有贴图 引自《The PBR Guide -Part 2》</figcaption></figure><p><strong>Diffuse (RGB - sRGB)</strong></p><figure><img src="/images/PBR/2-11.png"alt="漫反射贴图 引自《The PBR Guide - Part 2》" /><figcaption aria-hidden="true">漫反射贴图 引自《The PBR Guide - Part2》</figcaption></figure><p>高光工作流中，漫反射贴图只存储电介质的漫反射值，对于金属表面，因为没有漫反射故存储为黑色（0，0，0）。</p><p><strong>Specular (RGB - sRGB)</strong></p><figure><img src="/images/PBR/2-12.png"alt="反射率贴图 引自《The PBR Guide - Part 2》" /><figcaption aria-hidden="true">反射率贴图 引自《The PBR Guide - Part2》</figcaption></figure><p>高光流使用了一张独立贴图存储物体表面的反射率，相比于金属流中电介质默认的0.04基础反射率，高光流对电介质的基础反射率F0拥有更灵活的控制。由于电介质的反射率比金属低很多，所以贴图的电解质部分会比较暗。</p><p><strong>Glossiness (Grayscale - Linear)</strong></p><figure><img src="/images/PBR/2-13.png"alt="光泽度贴图 引自《The PBR Guide - Part 2》" /><figcaption aria-hidden="true">光泽度贴图 引自《The PBR Guide - Part2》</figcaption></figure><p>于金属流相同，高光流的光泽度贴图描述了物体表面的不规则度（SurfaceIrregularities）。不同的是，贴图中black（0.0）代表粗糙表面，white（1.0）代表光滑表面，是金属流中的粗糙度贴图的inverse版本。（上图颜色应该是标反了）</p><h2id="maps-common-to-both-workflows-两种工作流中的通用贴图"><strong>MapsCommon To Both Workflows 两种工作流中的通用贴图</strong></h2><p>这里只列举一些用于PBR的常见贴图，而且不做深入阐述。</p><p><strong>Normal 法线贴图</strong></p><p>法线贴图用于模拟物体表面细节，通常存储在切线空间中，是使用非常普遍的一张贴图。这里不再展开，具体可看文章<ahref="https://learnopengl.com/Advanced-Lighting/Normal-Mapping">《learnopengl- Normal Mapping》</a>。</p><p><strong>Height 高度贴图</strong></p><p>高度贴图通常被用来渲染Displacement效果，可以用来做视差映射（parallaxmapping）给纹理增加深度细节。具体见文章<ahref="https://learnopengl.com/Advanced-Lighting/Parallax-Mapping">《learnopengl- Parallax Mapping》</a>。</p><p><strong>Ambient Occlusion 环境光遮蔽贴图</strong></p><p>AO贴图用于定义表面上点在环境光下的暴露程度，其应该只影响漫反射而不影响高光反射。</p><figure><img src="/images/PBR/2-14.png"alt="AO贴图 引自《The PBR Guide - Part 2》" /><figcaption aria-hidden="true">AO贴图 引自《The PBR Guide - Part2》</figcaption></figure><h2 id="texel-density-纹素密度">Texel Density 纹素密度</h2><p>两种工作流中，如果贴图的纹素密度不合理都会产生<strong>边缘走样的现象（fringe）</strong>。</p><p>在金属流中边缘走样现象表现为，在电介质表面和明亮的金属表面的交界处产生白边。这是由于金属流的基础色贴图（BaseColor）中，同时存储电介质的漫反射颜色与金属的反射率。当纹素密度不合适时，因为金属反射率较高，两者插值(interpolated)会生成亮度较高的值从而导致白边。如下图：</p><figure><img src="/images/PBR/2-15.png"alt="金属流中的白边现象 引自《The PBR Guide - Part 2》" /><figcaption aria-hidden="true">金属流中的白边现象 引自《The PBR Guide -Part 2》</figcaption></figure><p>而在高光流中，在电介质表面和金属表面的交界处会产生黑边。相比于白边，黑边不易被观察到。高光流的边缘走样现象产生原因与金属流相同，只是由于高光流的漫反射贴图（diffuse）中金属漫反射为零，插值之后会生成颜色较暗的值。如下图：</p><figure><img src="/images/PBR/2-16.png"alt="高光流中的黑边现象 引自《The PBR Guide - Part 2》" /><figcaption aria-hidden="true">高光流中的黑边现象 引自《The PBR Guide -Part 2》</figcaption></figure><p>一张低分辨率的贴图，或是一高分辨率贴图局部UV面积太小，都可能导致纹素密度过低，从而出现边缘走样现象（如下图）。纹素的密度对边缘走样有着直接的影响，提供足够的纹素密度是缓解边缘走样现象最好的方法。</p><figure><img src="/images/PBR/2-17.png" alt="引自《The PBR Guide - Part 2》" /><figcaption aria-hidden="true">引自《The PBR Guide - Part2》</figcaption></figure><h3 id="comparison-两种工作流的对比">Comparison<strong>两种工作流的对比</strong></h3><p><strong>金属/粗糙度工作流</strong></p><p>优势：一张RGB贴图和两张灰度图，纹理缓存压力更小；</p><p>劣势：固定的电介质基础反射率0.04；白色的边缘走样影响渲染表现；</p><p><strong>高光/光泽度工作流</strong></p><p>优势：更可控的电介质基础反射率 F0；黑色的边缘走样对渲染表现影响较小；</p><p>劣势：两张RGB贴图和一张灰度图，纹理缓存压力大；</p><h3 id="参考文献">参考文献</h3><p><ahref="https://en.wikipedia.org/wiki/Gamma_correction">wiki-Gamma-correction</a></p><p><ahref="https://www.zhihu.com/question/27467127/answer/37555901">Gamma是什么</a></p><p><ahref="https://www.bilibili.com/video/BV1iQ4y1g7oX">你所不了解的Gamma究竟有多神奇|视频图像中的Gamma校正有什么作用_哔哩哔哩_bilibili</a></p><p><ahref="https://www.bilibili.com/video/BV15t411Y7cf">Gamma校正与线性工作流入门讲解</a></p><p><ahref="https://creativecloud.adobe.com/cc/learn/substance-3d-designer/web/the-pbr-guide-part-2?locale=en">ThePBR Guide - Part2</a></p><p><a href="https://cloud.tencent.com/developer/article/1603068">The PBRGuide - Part2 中文版</a></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graphic </tag>
            
            <tag> PBR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PBR简述（一）理论基础</title>
      <link href="/2024/07/26/Game%20Development/PBR%E7%AE%80%E8%BF%B0%EF%BC%88%E4%B8%80%EF%BC%89%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/"/>
      <url>/2024/07/26/Game%20Development/PBR%E7%AE%80%E8%BF%B0%EF%BC%88%E4%B8%80%EF%BC%89%E7%90%86%E8%AE%BA%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<p>本文中所引用的图示均标识出处，参考文章将在文末给出。</p><p><strong>基于物理的渲染（Physically BasedRendering)，</strong>简称为PBR，在一定程度上基于现实世界物理原则进行渲染，相比于Phong或者Blinn-Phong光照更真实。PBR因其优秀的视觉效果，被广泛应用于各大游戏引擎中。目前有两种较为主流的工作流，<strong>金属/粗糙度工作流（Metal/RoughnessWorkflow）</strong>和<strong>高光/光泽度工作流（Specular/GlossinessWorkflow）</strong>，这两个工作流只会在下文中被简单提及，具体内容会在下篇文章进行讨论，本文着重介绍PBR理论基础。</p><p>看一个光照模型是不是PBR一般会看它满不满足下面三个条件：</p><ol type="1"><li><strong>基于微面元表面模型（microfacet suface model）</strong></li><li><strong>能量守恒</strong></li><li><strong>应用基于物理的双向反射分布函数（Bidirectional reflectivedistribution function, BRDF）</strong></li></ol><p>当然上面只是一个判断标准，就是界定了一个模糊的PBR范围，本文叙述的是普遍性的PBR实现方案。渲染其实是解渲染方程（RenderingEquation）,PBR描述的就是渲染方程的一个特例。在看渲染方程之前，我们先了解PBR遵守的理论模型。</p><h3 id="微面元理论-microfacet-theory"><strong>微面元理论 MicrofacetTheory</strong></h3><p>微面元理论是PBR基础，其认为在微观尺度上，任何平面都可以被称为<strong>微面元</strong>的微小镜面描述。微小镜面排列的混乱程度会影响光线的行为，表现上即为：微小镜面排列越混乱，入射光的反射光线更趋向于向不同方向发散（Scatter），产生的镜面反射范围更大；反之，微小镜面排列越整齐，入射光的反射光线更更趋向于同一方向，镜面反射范围更小更锐利。</p><p>表面不规则（SurfaceIrregularities）程度在不同PBR流程中使用不同的名词进行描述，例如粗糙度（Roughness）、平滑度（Smoothness）和光泽度（Glossiness）。无论名词如何，都是在描述物体表面的同一性质，即物体微面元的几何细节。</p><p>PBR中的BRDF也是基于微面元理论的，具体内容将在公式部分展开。</p><figure><img src="/images/PBR/PBRGuide-1.png"alt="引自《The PBR Guide - Part1》" /><figcaption aria-hidden="true">引自《The PBR Guide -Part1》</figcaption></figure><h3 id="能量守恒-conservation-of-energy"><strong>能量守恒 Conservationof Energy</strong></h3><p>能量守恒是很容易理解的理论，我们多多少少都会接触相关知识。PBR中的能量守恒指，不考虑自发光的情况下，物体辐射的能量永远不能超过物体接收的能量。</p><h3 id="光线-light-rays"><strong>光线 Light Rays</strong></h3><p>从物理层面对渲染进行建模不可避免的要研究光线的行为。光线在均匀介质中沿直线传输，并在不同介质的交界处发生反射和折射。本文对光线行为的分析均遵循”光线从真空射向不透明介质“这一前提，即分析不透明物体表面附近光线的行为。当光束射向不透明物体表面时，光线会产生以下行为：</p><figure><img src="/images/PBR/PBRGuide-2.png"alt="引自《The PBR Guide - Part1》" /><figcaption aria-hidden="true">引自《The PBR Guide -Part1》</figcaption></figure><ul><li>部分光束被物体表面<strong>反射</strong>，反射方向遵循反射定律；</li><li>剩余光束<strong>折射</strong>进入物体内部；</li></ul><p>两种行为都会吸收光线的部分能量，不同物体会吸收特定波长的光线从而使物体呈现不同颜色。对于折射部分我们需要更深入展开，因为其行为较为复杂。</p><p>通常情况下我们渲染的物体都是非均匀的介质，光线在其内部并非沿直线传播，会发生散射现象。在物体内部进行多次散射之后，光线或被物体吸收，或再次离开物体。离开物体的光线对物体表面的影响需要根据物体的性质决定：</p><ul><li>对于高吸收、低散射的物体，光线经过多次散射之后就会吸收掉，那些离开物体的只能是经过少次散射的光线，此时我们认为其离开物体的位置与进入物体的位置之间的距离可以忽略不记。</li><li>对于高散射、低吸收的物体，例如牛奶、皮肤等，光线即使经过较多次散射也不会被吸收，所以离开物体的位置与进入物体的位置距离较远，不可以忽略不计，在渲染时应当考虑其对表面的影响。这类物体的渲染通常会使用用到<strong>次表面反射技术</strong>，并不在本文的讨论范围之内。</li></ul><p>在渲染中，被物体表面直接反射的光通常具有相同的方向，被称为<strong>高光反射（SpecularReflection）</strong>，折射进入物体内部又因散射离开物体的光的方向是随机的，被称为<strong>漫反射（DiffuseReflection）</strong>。</p><p>特殊的，金属（Metallic，或称导体，Conductor）表面会将光线折射部分全部吸收，只留下反射部分，非金属（Non-Metallic，或称介电质，Dielectricsl）表面则不会。</p><h3 id="菲涅尔方程-fresnel-equation">菲涅尔方程 Fresnel Equation</h3><p>菲涅尔效应简单来说就是，物体表面反射光的程度取决于观察角度。这种现象在生活中很常见，例如，越是以平行水面的方向观察水面，看到的反射光线越多。</p><figure><img src="/images/PBR/FresnelFactor-1.png"alt="引自《Fresnel Factor》" /><figcaption aria-hidden="true">引自《Fresnel Factor》</figcaption></figure><p>这里提到菲涅尔效应是因为其描述了观察角度对物体表面反射光线程度的影响，而菲涅尔方程可以帮助计算出光线被反射部分所占比例，如上文所述，根据能量守恒就能知道光线折射部分所占比例。完整的菲涅尔方程除了和两种介质的折射率<spanclass="math inline">\(n_1\)</span> 、<spanclass="math inline">\(n_2\)</span> 、入射角<spanclass="math inline">\(\theta_i\)</span>有关，还与入射光的偏振有关，具体如下:</p><p><strong>s偏振</strong> <span class="math inline">\(R_s=\left[\frac{n_1cos\theta_i - n_2\sqrt{1-\left( \frac{n_1}{n_2}sin\theta_i\right)^2}}{n_1cos\theta_i + n_2\sqrt{1-\left(\frac{n_1}{n_2}sin\theta_i \right)^2}} \right]^2\)</span></p><p><strong>p偏振</strong> <span class="math inline">\(R_p=\left[\frac{n_1\sqrt{1-\left( \frac{n_1}{n_2}sin\theta_i\right)^2}-n_2cos\theta_i}{n_1\sqrt{1-\left( \frac{n_1}{n_2}sin\theta_i\right)^2}+n_2cos\theta_i} \right]^2\)</span></p><p>入射光无偏振时，反射比取两者的平均值：<span class="math inline">\(R =\frac{R_s + R_p}{2}\)</span> 。</p><p>菲涅尔方程无容置疑的有些复杂，于是实时渲染中使用<strong>Fresnel-Schlick近似</strong>：<span class="math display">\[F_{Schlick}(h,v,F_0)=F_0+(1-F_0)(1-cos\theta_i)^5\]</span> 近似方程中的 <span class="math inline">\(F_0\)</span>是物体表面的基础反射率（basereflectivity），表示光线垂直地（与法线夹角为0）射在平面上时高光反射的比例，可由折射率IOR计算得出。</p><figure><img src="/images/PBR/PBRGuide-3.png"alt="引自《The PBR Guide - Part1》" /><figcaption aria-hidden="true">引自《The PBR Guide -Part1》</figcaption></figure><p>但是对于金属（导体）表面，其折射率IOR为复数，通过折射率计算的 <spanclass="math inline">\(F_0\)</span>略微复杂。于是提前测算出各种类物体表面的基础反射率 <spanclass="math inline">\(F_0\)</span> ,常见物体的基础反射率见下表：</p><figure><img src="/images/PBR/learnopengl-1.png"alt="引自《learnopengl-PBR》" /><figcaption aria-hidden="true">引自《learnopengl-PBR》</figcaption></figure><p>通过表格中的数据可以观察到一个有趣的现象，非金属表面的基础反射率都不高于0.17，而金属表面的基础反射率更高一些，在0.5-1.0范围内。此外，非金属表面反射率不会随波长变化，而金属表面的反射率会对波长产生剧烈反应。因此用RGB表示基础反射率<span class="math inline">\(F_0\)</span>时，金属会带有色彩。<strong>根据上文中光线部分内容我们可以认为，金属表面的颜色是由高光反射造成的，非金属表面的颜色是由漫反射造成的。</strong></p><p>金属。在金属工作流中，非金属表面的基础反射率被默认固定为常见非金属表面基础反射率的<strong>平均值0.04</strong>，过去的经验表明此默认值在大多数情况下都可以得到可信的物理效果。而金属表面的基础反射率<span class="math inline">\(F_0\)</span>则需要额外的输入，使用其作为金属表面的颜色。此方面更详细的内容会在下篇文章说明。</p><p><strong>注：</strong>理论上来说，一个表面的金属度应该是二元的：要么是金属要么不是金属，不能两者皆是。但是，大多数的渲染管线都允许在0.0至1.0之间线性的调配金属度。这主要是由于材质纹理精度不足以描述一个拥有诸如细沙/沙状粒子/刮痕的金属表面。通过对这些小的类非金属粒子/刮痕调整金属度值，我们可以获得非常好看的视觉效果。</p><h3 id="辐射度量学-radiometry">辐射度量学 Radiometry</h3><p>上文我们了解了光线在物体表面的行为，要将其应用于渲染中就需要对光线进行定量分析，了解渲染领域“光线”的物理含义。这就需要涉足一些辐射度量学的内容，其中的定义初看有些难以理解，本文也不确定可不可以清楚得对其进行阐述。于是建议大家观看<ahref="https://www.bilibili.com/video/BV1X7411F744/?p=15&amp;spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=a6d4de83b46a19b70d53fc1e9adb6574">闫令琪老师的讲义</a>中对辐射度量学的讲解，下文也只是对相关内容做概括性总结，英文对于名词的定义更加清晰，所以多会引用英文原文进行解释。</p><p><strong>辐射能量（Radiant energy）</strong></p><p>Radiant energy is the energy of electromagnetic radiation. It ismeasured in units of joules, and denoted by the symbol:</p><p><span class="math inline">\(Q[J = Joule]\)</span></p><p>辐射能量就是指能量源辐射出来的电磁能量，单位为焦耳。</p><p><strong>辐射通量（Radiant flux，power）</strong></p><p>Radiant flux(power) is the energy emitted, reflected, transmitted orreceived, per unit time.</p><p><span class="math inline">\(\Phi\equiv\frac{dQ}{dt}[W=Watt][lm=lumen]^*\)</span></p><p>辐射通量是单位时间内辐射的能量，单位为瓦特，可以简单理解为光源输出能量的功率。光是不同波长的能量集合，不同波长的光具有不同的能量，而每种波长与特定的颜色相关。在渲染中会使用三原色编码RGB作为辐射通量的简化表示。</p><p><strong>辐强度（Radiant intensity）</strong></p><p>The radiant(luminous) intensity is the power <strong>per unit solidangle</strong> emitted by a point light source.</p><p><span class="math inline">\(I(\omega)\equiv \frac{d\Phi}{d\omega}[\frac{W}{sr}][\frac{lm}{sr}=cd=candela]\)</span></p><p>辐强度表示在单位球面上光源向单位立体角投射的辐射通量，</p><p><strong>辐照度（Irradiance）</strong></p><p>The irradiance is the power <strong>per</strong>(perpendicular/projected) <strong>unit area</strong> incident on asurface point.</p><p><span class="math inline">\(E(x)\equiv\frac{d\Phi(x)}{dA}[\frac{W}{m^2}][\frac{lm}{m^2}=lux]\)</span></p><p>辐照度是单位照射面积接收到的辐射通量。既然提到表面的单位面积，于是这里就存在表面法线方向和光照方向的问题。更严谨一点的说法是，单位照射面积在光线方向上接收到收到的辐射通量，一般公式后方会再乘以。</p><figure><img src="/images/PBR/learnopengl-2.png"alt="引自《learnopengl-PBR》" /><figcaption aria-hidden="true">引自《learnopengl-PBR》</figcaption></figure><p><strong>辐亮度（Radiance）</strong></p><p>The radiance (luminance) is the power emitted, reflected, transmittedor received by a surface, <strong>per unit solid angle</strong>,<strong>per unit area</strong> (perpendicular area) <spanclass="math display">\[L(p,\omega) \equiv \frac{d^2\Phi(p, \omega)}{d\omega dAcos\theta}\\[\frac{W}{sr \cdot m^2}][\frac{cd}{m^2}=\frac{lm}{sr \cdot m^2}=nit]\]</span> 辐亮度是指每单位立体角、每单位垂直面积接受到的辐射通量。</p><p>在渲染中<strong>Irradiance</strong>和<strong>Radiance</strong>是经常提到的概念，通俗一些讲Irradiance是单位面积<span class="math inline">\(dA\)</span>在其所处上半球收到的所有能量和，而Radiance特指单位面积 <spanclass="math inline">\(dA\)</span> 在单位立体角 <spanclass="math inline">\(d\omega\)</span> 上收到的能量。</p><p>更通俗一点的类比就是，Irradiance是着色片段fragment在其所处上半球各个方向收到的光照强度和，Radiance是该着色片段fragment在某特定方向<span class="math inline">\(\omega\)</span> 接收到的光照强度。</p><p>渲染中涉及到的辐射度量学内容基本就上述这些，引入他们是为了更好的理解在渲染领域至关重要的渲染方程，消化完此部分内容后就直接进入PBR的公式部分。</p><p>PBR使用了渲染方程（Rendering Equation）的一个特化版本： <spanclass="math display">\[L_o(p,\omega_o)=L_e(p,\omega_o)+\int_\Omegaf_r(p,\omega_i,\omega_o)L_i(p,\omega_i)n\cdot\omega_id\omega_i\]</span></p><ul><li><span class="math inline">\(p\)</span>是物体表面片元</li><li><spanclass="math inline">\(L_o(p,\omega_o)\)</span>是物体表面片元p所反射朝向<span class="math inline">\(\omega_o\)</span>的辐亮度，o代表outgoing</li><li><spanclass="math inline">\(L_e(p,\omega_o)\)</span>是物体表面片元p朝向 <spanclass="math inline">\(\omega_o\)</span> 发出的辐亮度，e代表emission</li><li><spanclass="math inline">\(f_r(p,\omega_i,\omega_o)\)</span>是物体在片元p处的BRDF</li><li><span class="math inline">\(L_i(p,\omega_i)\)</span>物体表面片元p从<span class="math inline">\(\omega_i\)</span>接收到的辐亮度，i代表incoming</li><li><span class="math inline">\(n\cdot\omega_i\)</span>物体片元p处法线与<span class="math inline">\(\omega_i\)</span> 的点积</li><li><spanclass="math inline">\(\Omega\)</span>是物体表面片元p所在的上半球</li></ul><p>渲染方程描述的是物体表面某一片元 <spanclass="math inline">\(p\)</span> 向 <spanclass="math inline">\(\omega_o\)</span> 辐射出的最终能量值，包括其自身向<span class="math inline">\(\omega_o\)</span> 辐射的能量 <spanclass="math inline">\(L_e(p,\omega_o)\)</span> 和向 <spanclass="math inline">\(\omega_o\)</span> 反射的其他光源辐射的能量 <spanclass="math inline">\(\int_\Omegaf_r(p,\omega_i,\omega_o)L_i(p,\omega_i)n\cdot\omega_id\omega_i\)</span>。自身辐射的能量不必多说，PBR的主要研究内容是剩下的反射部分。</p><p>有了第一部分理论部分的铺垫，不难理解片元 <spanclass="math inline">\(p\)</span>能反射的能量都来源于其从上半球接收的能量Irradiance，公式中的积分就是对上半球的积分。将能量离散化为从各个方向接收到能量的和之后，从特定方向<span class="math inline">\(\omega_i\)</span>接收到的能量就是radiance：<spanclass="math inline">\(L_i(p,\omega_i)n\cdot\omega_i\)</span>。物体表面并不会将接受到的辐射能量全部反射出去，其反射能量的比例根据表面性质决定。<spanclass="math inline">\(f_r(p,\omega_i,\omega_o)\)</span>描述的就是表面的反射性质，其即是双向反射分布函数BRDF。</p><h3 id="brdf"><strong>BRDF</strong></h3><p>在文首曾提到PBR使用的是基于物理的BRDF，这个基于物理也就是基于微面元模型。它接受入射（光）方向<span class="math inline">\(\omega_i\)</span> ，出射（观察）方向 <spanclass="math inline">\(\omega_o\)</span> ，平面法线 <spanclass="math inline">\(n\)</span> 以及一个用来表示微面元性质的参数 <spanclass="math inline">\(\alpha\)</span>作为函数的输入参数。BRDF可以近似求出从 接受光线对从辐射出的光线的贡献程度。目前已经有多种BRDF被用于实时或离线渲染，在实施渲染中使用最广泛的是Cook-TorranceBRDF，其公式如下：<br /><span class="math display">\[f_r = k_df_{lambert}+k_sf_{cook−torrance}\\ f_{lambert}=\frac{c}{\pi}\\f_{cook-torrance}=\frac{DFG}{4(w_o \cdot n)(w_i \cdot n)}\]</span> Cook-TorranceBRDF将对接受的光线分为两部分处理，即第一段理论部分提到的反射部分和折射部分。<span class="math inline">\(k_d\)</span> 是漫反射能量所占比率，<spanclass="math inline">\(k_s\)</span>是高光反射部分比率。 <spanclass="math inline">\(k_s\)</span> 可由菲涅尔方程得出，根据能量守恒得<span class="math inline">\(k_d = 1-k_s\)</span> 。</p><p>对于漫反射部分，Cook-TorranceBRDF认为光线经过物体表面散射之后向各个方向均有光线出射，微面元性质不会影响漫反射强度。漫反射强度只与菲涅尔现象（观察角度）有关，所以 <spanclass="math inline">\(f_{lambert}\)</span> 只与漫反射颜色有关。</p><p>对于镜面反射，其影响因素就比较多。 <spanclass="math inline">\(f_{cook-torrance}\)</span>分子包含了三个函数，此外分母还有一个标准化因子（本文不对标准化因子进行讨论）。分子中的三个函数如下：</p><ul><li><strong>法线分布函数（Normal distributionfunction）D</strong>：估算在受到表面粗糙度的影响下，微小镜面朝向方向与半程向量一致的的数量；</li><li><strong>菲涅尔方程（Fresnelequation）F</strong>：在不同表面角下高光反射的占比；</li><li><strong>几何函数（Geometryfunction）G</strong>：描述微面元自成阴影的属性；</li></ul><p>菲涅尔方程的部分已经在上文理论部分进行讨论，这里我们直接使用<strong>Fresnel-Schlick近似</strong>：<span class="math display">\[F_{Schlick}(h,v,F_0)=F_0+(1-F_0)(1-cos\theta_i)^5\]</span> 可以看到 中已经包含了F，而，所以为避免重复，上述BRDF方程带入后应为： <span class="math display">\[f_r = k_d \frac{c}{\pi} + \frac{DFG}{4(w_o \cdot n)(w_i \cdot n)}\]</span></p><h3 id="法线分布函数-normal-distribution-function">法线分布函数 Normaldistribution function</h3><p>进行发现分布函数说明之前，先清楚一个概念，<strong>半程向量</strong><span class="math inline">\(h\)</span><strong>。</strong>半程向量在渲染中经常使用，其是指光源方向与视角方向的夹角<span class="math inline">\(h=\frac{L+V}{\left|| L+V \right||}\)</span>，如下图所示：</p><figure><img src="/images/PBR/wiki-1.png" alt="halfway vector - wiki" /><figcaption aria-hidden="true">halfway vector - wiki</figcaption></figure><p>于是可以知道，从 <span class="math inline">\(\omega_i\)</span>入射的光能够被反射到 <span class="math inline">\(\omega_o\)</span>需要表面法向量 <span class="math inline">\(n\)</span> 和半程向量 <spanclass="math inline">\(h\)</span>方向一致。而法线分布函数D就是对于某一微面元，从统计学上近似表示这些微小镜面法向量与半程向量方向一致的比率。常用的法线分布函数是Trowbridge-ReitzGGX: <span class="math display">\[NDF_{GGXTR}(n,h,\alpha) = \frac{\alpha^2}{ \pi ( (n \cdot h)^2  (\alpha^ 2 - 1) + 1)^2 }\]</span> <img src="/images/PBR/learnopengl-3.png"alt="引自《learnopengl-PBR》" /></p><p>此函数可以模拟出上图效果，当表面粗糙度较低（也可以说表面比较光滑）时，与半程向量取向一致的微小镜面会集中在很小的范围内，微面元可以将大部分能量反射；当表面粗糙度较高时，微小镜面的方向会更加随机，与半程向量去向一致的微小镜面就会分布在较大的范围内，微面元只可以反射部分能量。</p><h3 id="几何函数-geometry-function">几何函数 Geometry function</h3><p>在微面元理论中，微小镜面是随机排列的，这就可能导致自遮挡现象的出现（如下图），几何函数就是从统计学上近似微小镜面间相互遮挡的比率。</p><figure><img src="/images/PBR/learnopengl-3.png"alt="引自《learnopengl-PBR》" /><figcaption aria-hidden="true">引自《learnopengl-PBR》</figcaption></figure><p>与NDF类似，微面元的几何细节也会影响自遮挡性质，所以 <spanclass="math inline">\(\alpha\)</span>也会作为参数输入。GGX和Schlick-Beckmann近似的结合体几何函数如下：</p><p><span class="math display">\[G_{SchlickGGX}(n,v,k)=\frac{n\cdot v}{(n \cdot v)(1-k)+k }\]</span></p><p>这里的 <span class="math inline">\(k\)</span> 是 <spanclass="math inline">\(\alpha\)</span>的重映射（Remapping），这里先不细究，只需要知道几何函数与微面元的几何性质<span class="math inline">\(\alpha\)</span> 有关即可。</p><p>到这里，我们已经对PBR的渲染方程每一部分都有了了解，它完整描述了一个基于物理的渲染模型。当然这还不够，下面一部分主要讨论了方程在实际渲染中的一些细节。</p><h3 id="cook-torrance反射率方程">Cook-Torrance反射率方程</h3><p>让我们把使用的近似方程都带入到PBR渲染方程中，它现在应该是这个样子：<span class="math display">\[L_o(p,\omega_o)=L_e(p,\omega_o)+\int_\Omega  (k_d \frac{c}{\pi} +\frac{DFG}{4(w_o \cdot n)(w_i \cdot n)})L_i(p,\omega_i)n\cdot\omega_id\omega_i\]</span>将此方程应用到渲染中还需要解决最棘手的积分问题，我们需要考虑半球各个方向上射来的光线。幸运的是渲染中的光源数量是有限的，对于一个特定光源和特定片元fragment，只需要考虑从光源射向片元这一特定方向<span class="math inline">\(\omega\)</span>的光线对于片元的贡献。随着光源的增加，只需将每栈灯对片元的贡献做和便可得到整体贡献。</p><p>但是只考虑光源方向光线，而忽略半球上的其他方向射来的光线势必会使最终渲染结果大打折扣。于是实时渲染中将半球上的光线分为<strong>直接光照（direct）</strong>和<strong>间接光照（indirect）</strong>两部分，光源的光属于直接光照，其他部分属于间接光照。</p><p>间接光照就会考虑在直接光照中忽略的积分部分，这就是间接光照的领域了，例如IBL技术。上文几何函数中提到的重映射在直接光和IBL中就有不同取值： <span class="math display">\[k_{IBL}=\frac{\alpha ^2}{2}\]</span>对间接关照的讨论是另一个大模块了。目前可以简单认为，片元最终强度就是通过PBR方程计算出的光源直接光强度与间接光强度的和。</p><h3 id="小结">小结</h3><p>本文对PBR涉及到的理论和渲染方程做了简要的说明，而未涉及到其具体实现，希望能帮你在一定层面上理解PBR。下篇文章会着重介绍两个工作流——金属工作流与高光工作流。</p><h3 id="参考文献">参考文献</h3><p>推荐阅读原版，中文版的翻译可能会有遗漏和错误导致阅读不顺畅。</p><p><ahref="https://learnopengl.com/PBR/Theory">learnopengl-pbr原文</a></p><p><ahref="https://learnopengl-cn.github.io/07%20PBR/01%20Theory/">learnopengl-pbr中文版</a></p><p><ahref="https://creativecloud.adobe.com/cc/learn/substance-3d-designer/web/the-pbr-guide-part-1?locale=en">ThePBR Guide Part1原文</a></p><p><a href="https://cloud.tencent.com/developer/article/1571926">The PBRGuide Part1中文版</a></p><p><ahref="https://www.bilibili.com/video/BV1X7411F744/?p=15&amp;spm_id_from=333.1007.top_right_bar_window_history.content.click&amp;vd_source=a6d4de83b46a19b70d53fc1e9adb6574">GAMES101-现代计算机图形学入门-闫令琪_哔哩哔哩_bilibili</a></p><p><ahref="https://zhuanlan.zhihu.com/p/158025828">张亚坤：彻底看懂PBR/BRDF方程</a></p><p><a href="https://en.wikipedia.org/wiki/Fresnel_equations">wikiFresnel_equations</a></p><p><ahref="https://lettier.github.io/3d-game-shaders-for-beginners/fresnel-factor.html">图片来源Fresnel Factor</a></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Graphic </tag>
            
            <tag> PBR </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>山海经读书笔记（三）</title>
      <link href="/2024/05/15/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%B1%B1%E6%B5%B7%E7%BB%8F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/"/>
      <url>/2024/05/15/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%B1%B1%E6%B5%B7%E7%BB%8F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>笔记中有关《神异经》及郭璞注《尔雅》的内容多网上资料，笔者选取了其中可信性较高的部分。这两本古籍的原文可能之后有时间会买来读一读把，目前估计是没时间读了。</p><h4 id="狍鸮">狍鸮</h4><p>北山经·北次二经 钩吾之山</p><p>有兽焉，其状如羊身人面，其目在腋下，虎齿人爪，其音如婴儿，名曰狍鸮，是食人。</p>]]></content>
      
      
      <categories>
          
          <category> ink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>山海经读书笔记（二）</title>
      <link href="/2024/05/14/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%B1%B1%E6%B5%B7%E7%BB%8F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
      <url>/2024/05/14/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%B1%B1%E6%B5%B7%E7%BB%8F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h3 id="前言">前言</h3><p>笔记中有关《神异经》及郭璞注《尔雅》的内容多网上资料，笔者选取了其中可信性较高的部分。这两本古籍的原文可能之后有时间会买来读一读把，目前估计是没时间读了。</p><h4 id="猛豹">猛豹</h4><p>西山经·西次一经 南山 兽多猛豹，鸟多尸鸠。本书注解，猛豹即豹的一种，或因为豹子凶猛而称之为猛豹。嗯，很有道理的一句话。别看本书的注解很少，“猛豹”这个词还是很有说法的。后一句中的“尸鸠”即“鸤鸠”，布谷鸟，学名大杜鹃。由此可知，“猛豹”一词代只指一种兽类。晋·郭璞注，“猛豹似熊而小，毛浅，有光泽，能食蛇，食铜铁，出蜀中。”这句话可是牵扯出了一大堆东西，有资料根据“食物铜铁，出蜀中”推测其为大熊猫。这个推论的逻辑是，大熊猫古称食铁兽，又在四川，也确实是熊科动物，所以猛豹就是大熊猫。对于此观点笔者有些不认可，先不说大熊猫明显的黑白外观原文并未提及，只说了“毛浅，有光泽”，大熊猫古称食铁兽这个论据也可能并不正确。先来看看大熊猫的外号“食铁兽”是从何而来？一种说法是，大熊猫的“食铁兽”之名来自《神异经》。《神异经》现多认为是汉代东方朔模仿《山海经》撰写的志怪小说集，全书分为东荒经、东南荒经、南荒经、西南荒经、西荒经、西北荒经、北荒经、东北荒经、中荒经九章。“南方有兽焉，角足大小形状如水牛，皮毛黑如漆。食贴饮水，其粪可为兵器，其利如刚，名曰呲（啮）铁。”这句话应该是此种说法的根据，认为这句话描述的兽就是大熊猫，故古人就是称大熊猫为食铁兽。但是笔者认为《神异经》在行文内容上更偏向于神话志怪，不能作为依据，而且原文的描述与大熊猫形象有明显差异，所以呲铁不可能是古代对于大熊猫的称呼。附带一说，“蚩尤的坐骑是食铁兽”这种说法，也没在《神异经》及其他古籍找到来源，因此“蚩尤坐骑是大熊猫”这种说法更是立不住脚。另外一种说法是，大熊猫在觅食时有时会到村民家舔舐炊具上残存的盐分，村民误以为其在吃铁，故称其为“食铁兽”。这种说法比较合理，但是来源以不可考，应当是在铁质炊具普及之后的时期。回到上文，就算大熊猫古称确实为食铁兽。原文描述的其他特征与熊猫也并不相符，所以猛豹就是大熊猫的可能性很小。另外一种观点说，猛豹可能是如今的亚洲貘（或称马来貘），其根据是《尔雅》及其注中的两句话。《尔雅·释兽》云：“貘，白豹”。郭璞注：“熊，小头庳脚，黑白駁，能舐食铜铁及竹骨。骨节强直。中实少髓，皮辟湿，或曰豹白色者别名貘。”于是有资料认为，根据尔雅所云，貘是一种豹，“猛豹”当为“貘豹”的讹字。此种说法肯定了《尔雅》中的貘、白豹即为如今的亚洲貘。根据亚洲貘的外表来看，这种说法有一定可能，亚洲貘出生时体表有条纹状和点状保护色，与豹相似，成年后体表颜色变为黑白，其中身体主要为白色，可能会被称之为白豹。虽然亚洲貘如今在国内没有分布，但是国内曾出土周朝的貘尊，其形状与貘无异。但仍有疑点，根据郭璞注内容，首先其并不确定貘是指白色的豹还是一个单独的物种貘，其次郭璞对于貘的描述于大熊猫更为相符。就算撇开郭注不谈，此说法还是基于“猛”为“貘”讹字，但是这两个字结构有较为明显的差异，笔者认为对于古籍中的讹字推测还是要更为严谨些。最后，对于猛豹具体指什么，由于《山经》原文并未对其进行详细描述，而且没有其他古籍相互印证，如今还没有定论。原文并未对猛豹进行详细描述，可能是因为其和鸤鸠一样分布较为广泛比较常见吧。</p><h4 id="鹦䳇">鹦䳇</h4><p>西山经·西次一经 黄山 有鸟焉，其状如鸮，青羽赤喙，人舌能言，名曰鹦䳇。文中描述可以确定其为一种鸟类，本书作者直接将其推断为鹦鹉笔者觉得有些不够严谨。现代鹦鹉多指鹦形目下的各种鸟类，而文中有描述“其状如鸮”，“鸮”是古人对猫头鹰的称呼，属于的鸮形目。两者的头部区别还是比较明显的，鸮形目的头部明显更宽厚。根据文中的描述，只能确定其为一种会模仿人类语言的鸟类，而不能确定为鹦鹉。因为现实中除了鹦鹉还有其他鸟类可以模仿人类语言，如八哥属下的鸟类。</p><h4 id="帝江">帝江</h4><p>西山经·西次三经 天山有神焉，其状如黄囊，赤如丹火，六足四翼，浑敦无面目，是识歌舞，实惟帝江也。帝江也算衍生作品的常客，其模样通常会被刻画为圆嘟嘟、毛绒绒的类哺乳动物形象，有着两对扑棱扑棱的翅膀，没有五官，讨喜的模样应该有着不错的人气。本书作者并未对帝江的原型进行推断，只是说了部分资料会将“帝江”与“混沌”混为一谈。于是，笔者就依据本书的思路大胆进行一个猜测，其可能是国内分布的某种蝶科物种，“六足四翼”是蝶科动物的明显特征，“状如黄囊”“赤如丹火”也在蝶科外观的合理范围内，“是识歌舞”对蝶科物种描述也恰到好处。唯有“混敦无面目”不知《山经》作者是何用意，蝶科物种面部特征还是比较明显的，一般都具有一对触须，一对大型的半圆状复眼。</p><h4 id="上帝">上帝</h4><p>书中“帝之囿时”翻译为“上帝的花园”，当“上帝”出现在译文时，笔者脑子停转了一下，以为是作者的笔误。因为过去的经历告诉我，“上帝”一词都是伴随着西方宗教而出现的，其应该是一个舶来词，为何会出现在古籍的译文中。于是查阅了一些资料，解开了疑惑，“上帝”本就是中国古代存在的词语。关于“上帝”一词含义的演变，可以观看B站up文化遗产搬运委员会的视频《古老的众神之主，中华文明国家祭祀的至高之神，是怎么诞生的》详细了解，这里只做简要概述。<em>吾天主，乃古经书所称上帝也 ——《天主实义》利玛窦</em>中国上帝一般指昊天上帝，后有民间俗称老天爷。《诗经》与《圣经》中“上帝”一词含义的混淆出现于明清之际，当时传教士为了便于传教采用了“援儒入耶”策略，有意将先秦典籍中的“上帝”与《圣经·旧约》中的“主”混为一谈。这种行为一方面造成了中西方上帝形象的混淆，另一方面使得中国上帝即西方之“主”的观念根深蒂固。以致于现代国人一提到“上帝”一词多想到的是西方的“主”。想起浏览本书的商店页面评论区时，看到有人因书中有“上帝”的翻译而给本书打了低星，笔者也不知道该说什么好，毕竟笔者也是后来查阅资料了解到先秦文献中的昊天上帝的。</p>]]></content>
      
      
      <categories>
          
          <category> ink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>山海经读书笔记（一）</title>
      <link href="/2024/05/13/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%B1%B1%E6%B5%B7%E7%BB%8F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"/>
      <url>/2024/05/13/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E5%B1%B1%E6%B5%B7%E7%BB%8F%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h4 id="前言">前言</h4><p>想读《山海经》已经很多年了，最近空闲时间比较多，买了本实体书读来看看。</p><p>最早了解到这本书应该是鲁迅先生的文章《阿长与<山海经>》，随后由于媒体的发展，各种影视作品中也不断出现《山海经》的影子。伴随着各种论坛中对它千奇百怪的评价，我对《山海经》的兴致愈来愈高，路过书店或者书摊时看到它总会多瞅几眼。但是,无论是一些相关书籍中浮夸的画风，还是让人觉得不怎么踏实的奇怪修饰语，都让笔者不敢轻易出手购买，直到在线上的中国国家地理图书专营店闲逛看到了由刘宗迪注译、任才峰绘的《山海经：插图珍藏版》。这本书由中信出版集团出版，与中国国家地理·图书项目合作，想必会比较靠谱罢。</p><p>后续撰写读后感时发现一些语句主语容易混淆，本系列文章中做出以下约定：</p><p>此书 、本书： 笔者阅读的《山海经：插图珍藏版》————————</p><h4 id="山海经导读一-读后记">《山海经》导读一 读后记</h4><p>此书导读部分较长且分了章节，于是决定细细读一下。</p><p>导读中提到《山海经》自古被冠以怪物之书的缘由，三头六目、六足三翼的怪鸟和长着人脸、叫起来像鸳鸯的怪物，很难不让人直接把这本书定性。</p><p><em>司马迁在《大宛列传》结尾写道：”故言九州山川，《尚书》近之矣。至《禹本纪》《山海经》所有怪物，余不敢言之也。”</em></p><p><em>鲁迅后来写《中国小说史略》，讲到神话与志怪，首先就以《山海经》为例，说它记录的都是山川神祇异物，”盖古之巫书也“。</em></p><p>其后导读部分简单介绍了《山海经》的结构和内容，确实会对我们后续阅读有所帮助，这里就简单记述一下。</p><p>《山海经》包括《山经》和《海经》两部分，这两部分原本是两部书，其性质、体例和成书年代都不相同。导读一着重讨论了《山经》。《山经》又包括《南山经》三篇、《西山经》四篇、《北山经》三篇、《东山经》四篇和《中山经》十二篇，以规整的格式记录了山峦和河流，以及生活在其中的各种生物、矿物等。</p><p>这种行书方式确实不像是独出心裁、异想天开的志怪或者神话，更像精心规划、一五一十记录各种自然资源的账本。《山经》在最后也假托大禹的口吻对天下土地面积和自然资源做了统计，并以“能者有余，拙者不足”为结语。</p><p>其后此书作者以穿山甲和鱿鱼为例，讲述了《山经》中“怪物的生成机制”，即《山经》中为何以“三头六目、六足三翼”来描述生物。本书作者将《山经》中的描述做了详细分析，涉及到了山经中的原文，于是笔者打算留到后续原文处再细讲。</p><p>本书作者还对《山经》书名进行了讨论。其提到，《山经》全称《五臧山经》，“五臧”就是五类宝藏的意思。至于五臧具体指哪五类宝藏，《山经》中并未明说，可能指草、木、鸟、兽、矿物这五类，文中记载的这五类事物最多。</p><p>除此之外，本书作者还指出“山经”或许原为“山志”，依据为《山经》五篇结尾都有一段文字进行总结，例如《南山经》末尾：“右南经之山志，大小凡四十山，万六千三百八十里。”“经”字，在原文中并不是经书、经典的意思，而是经过的意思。后来的抄书匠没有读懂这句话，将“经”当作书名，一讹传讹，直到现在。</p><p>导读一基本上已经表明了本书解读《山海经》的大致方向，即不以怪物之书歪曲其内容，力图科学还原其描述之物原本面貌。</p>]]></content>
      
      
      <categories>
          
          <category> ink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 读书笔记 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unity Shader Base</title>
      <link href="/2023/12/29/Unity/UnityEngine/Unity-Shader-Base/"/>
      <url>/2023/12/29/Unity/UnityEngine/Unity-Shader-Base/</url>
      
        <content type="html"><![CDATA[<h2 id="语义-semantics">语义 Semantics</h2><h3 id="基础语义">基础语义</h3><p>Unity ShaderLab 的语义实际上就是 Cg/HLSL 的语义，<ahref="https://learn.microsoft.com/zh-cn/windows/win32/direct3dhlsl/dx-graphics-hlsl-semantics?redirectedfrom=MSDN">微软官方DirectX的文档</a>中可以找到关于语义的详细说明界面。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">struct</span> <span class="title class_">appdata</span> &#123;</span><br><span class="line">float4 vertex : POSITION;</span><br><span class="line">float3 normal : NORMAL;</span><br><span class="line">float2 uv : TEXCOORD0;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>上述代码中的<code>POSITION</code><code>NORMAL</code>等，就是语义。实际上，语义就是赋给Shader输入和输入的字符段，这个字符段表达了这个参数的含义，用于传达有关参数预期使用的信息。通俗一些讲，这些语义可以让Shader知道从哪里获取数据，并把数据输出到哪里。</p><p>Unity中，为了方便对模型数据的传输，对一些语义进行了特别的含义规定。例如，在顶点着色器的输入结构体a2v用<code>TEXCOORD0</code>来描述texcoord，Unity会时别<code>TEXCOORD0</code>语义，以吧模型的第一组纹理坐标填充到texcoord中。</p><p>但需要注意的是，即使语义的名称一样，如果出现的位置不同，含义也不同。例如，<code>TEXCOORD0</code>既可以描述顶点着色器的输入结构体a2v，也可以描述输入结构体v2f。但是，在a2v中，<code>TEXCOORD0</code>有特别的含义，即把模型的第一组纹理坐标存储在该变量中，而在v2f中，<code>TEXCOORD0</code>修饰的变量含义就可以由我们自己来决定。</p><h3 id="系统数值语义-system-value-semantics">系统数值语义 system-valuesemantics</h3><p>在DirectX10之后，有一种新的语义类型，就是系统数值语义(system-valuesemantics)。这类语义以SV开头，SV代表的含义就是系统数值(system-value)。</p><p>这些语义在渲染流水线中有特殊的含义。例如，<code>SV_POSITION</code>语义修饰顶点着色器的输出变量pos，那么就表示pos包含了可用于光栅化的变换后的顶点坐标(即齐次裁剪空间中的坐标)。这些语义描述的变量是不可以随便赋值的，因为流水线需要使用它们来完成特定的目的，渲染引擎把使用<code>SV_POSITION</code>修饰的变量经过光栅化后显示在屏幕上。</p><p>Unity并没有支持cg/HLSL提供的所有语义，可通过Unity官方文档查看<ahref="https://docs.unity3d.com/Manual/SL-ShaderSemantics.html">Shadersemantics</a></p><h2 id="标签-tags">标签 Tags</h2><p>标签Tags是键值对数据(key-value pairs ofdata)，Unity使用这些预定义的键值对定义着色器subShader或通道Pass如何使用。除了使用提供的Tags，我们也可以使用自定义的键值对。</p><h3 id="subshader-tags-in-shaderlab-reference">SubShader tags inShaderLab reference</h3><h4 id="renderpipeline">RenderPipeline</h4><p>用于URP和HDRP的标签，告诉Unity着色器适用于哪种渲染管线。<code>UniversalPipeline</code>和<code>HDRenderPipeline</code>可作为其值，当然也可以使用其他值。</p><h4 id="queue">Queue</h4><p>定义渲染队列，Unity提供了一些内置值如<code>Background</code>、<code>Geometry</code>等，可以使用如"Queue"= "[queue name]" 和 "Queue" = "[queue name] +[offset]"格式。在SRP中此值和DrawingSettings与SortingCriteria有关。</p><h4 id="rendertype">RenderType</h4><p>此标签没有固定值，是用于替换着色器的键值对。直接贴官方文档以及一篇博客：</p><p><ahref="https://docs.unity3d.com/Manual/SL-ShaderReplacement.html">Replaceshaders at runtime in the Built-In Render Pipeline</a></p><p><ahref="https://blog.csdn.net/wpapa/article/details/51319434">https://blog.csdn.net/wpapa/article/details/51319434</a></p><p>这里说一下重点。Unity提供了函数<code>Camera.RenderWithShader()</code>以及<code>Camera.SetReplacementShader()</code>来实现运行时替换物体着色器的功能。两者都具有两个参数：</p><ul><li><code>Shader shader</code>：替换的Shader</li><li><code>string replacementTag</code>：替换使用到的Tag</li></ul><p>如果replacementTag为空""，场景中的所有物体都使用给定着色器进行渲染。如果replacementTag不为空，则又分为两种情况：如果场景物体使用的着色器没有与replacementTag相同的Tag，则不渲染；如果场景物体有与其相同的Tag则使用当前着色器Tag对应的值value去函数提供的着色器中寻找相同的值，如果没有则不渲染，如果有则使用找到的子着色器进行渲染。</p><p>Unity中内置的着色器提供了<code>RenderType</code>键值对来实现此功能，所以如果我们要在运行时替换着色器一般使用此标签。</p><h4 id="forcenoshadowcasting">ForceNoShadowCasting</h4><p>控制着色器是否禁止向其他物体投射（有时是接收，因不同渲染管线及渲染路径不同）阴影，有<code>True</code>和<code>False</code>两种状态。</p><h4 id="disablebatching">DisableBatching</h4><p>控制着色器是否禁止动态合批(DynamicBatching)，有三种状态<code>True``False</code>和<code>LODFading</code>，默认值为<code>False</code>。</p><h4 id="ignoreprojector">IgnoreProjector</h4><p>着色器是否忽略Projectors的影响。默认值为<code>False</code>。</p><h4 id="previewtype">PreviewType</h4><p>是UnityEditor有关的标签，决定材质的预览状态，有球体、平面、天空盒。</p><h3 id="pass-tags-in-shaderlab-reference">Pass tags in ShaderLabreference</h3><h4 id="lightmode">LightMode</h4><p>Unity中最常用的Tags便是<code>LightMode</code>，所有的渲染管线都会使用它，而其他Tags可能只用于特定管线。</p><p>在UPR中，如果没有为Pass设置LightMode标签，其会使用<code>SRPDefaultUnlit</code>作为默认值。如果想为Pass设置LightMode标签，URP提供了以下选项：</p><ul><li><p>UniversalForward：使用此Tags的Pass用于渲染物体几何以及评估所有光照的贡献，URP在正向渲染路径中使用此标签。</p></li><li><p>UniversalGBuffer：此Pass用于渲染物体几何，但不会评估任何光照贡献，URP在延迟渲染路径中使用此标签。</p></li><li><p>UniversalForwardOnly：此Pass用于渲染物体几何以及评估所有光照贡献，与UniversalForward标签类似。其与UniversalForward不同之处在于，不仅可以用于前向渲染路径，还可以用于延迟渲染路径。</p></li></ul><p>当URP在使用延迟渲染路径时，如果一个Pass必须使用前向渲染路径来渲染物体，那么它就可以使用UniversalForwardOnly作为LightMode值。例如，URP使用延迟渲染路径渲染一个场景，而场景包含使用不适用于GBuffer的数据的物体（比如ClearCoat normals），此物体的Pass便可以使用Only值。</p><p>如果一个着色器必须在前向和延迟渲染路径中都渲染，它可以声明两个Pass然后分别使用<code>UniversalForward</code>标签和<code>UniversalGBuffer</code>标签。如果一个着色器不管URP使用的渲染路径是什么，必须使用前向渲染路径，可以声明一个Pass使用<code>UniversalForwardOnly</code>做为LightMode标签的值。</p><ul><li><p>Universal2D：此Pass用于渲染物体以及评估2D灯光的贡献，URP在渲染2D场景时使用它。</p></li><li><p>ShaderCaster：此Pass用于从灯光视线将物体深度值渲染到阴影图ShadowMap或者深度贴图Depth Texture。</p></li><li><p>DepthOnly：从相机的视角仅将深度信息渲染到深度贴图。</p></li><li><p>Meta：专门用于UnityEditor中烘焙光照贴图lightmaps，在打包时会自动剔除此Pass。</p></li><li><p>SRPDefaultUnlit：渲染物体时可以使用此Tags绘制一个额外的通道(extraPass)，例如绘制一个物体的描边。此Tags值可以用于前向渲染路径和延迟渲染路径。</p></li></ul><p><strong>NOTE</strong>: UPR does not support the following LightModetags: Always, ForwardAdd, PrepassBase, PrepassFinal, Vertex,VertexLMRGBM, VertexLM.</p><h4 id="universalmaterialtype">UniversalMaterialType</h4><p>Unity在延迟渲染路径中使用此标签，具有<code>Lit</code>和<code>SimpleLit</code>两个值，默认使用Lit。</p><ul><li>Lit：在G-bufferPass期间，Unity使用模板stencil来标记此像素使用Lit着色器类型（高光模型使用PBR）。</li><li>SimpleLit：在G-bufferPass期间，Unity使用模板stencil来标记此像素使用Simple着色器模型（高光模型使用Blinn-Phong）。</li></ul><h4 id="passflags">PassFlags</h4><p>在内置渲染管线中，使用此标签决定Unity提供给Pass何种数据。</p><h4 id="requireoptions">RequireOptions</h4><p>在内置渲染管线中，通过此标签以及projectsettings决定是否启用此Pass。只有一个值<code>SoftVegetation</code>。</p><h3 id="参考资料">参考资料</h3><p><ahref="https://docs.unity3d.com/Manual/writing-shader-tags-introduction.html">Introductionto shader tags</a></p>]]></content>
      
      
      <categories>
          
          <category> Unity </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
            <tag> Shader </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文明随笔</title>
      <link href="/2023/12/28/%E9%9A%8F%E7%AC%94/%E9%9A%8F%E7%AC%94-%E6%96%87%E6%98%8E/"/>
      <url>/2023/12/28/%E9%9A%8F%E7%AC%94/%E9%9A%8F%E7%AC%94-%E6%96%87%E6%98%8E/</url>
      
        <content type="html"><![CDATA[<h1 id="文明随笔">文明随笔</h1><p>###<strong>卡尔达舍夫等级</strong></p><p>卡尔达舍夫等级是一种用来衡量一个文明的技术的先进等级的方法，以一个文明能用来与通讯交流外行星的能量的多少为基础。也就是说能用大量能量与外界沟通的行星，才可以算入卡尔达舍夫等级。</p><p>类型I：该文明是行星能源的主人，这意味着他们可以主宰这颗行星以及周围卫星能源的总和。</p><p>类型II ：该文明能够收集整个恒星系统的能源。</p><p>类型III ：该文明可以利用<ahref="https://baike.baidu.com/item/银河系/189795">银河系</a>系统的能源而为其所用。</p><h3 id="元小说-meta">元小说 meta</h3><p>英国杜伦大学英文系教授帕特里夏·沃芙（PatriciaWaugh）是最早对元小说作出体系阐释的理论家之一。她在其专著《元小说：自我认识小说的理论与实践》中给“元小说”一词下的界说是：“<strong>元小说</strong>是小说写作的一个术语，它<strong>有认识地、体系地使我们重视其作为人工制品的位置</strong>，以此提出有关小说和实际之间联系的问题。</p><p>mate-game 元游戏 概念来自于 元小说</p><p>《思考的乐趣》一本有趣的书</p><p>《苏菲的世界》书</p><p>《史丹利寓言》《映月城与电子姬》元游戏</p><p>《Doki Doki Literature Club》元游戏</p><h3 id="游戏纪事">游戏纪事</h3><h4 id="雅达利大崩溃">雅达利大崩溃</h4><p>雅达利大崩溃发生在1983年的雅达利大崩溃(学名：雅达利冲击 AtariShock)之前一直是少部分经济学专业同学和游戏圈内人士知道的一个事件。</p><p>华纳用6个星期制作的《ET外星人》，玩家的唾弃让大量游戏只能烂在仓库里，雅达利的声望跌至谷底，其母公司不得不将一代霸主分拆出售，史称“雅达利大崩溃”。</p><p>雅达利母公司华纳用6个星期时间做出《ET外星人》这一史上最著名IP烂作。1982年的圣诞季，《ET外星人》卖出150W份，而雅达利准备了400万份游戏。多出来的250W份游戏卡带，最终同其他一些卖不出的存货一起被埋进了新墨西哥州的阿拉莫戈多垃圾镇。</p><p>在当时游戏硬件&amp;软件生产商<ahref="https://baike.baidu.com/item/雅达利">雅达利</a>实施的“数量压倒质量”的政策下，一年之内美国市场上出现了近万款游戏。大量同质化垃圾游戏让<ahref="https://baike.baidu.com/item/美国/125486">美国</a>玩家彻底失去信心，并最终导致了1982年圣诞节的市场大崩溃。自此之后的四年，美国再无人敢谈及游戏行业，本土游戏机市场彻底消失</p><p>而后为任天堂撑起了北美的游戏产业</p>]]></content>
      
      
      <categories>
          
          <category> ink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> civilization </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>生物随笔</title>
      <link href="/2023/12/28/%E9%9A%8F%E7%AC%94/%E9%9A%8F%E7%AC%94-%E7%94%9F%E7%89%A9/"/>
      <url>/2023/12/28/%E9%9A%8F%E7%AC%94/%E9%9A%8F%E7%AC%94-%E7%94%9F%E7%89%A9/</url>
      
        <content type="html"><![CDATA[<p>自然界绝对是最适合找灵感的地方，有很多奇妙的事物。以自己过目就忘的本领，想必看过的新奇玩意过几天就忘记的差不多了，这里就当作字典吧，方便以后回顾。</p><hr /><h3 id="光明女神闪蝶">光明女神闪蝶</h3><p>光明女神闪蝶（学名：MorphoHelena）前足相当退化，短小无爪。前翅R脉5条，常共柄。卵半圆球形。幼虫头上有突起，体节上有枝刺，腹足趾钩1至3序中列式。蛹为<ahref="https://baike.baidu.com/item/垂蛹/5500790">垂蛹</a>。雄闪蝶的翅上有绚丽的金属般光泽，这与其翅膀上有各种形状的鳞片有关，闪蝶的鳞片结构复杂，细微结构是由多层立体的栅栏构成，当光线照射到翅上时，会产生折射、反射和绕射等物理现象。翅的底面具有成列的眼状斑纹。生活史包括了卵、幼虫、蛹、成虫四个期，卵呈半球形，幼虫一般群集生活，主要取食豆科植物，分布于南美洲北部的热带雨林。</p><p>光明女神闪蝶(海伦娜闪蝶)是<ahref="https://baike.baidu.com/item/秘鲁/258354">秘鲁</a>国蝶，是世界上最美丽的蝴蝶。其前翅两端的蓝色有深蓝、湛蓝、浅蓝层次的变化，,整个翅面犹如蓝色的天空镶嵌串亮丽的光环，给人间带来光明。它的形状、颜色都无与伦比、无可挑剔光眀女神闪蝶主要生活在南美的秘鲁<ahref="https://baike.baidu.com/item/亚马孙河/155637">亚马孙河</a>流域，如今基本绝迹。</p><h3 id="蛾蜡蝉"><strong>蛾蜡蝉</strong></h3><p>蛾蜡蝉若虫</p><p><img src="/images/ink/1.jpg" /></p><p>蛾蜡蝉成虫（碧蛾蜡蝉） 中国各地均有分布，为农业害虫</p><p><img src="/images/ink/2.jpg" /></p><h3 id="尺蠖huo"><strong>尺蠖</strong>(huo)</h3><p>尺蛾幼虫，完全变态</p><p><img src="/images/ink/3.jpg" /></p><p>尺蛾</p><p><img src="/images/ink/4.jpg" /></p><h3 id="大腹园蛛"><strong>大腹园蛛</strong></h3><p>农村常见的织网蜘蛛，国内分布较广</p><p>!<img src="/images/5.jpg" /></p><h3 id="盲蛛"><strong>盲蛛</strong></h3><p>与蜘蛛为不同种属，特点为大长腿，视力很差，通常无毒</p><p><img src="/images/ink/6.jpg" /></p><h3 id="叩甲科类"><strong>叩甲（科类）</strong></h3><p>民间常称之为“磕头虫”，北方多为黑色，南方叩甲颜色较为鲜艳</p><p><img src="/images/ink/7.jpg" /></p><h3 id="乌鸫"><strong>乌鸫</strong></h3><p>雄性除黄色眼圈和喙外，全身都为黑色，幼鸟和雌性没有黄色眼圈。在欧亚大陆广泛分布，为瑞典国鸟</p><p><img src="/images/ink/8.jpg" /></p><h3 id="毒隐翅虫"><strong>毒隐翅虫</strong></h3><p>鞘翅目 隐翅虫科 毒隐翅虫属</p><p>分布广泛，我国约有21种，潮湿环境较为常见。其体内含有强烈接触毒物，强酸性，触及皮肤可导致皮炎，出现痒红肿痛、水疱、液疱，与皮肤烧伤类似。</p><p><img src="/images/ink/9.jpg" /></p><h2 id="云">云</h2><h3 id="糙面云"><strong>糙面云</strong></h3><p>一种极端的天气现象，出现一般会下大雨（也不是特别大）看起来有一种非真实感</p><p><img src="/images/ink/10.jpg" /></p><p><img src="/images/ink/11.jpg" /></p><h3 id="乳状积雨云"><strong>乳状积雨云</strong></h3><p>常伴有闪电、雷击和暴风，若带有绿色光晕可能会形成冰雹</p><p><img src="/images/ink/12.jpg" /></p>]]></content>
      
      
      <categories>
          
          <category> ink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Animals </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《3D数学基础》—— 10.5 纹理映射</title>
      <link href="/2023/05/30/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A3D%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E2%80%94%E2%80%94%2010.5%20%E7%BA%B9%E7%90%86%E6%98%A0%E5%B0%84/"/>
      <url>/2023/05/30/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A3D%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E2%80%94%E2%80%94%2010.5%20%E7%BA%B9%E7%90%86%E6%98%A0%E5%B0%84/</url>
      
        <content type="html"><![CDATA[<h3 id="概念介绍">概念介绍</h3><p><strong>纹理贴图（Texture Map）</strong></p><p>简单来讲就是“粘贴”到对象表面的位图图像，是应用于模型表面的常规位图。</p><p><strong>纹理元素（Texel）</strong></p><p>纹理元素是指纹理贴图中的单个像素，可以将Texel理解为TexturePixel的简称，有时简称为“纹素”。它和体素（Voxel）的构词法类似，后者为体积元素（VolumePixel）的简称。计算机图形学的上下文中会谈及很多不同类型的位图，该术语就是通过一种简洁的方式区分帧缓冲区中的像素（Pixel）和纹理中的像素（Texel）。</p><h3 id="工作原理">工作原理</h3><p>有许多种不同的方法可以将纹理贴图应用到网格上，如平面映射（PlanarMapping）可以将纹理按正交方式投影到网格上，除此之外还有，球体映射（SphericalMapping）、圆柱形映射（Cylindrical Mapping）和立方体映射（CubicMapping）等类型。</p><p>纹理映射的<strong>关键要点</strong>是，在网格表面上的每个点处，可以获得纹理映射坐标（Texture-MappingCoordinate），其定义了纹理贴图中与网格三维位置对应的二维位置。</p><p>传统上，纹理映射坐标分配了变量（u,v），其中u为水平坐标，v为垂直坐标。因此，纹理坐标通常称为UV坐标（UVCoordinate）或者简称为UV。虽然位图有不同大小，但是UV坐标会被归一化。</p><p>通常情况下，只在顶点级别计算或指定UV坐标，并通过插值获得某个面的任意内部位置的UV坐标。有一个很形象的比喻：</p><blockquote><p>可以将纹理贴图想象成一块弹力布，当将纹理贴图坐标指定给一个顶点时，就好像使用一根大头针将该弹力布固定在那些UV坐标上，以便通过这种方式将布钉在该顶点的表面上。对于每个顶点都有一根大头针，因此整个表面都会被覆盖贴图。</p></blockquote><h3 id="样例">样例</h3><p>同一张纹理贴图和同一个网格，分配给顶点的UV值不同时，效果也是不一样的，如下图：</p><p><img src="/images/3DMath/texcoord-1.png" /></p><p>除此之外，UV坐标值允许在[0,1] 范围之外，可以有多种方式解释。</p><p>最常见的处理方式是<strong>重复（Repeat）</strong>——也称为<strong>平铺（Tile）</strong>，其会丢弃坐标的整数部分，仅使用小数部分，结果就是使纹理重复。</p><p>另外还有一种方式是<strong>锁住（Clamp），</strong>当使用[0,1]范围之外的坐标访问位图时，它会被锁在[0,1]范围内，结果就是对于超出范围的区域复制位图边缘像素。</p><p>当然还有其他方式，某些硬件可能还支持<strong>镜像（Mirror）</strong>等选项，镜像类似于重复，区别在于每一个其他平铺的块都是镜像的，可以保证相邻的平铺块之间不存在“接缝”，而是一个连续的图像。</p><p><img src="/images/3DMath/texcoord-2.png" /></p>]]></content>
      
      
      <categories>
          
          <category> Graphic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D Math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>《3D数学基础》 —— 10.1 图形工作原理</title>
      <link href="/2023/02/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A3D%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E2%80%94%E2%80%94%2010.1%20%E5%9B%BE%E5%BD%A2%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/"/>
      <url>/2023/02/24/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/%E3%80%8A3D%E6%95%B0%E5%AD%A6%E5%9F%BA%E7%A1%80%E3%80%8B%E2%80%94%E2%80%94%2010.1%20%E5%9B%BE%E5%BD%A2%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p><strong>书籍简述</strong> 《3D数学基础：图形和游戏开发》(第2版)清华大学出版社 《3D Math Primer for Graphics and Game Development,Second Edition》 [ Fletcher Dunn, lan Parberry ]本书是关于3D数学、三维空间的几何和代数的入门教材，也是计算机图形学不错的入门书籍。除实体书外，作者还开放了网站<a href="http://gamemath.com/">3D Math Primer for Graphics and GameDevelopment</a>，可以阅读书内的所有内容。网站作者亲口告诉我们“阅读第二版要远胜于第一版，何况网站可以免费阅读第二版全书，pleasedo me a favor and never show it to any person, includingyourself”，不过无论任何一版看完都会很受益把。</p><h2 id="两种主要的渲染方式">两种主要的渲染方式</h2><p>渲染的最终目标是位图(Bitmap)，位图是一个矩形的颜色数组，每个网格条目都称为像素(Pixel)，它是“图像元素”的简称。如何确定每个像素的颜色呢？这是渲染的基本问题。像计算机科学中的许多挑战一样，一个很好的起点是通过调查来了解大自然是如何运作的。</p><p>我们人眼感知的图象是光线在环境中反弹并进入眼睛的结果，这是一个极其复杂的光物理和生理学过程。在忽略大量细节后，于是渲染系统必须为每个像素回答基本问题是：从对应于该像素的方向接近相机的光的颜色是什么？</p><p>可以将此基本问题分解为两个任务，本书将这两个任务称为<strong>渲染算法（RenderingAlgorithm）</strong>。</p><table><colgroup><col style="width: 100%" /></colgroup><thead><tr><th>渲染算法</th></tr></thead><tbody><tr><td>① 可见表面确定(Visible Surface Determination)，在与当前像素对应方向上找到最靠近眼睛的表面。 ②照明(Lighting)，确定在眼睛方向上从该表面发射和/或反射的光。</td></tr></tbody></table><h3 id="光线追踪-ray-tracing">光线追踪 Ray Tracing</h3><p><strong>可见表面确定</strong>有两种常见的解决方案，第一种称为<strong>光线追踪(RayTracing)，</strong>不是从发射避免沿着光线行进方向<strong>跟踪(Follow)</strong>光线，而是向后<strong>追踪(Tracing)</strong>光线。这样只处理重要光线，从眼睛的穿过每个像素中心的方向发出光线，查看管线照射到场景中的第一个对象，计算从该方向返回的颜色。以下为该算法高度简化代码摘要：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">for(each x,y screen pixel)</span><br><span class="line">&#123;</span><br><span class="line">    // 为该像素选择光线</span><br><span class="line">    Ray ray = getRayForPixel(x, y);</span><br><span class="line"></span><br><span class="line">    // 光线与集合体相交，不仅会返回相交点，还包括该点所需的表面法线和其他信息</span><br><span class="line">    Vector3 pos, normal;</span><br><span class="line">    Object *obj;</span><br><span class="line">    Material *mtl;</span><br><span class="line">    if(rayIntersectScene(ray, pos, normal, obj, mtl))</span><br><span class="line">    &#123;</span><br><span class="line">        // 对相交的点着色</span><br><span class="line">        Color c = shadePoint(ray, pos, normal, obj, mtl);</span><br><span class="line">        // 放入帧缓冲</span><br><span class="line">        writeFrameBuffer(x, y, c);</span><br><span class="line">    &#125;</span><br><span class="line">    else</span><br><span class="line">    &#123;</span><br><span class="line">        //光线未命中场景，采用普通背景颜色</span><br><span class="line">        writeFrameBuffer(x, y, backgroundColor);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="深度缓存-depth-buffering">深度缓存 Depth Buffering</h3><p>另外一种<strong>可见表面确定</strong>主要策略为<strong>深度缓存(DepthBuffering)。</strong>其基本思路为，每个像素处不仅存储颜色，还存储深度值，记录从眼睛到表面的距离。</p><p>在上述代码清单中光线追踪算法的“外部循环”是屏幕空间，但在实时图形中，“外部循环”是构成场景表面的几何元素（这句话并不是十分准确？但大多情况下应该是这样的）。描述场景表面有许多不同方法，这里不做细究。在将表面投影到屏幕空间，并进行光栅化(Rasterization)成为源片段(SourceFragment)，也就是像素，将计算该像素的表面深度并与深度缓冲区现有值进行对比。若当前片段距离比缓冲区中的现有片段更远，则抛弃此片段，然后继续下一个片段。若当前片段比缓冲区中的现有片段更近，将会更新深度缓存，而且此时还可以进行渲染算法的第2步（至少对于此像素来说是如此），使用从该点的表面发射或反射光的颜色更新帧缓冲区。以上描述的过程也被称为<strong>前向渲染(ForwardRendering)。</strong>代码清单如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">// 清除帧缓冲区和深度缓冲区</span><br><span class="line">fillFrameBuffer(backgroundColor);</span><br><span class="line">fillDepthBuffer(infinity);</span><br><span class="line">// 外部循环对于所有图元（一般来说是三角形）进行迭代</span><br><span class="line">for(each geometric primitive)</span><br><span class="line">&#123;</span><br><span class="line">    // 对图元进行光栅化</span><br><span class="line">    for(each pixel x,y in the projection of the primitive)</span><br><span class="line">    &#123;</span><br><span class="line">        // 测试深度缓冲区，查看是否有更近的像素被写入</span><br><span class="line">        float primDepth = getDepthOfPrimitiveAtPixel(x, y);</span><br><span class="line">        if(primDepth &gt; readDepthBuffer(x, y))</span><br><span class="line">        &#123;</span><br><span class="line">            // 该图元的像素被模糊，丢弃</span><br><span class="line">            continue;</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        // 确定在此像素处图元的颜色</span><br><span class="line">        Color c = getColorOfPrimitiveAtPixel(x, y);</span><br><span class="line"></span><br><span class="line">        // 更新颜色和深度缓冲区</span><br><span class="line">        writeFrameBuffer(x, y, c);</span><br><span class="line">        writeDepthBuffer(x, y, primDepth);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>和前向渲染相反的是<strong>延迟渲染(DeferredRendering)。</strong>除帧缓冲区和深度缓冲区外，延迟渲染器还是使用其他缓冲区，统称为<strong>G缓冲区(G-Buffer)</strong>。其实就是几何缓冲区(GeometryBuffer)的缩写，它把保存有关该位置最靠近眼睛的表面的额外信息，例如表面的三维位置、表面法线和照明计算所用的材质属性等。与前向渲染器相比，延迟渲染器从字面意义上更好遵循两部渲染算法。</p><p>首先，将场景“渲染”到G缓存区，基本上仅执行可见性确定——获取每个像素“看到”但尚未执行照明计算的点的材质属性。第二遍真正执行照明计算。代码清单如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">// 清除几何缓冲区和深度缓冲区</span><br><span class="line">clearGeometryBuffer();</span><br><span class="line">fillDepthBuffer(infinity);</span><br><span class="line"></span><br><span class="line">//光栅化所以图元到G缓冲区</span><br><span class="line">for(each geometric primitive)</span><br><span class="line">&#123;</span><br><span class="line">    for(each pixel x,y in the projection of the primitive)</span><br><span class="line">    &#123;</span><br><span class="line">        // 测试深度缓冲区</span><br><span class="line">        float primDepth = getDepthOfPrimitiveAtPixel(x,y);</span><br><span class="line">        if(primDepth &gt; readDepthBuffer(x,y))</span><br><span class="line">        &#123;</span><br><span class="line">            continue;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        // 提取在第2遍中着色所需的信息</span><br><span class="line">        MaterialInfo mtlInfo;</span><br><span class="line">        Vector3 pos, normal;</span><br><span class="line">        getPrimitiveShadingInfo(x, y, mtlInfo, pos, normal);</span><br><span class="line"></span><br><span class="line">        // 将它保存到G缓冲区和深度缓冲区</span><br><span class="line">        writeGeometryBuffer(x, y, mtlInfo, pos, normal);</span><br><span class="line">        writeDepthBuffer(x, y, primDepth);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">// 现在是第二遍，开始在屏幕空间着色</span><br><span class="line">for(each x,y screen pixel)</span><br><span class="line">&#123;</span><br><span class="line">    if(readDepthBuffer(x,y) == infinity)</span><br><span class="line">    &#123;</span><br><span class="line">        // 这里没有几何体，只需写入背景颜色即可</span><br><span class="line">        writeFrameBuffer(x, y, backgroundColor);</span><br><span class="line">    &#125;</span><br><span class="line">    else</span><br><span class="line">    &#123;</span><br><span class="line">        // 从gBuffer中取回着色信息</span><br><span class="line">        MaterialInfo mtlInfo;</span><br><span class="line">        Vector3 pos, normal;</span><br><span class="line">        readGeometryBuffer(x,y, mtlInfo, pos, normal);</span><br><span class="line"></span><br><span class="line">        // 将它放入帧缓冲区</span><br><span class="line">        writeFrameBuffer(x, y, c);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="描述表面特性brdf">描述表面特性：BRDF</h2><p>在了解完渲染算法的第一步可见表面确定后，来到了第二步：<strong>照明</strong>。在找到最接近眼睛的表面后，需要确定直接从该表面发射的光量，或者从其他光源发出并沿着眼睛方向从表面反射的光量。大多数情况下，我们将注意力集中在非发光表面上。</p><p>我们平时所讨论的物体“颜色”，实际上就是进入眼睛的光。因此我们需要了解的问题是：什么颜色的光入射到表面？从哪个方向？从哪个方向观察表面？因此，适用于渲染的表面的描述不能回答“这个表面是什么颜色的？”，这个问题有时毫无意义——例如，镜子是什么颜色的？相反，它有点像问：“<strong>当给定颜色的光从给定入射方向照射到表面上是，有多少光被反射到其他特定方向上</strong>？”</p><p>对于这个问题的答案可以由<strong>双向反射分布函数(BidirctionalReflectance Distribution Function,BRDF)</strong>给出。因此，我们不应该问，“该对象是什么颜色的”而是要问：“反射光的分布是什么样的？”</p><p>BRDF可写为 ，该函数的值是一个标量，描述了在点 上从 方向入射的光反射到而不是其他出射方向的相对可能性。</p><p>BRDF通过依赖 和来解释两个物体的“闪亮”差异：一个完美的反射器（如镜子），可以在一个出射方向上反射来自一个入射方向的所有光线。而一个完美的漫反射表面可以在所有出射方向上均匀的反射光线，无论入射方向如何。</p><p>即光的波长，不同颜色的光通常以不同的方式反射，BRDF通过对的依赖来解释两个物体之间的颜色差异：任何给定波长的光具有其自身的反射分布。</p><p>初次之外，BRDF必须满足某些标准才能在物理上变得合理。首先，在任何方向上反射负的光量是没有意义的。其次，全反射光也不可能比入射的光多，因此反射光小于入射光。此规则通常称为<strong>规范化约束(NormalizationConstraint)</strong>。最后还有一个不太明显的物理表面应该遵循的原则，即<strong>赫尔姆霍兹互反定律(HelmholtzReciprocity)</strong>：如果选择两个任意方向，无论哪个方向是入射方向，哪个是出射方向，应该反射相同比例的光。由于此定理的存在，BRDF中的两个方向标记“in”和“out”有些作者会省略。</p><h3 id="brdf推广">BRDF推广</h3><ul><li>双向表面散射分布函数(Bidirectional Surface Scattering DistributionFunction, BSSDF)</li></ul><p>通过允许方向矢量指向回表面，可以很容易的结合半透明和光折射。有时候，光线照射到物体，在物体内部反射，然后在不同的点出射。这种现象被称为此表面反射(SubsurfaceScattering)，并且是许多常见物质，如皮肤和牛奶，外观的重要表现。需要将单个反射点分成 和，其由BSSDF使用。</p><ul><li>双向散射分布函数(Bidirctional Scattering Distribution Function,BSDF)</li></ul><p>甚至体积效应，例如雾，也可以通过删除“表面”一词并在空间中的任何点定义双向散射分布函数(BSDF)来表达，而不仅仅在表面上。</p>]]></content>
      
      
      <categories>
          
          <category> Graphic </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 3D Math </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shader杂记</title>
      <link href="/2022/12/31/Game%20Development/Shader-Tools/"/>
      <url>/2022/12/31/Game%20Development/Shader-Tools/</url>
      
        <content type="html"><![CDATA[<h2 id="cg-standard-library">Cg Standard Library</h2><p>因为Unity的ShaderLab中包含了cg，所以cg标准库中的函数可以直接在UnityShader中使用</p><p>网址 <ahref="https://developer.download.nvidia.cn/cg/index_stdlib.html">NVIDIADeveloper Cg Standard Library</a></p><h4 id="step">step</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">float</span> <span class="title">step</span><span class="params">(<span class="type">float</span> a, <span class="type">float</span> x)</span></span>;</span><br><span class="line"><span class="comment">// float1, float2, float3, float4, </span></span><br><span class="line"><span class="comment">// half, fixed</span></span><br></pre></td></tr></table></figure><h5 id="parameters">Parameters</h5><p><code>a</code> Scalar or vector reference value</p><p><code>x</code> Scalar or vector</p><h5 id="description">Description</h5><p>return <u>one</u> for each component of <code>x</code> that isgreater than or equal to the corresponding component in the referencevector <code>a</code>, and <u>zero</u> otherwise</p><h4 id="smoothstep">smoothstep</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">float</span> <span class="title">smoothstep</span><span class="params">(<span class="type">float</span> a, <span class="type">float</span> b, <span class="type">float</span> x)</span></span>;</span><br><span class="line"><span class="comment">// float1 float2 float3 float4</span></span><br><span class="line"><span class="comment">// half fixed</span></span><br></pre></td></tr></table></figure><h5 id="parameters-1">Parameters</h5><p><code>a</code> Scalar or vector minimum reference value</p><p><code>b</code> Scalar or vector minimum reference value</p><p><code>x</code> Scalar or vector</p><h5 id="decription">Decription</h5><p>Interpolates smoothly from <u>0</u> to <u>1</u> based on<code>x</code> compared to <code>a</code> and <code>b</code></p><pre><code>1. Returns 0 if &lt;u&gt;x &lt; a &lt; b&lt;/u&gt; or &lt;u&gt;x &gt; a &gt; b&lt;/u&gt;1. Returns 1 if &lt;u&gt;x &lt; b &lt; a&lt;/u&gt; or &lt;u&gt;x &gt; b &gt; a&lt;/u&gt;1. Returns a value in the range[0,1] for the domain [a,b]</code></pre><p>The slope of smoothstep(a, b, a) and smoothstep(a, b, b) is zero.</p><p>For vectors, the return vector contains the smooth interpolation ofeach element of the vector x</p><h4 id="ddx-ddy">ddx ddy</h4><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="type">float</span> <span class="title">ddx</span><span class="params">(<span class="type">float</span> a)</span></span>;</span><br><span class="line"><span class="comment">// float1 float2 float3 float4</span></span><br><span class="line"><span class="comment">// half, fixed</span></span><br></pre></td></tr></table></figure><h5 id="parameters-2">Parameters</h5><p><code>a</code> Vector or scalar of which to approximate the partialderivative with respect to window-space X.</p><h5 id="description-1">Description</h5><p>Returns approximate partial derivative of <em>a</em> with respect tothe window-space (horizontal) <em>x</em> coordinate.For vectors, thereturned vector contains the approximate partial derivative of eachelement of the input vector.</p><p>这个函数单看文档有些难以理解，贴一个方便理解的博客：https://blog.csdn.net/yinhun2012/article/details/109393911</p><h2 id="unity-built-in-shader-include-files">Unity Built-in shaderinclude files</h2><p>Unity提供了很多内置的工具函数来让编写Shader变得更加简化和容易。这些工具函数都被声明在<code>.cginc</code>文件中，可以在这里查看Unity内置的文件：<ahref="https://docs.unity3d.com/Manual/SL-BuiltinIncludes.html">Built-inshader include files</a></p><p><code>.cginc</code>是可以被Shader引用的文件，Unity内置如下文件：</p><ul><li>HLSLSupport.cginc - ( <u>automatically included</u> ) Helper macrosand definitions for cross-platform shader compilation</li><li>UnityShaderVariables.cginc - ( <u>automatically included</u> )Commonly used global variables.</li><li>UnityCG.cginc - commonly used helper function</li><li>AutoLight.cginc - lighting &amp; shadowing functionality, e.g.surface shaders use this file internally</li><li>Lighting.cginc - standard surface shader(automatically included bysurface shader) lighting models</li><li>TerrainEngine.cginc - helper functions for Terrain &amp; Vegetationshaders</li></ul><h3 id="unitycg.cginc">UnityCG.cginc</h3><p>官方文档：<ahref="https://docs.unity3d.com/Manual/SL-BuiltinFunctions.html">Functionsdeclared in UnityCG.cginc</a></p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将点坐标从object space转换到camera&#x27;s clip space</span></span><br><span class="line"><span class="comment">// 同 mul(UNITY_MATRIX_MVP, float4(pos, 1.0))</span></span><br><span class="line"><span class="comment">// 一般在vertex shader中使用</span></span><br><span class="line"><span class="function">float4 <span class="title">UnityObjectToClipPos</span><span class="params">(float3 pos)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将点坐标从object space转换到view space/camera space</span></span><br><span class="line"><span class="comment">// 同 mul(UNITY_MATRIX_MV, float4(pos, 1.0)).xyz</span></span><br><span class="line"><span class="comment">// 一般在vertex shader中使用</span></span><br><span class="line"><span class="function">float3 <span class="title">UnityObjectToViewPos</span><span class="params">(float3 pos)</span></span>;</span><br></pre></td></tr></table></figure><p>UnityCG.cginc文件还提供了 Screen-space helperfunctions，这些函数可以帮助我们计算出屏幕空间坐标，方便对screen-spacetexture进行采样。其返回<code>float4</code>，方便后续采样时做齐次除法(xy/w)。</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 将裁剪空间的坐标转换为可采样screenspace-mapped texture的纹理坐标</span></span><br><span class="line"><span class="function">float4 <span class="title">ComputeScreenPos</span><span class="params">(float4 clipPos)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 将裁剪空间坐标转换为可采样GrabPass texture的纹理坐标</span></span><br><span class="line"><span class="function">float4 <span class="title">ComputeGrabScreenPos</span><span class="params">(float4 clipPos)</span></span>;</span><br></pre></td></tr></table></figure><p>除此之外，Unity还提供了用于前向渲染的有关灯光相关的工具函数，请查阅官方文档。</p><h4 id="data-structures">Data structures</h4><p>UnityCG.cginc还封装了一些包含着色器语义结构体，可以直接使用而不用自主声明，直接用于顶点着色器的数据输入：</p><ul><li><code>appdata_base</code>: position, normal and one texturecoordinate</li><li><code>appdata_tan</code>: position, tangent, normal and one texturecoordinate</li><li><code>appdata_full</code>:position, tangent, normal, four texturecoordinates and color</li><li><code>appdata_img</code>: position, one texture coordinate</li></ul><p>虽然Unity提供了封装的语义，但是一般我们还是会通过语义自主声明顶点着色器的输入数据。两者的使用案例如下：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// #使用内置结构体</span></span><br><span class="line"><span class="function">v2f <span class="title">vert</span><span class="params">(appdata_base v)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    v2f o;</span><br><span class="line">    o.pos = <span class="built_in">UnityObjectToClipPos</span>(v.vertex);</span><br><span class="line">    o.color.xyz = v.normal * <span class="number">0.5</span> + <span class="number">0.5</span>;</span><br><span class="line">    o.color.w = <span class="number">1.0</span>;</span><br><span class="line">    <span class="keyword">return</span> o;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义结构体</span></span><br><span class="line"><span class="keyword">struct</span> <span class="title class_">appdata</span>&#123;</span><br><span class="line">    float4 vertex : POSITION;</span><br><span class="line">    float3 normal : NORMAL;</span><br><span class="line">    float2 uv : TEXCOORD0;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function">v2f <span class="title">vert</span><span class="params">(appdate v)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="comment">// ...  </span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>其中，<code>POSITION</code>等字段被称作语义，其相关内容请看文章《UnityShader Base》。</p>]]></content>
      
      
      <categories>
          
          <category> ink </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity Shader </tag>
            
            <tag> Game Development </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例复现 -- Believable Caustics 2</title>
      <link href="/2022/12/23/Game%20Development/Believable-Caustics-2/"/>
      <url>/2022/12/23/Game%20Development/Believable-Caustics-2/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>时隔而个多月，上篇的后续姗姗来迟。上一篇文章：</p><p>铭自：学习随笔 —— Believable Caustics （一）1 赞同 · 0 评论文章</p><p>上篇中，只是完成了物体shader，只能挂载物体上来展示焦散效果，在实际应用中略显尴尬。上篇中也提到，其实那位老哥的推还有续集：</p><p>https://twitter.com/flogelz/status/1167866285369131008?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1167866285369131008%7Ctwgr%5E%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fwww.alanzucconi.com%2F2019%2F09%2F13%2Fbelievable-caustics-reflections%2Ftwitter.com/flogelz/status/1167866285369131008?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1167866285369131008%7Ctwgr%5E%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fwww.alanzucconi.com%2F2019%2F09%2F13%2Fbelievable-caustics-reflections%2F</p><p>可以看到是完成了一个比较泛用的焦散效果，不需要为每个物体挂载shader，可以在水体shader中将焦散投射到水面下的物体上。其他不再多言，直接快进到如何实现类似的一个效果。</p><h2 id="连续的焦散">连续的焦散</h2><p>单物体shader会出现一下现象：</p><figure><imgsrc="https://picx.zhimg.com/80/v2-2c449f317f56b546c8bb6c0ea9724382_720w.jpg?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>物体shader的缺点</p><p>此前，我们使用的物体的uv，其会在物体的某些部分发生突变，用它来采样焦散tex的，就会产生上面的现象。现在需要的一个在物体上连续的采样坐标来避免上述问题，除此之外，场景平行光是以一个平面的方式行进得，这不由让我们联想到一个东西，世界坐标的xz平面。于是，我们可以用物体世界坐标的xz轴值对焦散tex进行采样。结果如下图，以结果来说还算符合常理：</p><figure><imgsrc="https://pic1.zhimg.com/80/v2-539ed1a2e6b88659be9916bac9368aff_720w.jpg?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>顶视图</p><figure><imgsrc="https://pic1.zhimg.com/80/v2-9e23fc41387bd4013473c94c2a798baf_720w.jpg?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>侧视图</p><p>部分代码展示：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">// vertex function</span><br><span class="line"> o.worldPos = mul(unity_ObjectToWorld, v.vertex).xyz;</span><br><span class="line">// fragment function</span><br><span class="line"> half2 uv = i.worldPos.xz;</span><br><span class="line"> fixed3 caustics1 = causticsSample(_CausticsTex, _CausticsTex1_ST, uv,</span><br><span class="line">            _CausticsSpeed1, _SplitRGB);</span><br><span class="line"> fixed3 caustics2 = causticsSample(_CausticsTex, _CausticsTex2_ST, uv,</span><br><span class="line">            _CausticsSpeed2, _SplitRGB);</span><br><span class="line"> fixed3 caustics = min(caustics1, caustics2);</span><br></pre></td></tr></table></figure><h2 id="从光源方向投射">从光源方向投射</h2><p>焦散是由光线经过透明表面投射到不透明表面而形成的，所以如果物体表面焦散不随场景平行光方向变化而变化，就会有一些违和感。只要再从上部分的思路上前进一点，便可以得到我们期望的结果。那就是：</p><blockquote><p><strong>使用光源坐标空间的xy坐标平面</strong></p></blockquote><p>具体做法，只要将得到的世界坐标通过矩阵变化，就能得到物体在光源空间的坐标，取其xy值对焦散tex采样即可：</p><figure><imgsrc="https://picx.zhimg.com/80/v2-08fb29cd31cb33aeefaec790b6928c86_720w.jpg?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>光源空间坐标采样</p><p>部分代码（与上部分只差了一个矩阵乘法）：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// fragment function</span><br><span class="line">half2 uv = mul(PrjMatrix, float4(i.worldPos, 1)).xy;</span><br><span class="line">fixed3 caustics1 = causticsSample(_CausticsTex, _CausticsTex1_ST, uv, _CausticsSpeed1, _SplitRGB);</span><br><span class="line">fixed3 caustics2 = causticsSample(_CausticsTex, _CausticsTex2_ST, uv, _CausticsSpeed2, _SplitRGB);</span><br><span class="line">fixed3 caustics = min(caustics1, caustics2);</span><br></pre></td></tr></table></figure><p>在加一个遮罩，消去光找不到的地方：</p><figure><imgsrc="https://picx.zhimg.com/80/v2-9c78ce82e384fa7af56e6be8e8325098_720w.jpg?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>NdotL遮罩后的焦散（调整亮度之后）</p><h2 id="添加至水体">添加至水体</h2><p>上述只是实现在一个物体上效果，正如我们之前提到的那样，焦散应该由水体shader单独完成。有了上面的基础，只需要在水体的shader上获取到水面下物体的世界空间坐标即可（再转换为光源空间坐标）。至于，如何具体实现，请转步：</p><p><em>对于焦散，如果，希望看推作者自己写的文章，请点参考文献第三篇。</em></p><p>最终效果：</p><figure><imgsrc="https://pic1.zhimg.com/80/v2-48aeb54d242c79753fd452287c7376cf_720w.jpg?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>最终效果</p><h2 id="参考文献">参考文献</h2><p><ahref="https://twitter.com/flogelz/status/1167866285369131008?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1167866285369131008%7Ctwgr%5E%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fwww.alanzucconi.com%2F2019%2F09%2F13%2Fbelievable-caustics-reflections%2F">FLOGELZ❄️on Twitter: Caustics Part 2/2✨</a></p><p><ahref="https://forum.unity.com/threads/solved-project-caustics-texture-from-light-direction.733637/">UnitForum</a></p><p><ahref="https://80.lv/articles/caustic-surface-production-guide/">80.lv</a></p><p><ahref="https://zhuanlan.zhihu.com/p/92315967">CJT：Unity从深度缓冲重建世界空间位置</a></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity Shader </tag>
            
            <tag> Caustics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>案例复现 -- Believable Caustics 1</title>
      <link href="/2022/12/23/Game%20Development/Believable-Caustics-1/"/>
      <url>/2022/12/23/Game%20Development/Believable-Caustics-1/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>注：此为学习其他资料时的个人笔记，文章多为引用</p><p>制作出较为真实的焦散效果是游戏开发者/计算机图形学相关领域从业者的必经之路。</p><figure><imgsrc="https://pica.zhimg.com/80/v2-919c4c829fbe428dcd6de7cc5b23d89c_720w.png?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>Twitter上一位游戏相关从业者（上图）曾经发推向大家描述过如何制作焦散效果，本文便以此为基础进行展开。</p><h2 id="何为焦散效果-caustics">何为焦散效果 Caustics</h2><p>可能你会对“焦散”这个词感到陌生，但它绝对存在在你的日常生活中。“焦散”是由光的折射产生的一种光学现象，你很可能会在洗手时观察到这种现象。透过不平整的水面光便会因为折射产生聚集的现象，当面积足够大、光线足够亮时这种现象会很明显。</p><figure><imgsrc="https://picx.zhimg.com/80/v2-4d97894b4a44124b70ec0e3383464743_720w.jpeg?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>光透过水面产生焦散现象</p><p>（顺便一提，闫老师并不赞同将 Caustics译为“焦散”，因为这种现象更多体现的是光的聚集，更应该被称为“聚焦”。在此我们便不多做讨论，称为“焦散”）</p><h2 id="效果模拟">效果模拟</h2><p>焦散非常有具有辨识度的一点便是，它会不停的移动，动态赋予了焦散效果灵魂。通过物理原理来重现焦散现象是非常昂贵的，因为需要模拟巨量的光线。因此,现在在游戏领域多使用tex来实现焦散效果。</p><figure><imgsrc="https://pic1.zhimg.com/80/v2-c3a418defd9ff05890f63c4c35e728ad_720w.gif?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>caustics_texture</p><p><em>flogelze</em>提出通过一下几点来制作一个可信的焦散现象：</p><ul><li>Impressing the caustics pattern on a model surfaces twice, each timeusing different sizes and speeds</li><li>Blending the two patterns using the <em>min</em> operator</li><li>Splitting the RGB channels during the sampling.</li></ul><h2 id="in-unity">In Unity</h2><p>尽量简洁的描述每一步的实现 然后展示当前shader效果</p><h3 id="创建一个surface-shader">创建一个Surface Shader</h3><p>因为会使用到场景中的灯光，所以最好的选择是创建一个surfaceshader，使用起来比较方便。使用默认的shader创建一个材质，赋予给物体即可。</p><figure><imgsrc="https://picx.zhimg.com/80/v2-3febe0569d3ed30671ab1533021afbd8_720w.png?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>默认shader</p><h3 id="采样-templing-texture">采样 Templing Texture</h3><p>这部分使用到的焦散采样图即为上文提及的采样图，首先需要声明shaderproperties 及相关变量：</p><figure><imgsrc="https://picx.zhimg.com/80/v2-a60e3744fe0782a14507aa3f0da598b3_720w.png?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure><imgsrc="https://picx.zhimg.com/80/v2-91cbfdc96ecff4844ff50a6e47dd33b2_720w.png?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>*_CausitcsTex_ST* 是与 *_CausticsTex*配套的缩放和偏移存储变量，使用了unity默认的命名方式，即xxx_ST。随后便是表面着色器的主要函数<em>surf(),</em>它决定了每一个像素如何被绘制到屏幕上，像素的最终颜色取决于变量o的值。o是 SurfaceOutputStandard类型的，unity把一系列决定像素颜色的字段都封装进了这个类型。现在我们只关注Albedo 字段，它大致对应于物体在白光照射下的颜色。</p><p>在一个新创建的表面着色器中，Albedo 值来自于默认的_MainTex。因为焦散效果是作用于原有物体效果之上的，于是直接采用简单的相加。</p><figure><imgsrc="https://pic1.zhimg.com/80/v2-48b38a83d5750265c33a5bc336450486_720w.png?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure><imgsrc="https://pica.zhimg.com/80/v2-7cbd609706dfc88b467647c6b125bdd1_720w.png?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>caustics sampling</p><h3 id="让caustics动起来">让Caustics动起来</h3><p>上面也提到过，焦散现象的一个非常标志性的特征，就是它会动。在shader中制作动画效果便会用到unity提供的接口*_Time* ，使用它可以得到游戏当前的运行时间。</p><figure><imgsrc="https://pica.zhimg.com/80/v2-f5b12e4c1b75f56e0c5c1835d2ff33b4_720w.png?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>上面便是一种非常简单的让焦散动起来的方法，创建 *_CausticsSpeed(Vector)* 属性来控制移动速度， 字段 *_Time.y*可以获得游戏运行时间（单位：秒）。</p><figure><imgsrc="https://picx.zhimg.com/80/v2-9c7e9764130281b8ce9a7574115a4305_720w.gif?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>animated caustics</p><h3 id="多次采样-multiple-sampling">多次采样 Multiple Sampling</h3><p>为什么需要多次采样？因为多次采样得到的效果更加自然。只需要将上面的过程重复两次即可。</p><figure><imgsrc="https://picx.zhimg.com/80/v2-fd3799d8650b22dfa28e663f50f86d97_720w.png?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure><imgsrc="https://picx.zhimg.com/80/v2-6be67324b272181f876e593b67901406_720w.gif?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>multiple sampling</p><h3 id="rgb-split">RGB split</h3><p>两次采样以后焦散效果已经不错了，但是，还可以让它的效果更进一步。不同波长的波在介质中的折射率不同，这意味光线会被分裂为不同的颜色，在物体表面上会形成更漂亮的彩色。</p><p>为了制作彩色的焦散效果，我们可以分别对 cautics texture的rgb三通道以细微的差距分别采样。</p><p>增加另外一个属性 *_SplitRGB*控制色彩分离的程度，然后可以将前面的重复操作封装起来，使代码更加美观。</p><figure><imgsrc="https://pic1.zhimg.com/80/v2-864e3d7e316a36ea3029ea99aacf1454_720w.png?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure><imgsrc="https://pic1.zhimg.com/80/v2-2bec6f3f85e9919ee65c18646c18f827_720w.png?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><figure><imgsrc="https://pica.zhimg.com/80/v2-3d195392a9fea2046a02e0b719f46530_720w.gif?source=d16d100b"alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>rgb split</p><h2 id="参考资料">参考资料</h2><p><ahref="https://www.alanzucconi.com/2019/09/13/believable-caustics-reflections/">https://www.alanzucconi.com/2019/09/13/believable-caustics-reflections/BelievableCaustics Reflections - Alan</a></p><p><ahref="https://twitter.com/flogelz/status/1165251296720576512?ref_src=twsrc%5Etfw%7Ctwcamp%5Etweetembed%7Ctwterm%5E1165251296720576512%7Ctwgr%5E%7Ctwcon%5Es1_&amp;ref_url=https%3A%2F%2Fwww.alanzucconi.com%2F2019%2F09%2F13%2Fbelievable-caustics-reflections%2F">https://twitter.com/flogelz/status</a></p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity Shader </tag>
            
            <tag> Caustics </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Unity学习笔记 - 渲染路径</title>
      <link href="/2022/02/14/Game%20Development/Unity%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E6%B8%B2%E6%9F%93%E8%B7%AF%E5%BE%84/"/>
      <url>/2022/02/14/Game%20Development/Unity%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%20-%20%E6%B8%B2%E6%9F%93%E8%B7%AF%E5%BE%84/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>看文章时有点搞不清楚渲染路径是什么东西，记录一下学习《UnityShader入门精要》9章1节的所得。</p><h2 id="unity的渲染路径">Unity的渲染路径</h2><p>在Unity中，渲染路径（RenderingPath）决定了<strong>光照</strong>是如何应用到UnityShader中的。如果要和光源打交道，就需要为每个pass指定使用的渲染路径。这样，才能让Unity为我们提供正确的数据信息。</p><p>Unity5.0版本之后，Unity主要支持两种渲染路径：<strong>前向渲染路径（ForwardRendering Path）、延迟渲染路径（Defferred Rendering Path）</strong></p><p>此外还有两种遗留的渲染路径：顶点照明渲染路径（Legacy Vertex LitRendering Path）、旧版延迟渲染路径（Legacy Defferred RenderingPath）</p><p>考虑到提及遗留的渲染路径会比较繁琐，所以在下文中大部分时间只说明两种主要渲染路径。</p><figure><img src="/images/Unity/RenderingPath-1.png" alt="img" /><figcaption aria-hidden="true">img</figcaption></figure><p>Unity 2019.4.11</p><p>在pass中，通过设置LightMode标签来指定使用的渲染路径。</p><blockquote><p>Tags { "LightMode" = "ForwardBase" }</p></blockquote><p>下表给出Pass的LightMode标签支持的主要渲染路径设置：</p><table><colgroup><col style="width: 16%" /><col style="width: 83%" /></colgroup><thead><tr><th>标签名</th><th>描述</th></tr></thead><tbody><tr><td>Always</td><td>不管使用哪种渲染路径，该Pass总会被渲染，但不会计算任何光照</td></tr><tr><td>ForwardBase</td><td>用于前向渲染。该Pass会计算环境光，最重要的平行光、逐顶点/SH光源和Lightmaps</td></tr><tr><td>ForwardAdd</td><td>用于前向渲染。该Pass会计算额外的逐像素光源，每个Pass对应一个光源</td></tr><tr><td>Deffered</td><td>用于延迟渲染。该Pass会渲染G缓冲（G-buffer）</td></tr><tr><td>ShadowCaster</td><td>把物体的深度信息渲染到阴影映射纹理（shadowmap）或一张深度纹理中</td></tr></tbody></table><p>官方LightMode介绍见链接：<ahref="https://docs.unity3d.com/2019.4/Documentation/Manual/shader-predefined-pass-tags-built-in.html">ShaderLab:Predefined Pass tags in the Built-in Render Pipeline</a></p><p>为Pass指定渲染路径标签可以使Unity提供正确的（对于shader编写者来说）内置光照变量。</p><h2 id="前向渲染路径">前向渲染路径</h2><p>前向渲染路径是传统的渲染方式，也是最常用的一种渲染路径。</p><h3 id="原理">原理</h3><p>每进行一次完整的前向渲染，我们需要渲染该对象的渲染图元，并计算两个缓冲区信息：<strong>颜色缓冲区</strong>和<strong>深度缓冲区。</strong>随后，利用深度缓冲来决定一个片元是否可见，如果可见就更新颜色缓冲区中的颜色值。描述前向渲染路径的大致过程：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Pass &#123;</span><br><span class="line">  for(each primitive in this model)&#123;</span><br><span class="line">    for(each fragment covered by this primitive)&#123;</span><br><span class="line">      if(failed in depth test)&#123;</span><br><span class="line">        discard;</span><br><span class="line">      &#125;else&#123;</span><br><span class="line">        float4 color = Shading(materialInfo, pos, normal, lightDir, viewDir);</span><br><span class="line">        writeFrameBuffer(fragment, color);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>对于每个逐像素光源，我们都需要进行上面一次完整的渲染过程。如果一个<strong>物体在多个逐像素光源的影响区域内</strong>，那么该物体就需要<strong>执行多个Pass，每个Pass计算一个逐像素光源的光照结果</strong>，然后在帧缓存中把这些光照结果混合起来得到最终的颜色值。</p><p>所以，当有大量逐像素光照的情况下，执行Pass数过多，会导致性能下降。一般，渲染引擎会限制每个物体的逐像素光照的数目。</p><h3 id="unity中的前向渲染">Unity中的前向渲染</h3><p>实际上，一个Pass不仅仅可以用来计算逐像素光照，也可以用来计算逐顶点等其他光照。</p><p>在Unity中，前向渲染有3种处理光照（即照亮物体）的方式：逐顶点处理、逐像素处理、球谐函数（SphericalHarmonics,SH）处理。决定一个光源使用哪种处理模式取决于它的<strong>类型</strong>和<strong>渲染模式</strong>。</p><blockquote><p>光源类型指的是该光源是平行光还是其他类型的光源光源渲染模式指的是该光源是否<strong>重要（Important）</strong></p></blockquote><p>如果一个光源的模式为Important，Unity就会把他当成一个逐像素光源来处理。可以在Light组件中进行设置：</p><figure><img src="/images/Unity/RenderingPath-2.png"alt="Light in Unity2019.4.11" /><figcaption aria-hidden="true">Light in Unity2019.4.11</figcaption></figure><p>在前向渲染中，当我们渲染一个物体时，Unity会根据场景中各个光源的设置以及这些光源对物体的影响程度（如距离的远近、光照的强度等）对这些光源进行一个重要度排序。其中，一定数目的光源会按逐像素的方式处理，然后最多有4个（书中说的应该是unity5，后续版本可能会变化）光源按逐顶点的方式处理，剩下的光源可以按照SH方式处理。Unity使用的判断规则如下：</p><ul><li>场景中最亮的平行光总是按逐像素处理</li><li>渲染模式被设置为Not Important的光源，会按逐顶点处理</li><li>渲染模式被设置为Important的光源，会按逐像素处理</li><li>如果根据以上规则得到的逐像素光源数量小于Quality Setting中的逐像素光源数量（Pixel LightCount），会有更多的光源以逐像素的方式进行渲染</li></ul><p>光照计算是Pass中进行的。前面有说，前向渲染有两种Pass：Base Pass 和AdditionalPass。通常来说，两种Pass进行标签和渲染设置以及常规光照计算如下图：</p><figure><img src="/images/Unity/RenderingPath-3.png" alt="两种前向渲染的Pass" /><figcaption aria-hidden="true">两种前向渲染的Pass</figcaption></figure><p>注：在AdditionalPass的渲染设置中，开启和设置了混合模式。这是因为，我们希望每个AdditionalPass可以和上一次的光照结果在帧缓存中进行叠加，从而得到最终的有多个光照的渲染效果。如果没有开启和设置渲染混合模式，那个AdditionalPass的渲染结果会覆盖掉之前的渲染结果，看起来就好像该物体只受该光源的影响。通常情况下，我们选择的混合模式是BlendOne One</p><h2 id="延迟渲染路径">延迟渲染路径</h2><p>前向渲染问题是：当场景中包含大量实时光源时，前向渲染的性能会急速下降。</p><p>延迟渲染是一种更古老的渲染方法，但由于前向渲染上述不足，其又流行起来。除了前向渲染中使用的颜色缓冲和深度缓冲外，<strong>延迟渲染还会利用额外的缓冲区，这些缓冲区也被统称为G缓冲（G-buffer），其中G是Geometry的缩写</strong>。G缓冲区存储了我们所关心的表面的其他信息，例如该表面的法线、位置、用于光照计算的材质属性等。</p><h3 id="原理-1">原理</h3><p>延迟渲染主要包括了两个Pass。在第一个Pass中，不进行任何光照计算，而仅仅计算哪些片元是可见的，这主要是通过深度缓存技术来实现，当发现一个片元是可见的，我们就把它的相关信息存储到G缓存中。然后，在第二个Pass中，利用G缓存区的各个片元信息，例如表面法线、视角方向、漫反射系数等，进行真正的光照计算。</p><p>延迟渲染的整个过程大致可以用下面的伪码来描述：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">Pass1&#123;</span><br><span class="line">  for(each primitive in this model)&#123;</span><br><span class="line">    for(each fragment covered by this primitive)&#123;</span><br><span class="line">      if(failed in depth test)&#123;</span><br><span class="line">        discard;</span><br><span class="line">      &#125;else&#123;</span><br><span class="line">        writeGBuffer(materialInfo, pos, normal);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">Pass2&#123;</span><br><span class="line">  for(each pixel in the screen)&#123;</span><br><span class="line">    if(the pixel is valid)&#123;</span><br><span class="line">      // 如果该像素是有效的</span><br><span class="line">      // 读取它对应G缓存中的数据</span><br><span class="line">      readGBuffer(pixel, materialInfo, pos, normal);</span><br><span class="line">      // 根据数据进行光照计算</span><br><span class="line">      float4 color = Shading(...);</span><br><span class="line">      writeFrameBuffer(pixel, color);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看出，延迟渲染使用的Pass数目通常就是两个，这个场景中包含的光源数目是没有关系的。换句话说，延迟渲染的效率不依赖于场景的复杂度，而是和我们使用的屏幕空间的大小有关。这是因为，我们需要的信息都存储在缓存区中，而这些缓存区可以理解成是一张张2D图象，我们计算实际就是在这些图像空间中进行的。</p><h3 id="unity中的延迟渲染">Unity中的延迟渲染</h3><p>Unity中有两种延迟渲染，在此我们不讨论遗留的延迟渲染。如果游戏中含有大量的实时光照，那么我们可能希望选择延迟渲染路径，但需要一定的硬件支持。</p><p>对于延迟渲染路径来说，它适合场景光源数目多、如果使用前向渲染会造成性能瓶颈的情况下使用。但是，延迟渲染有一些缺点：</p><ul><li>不支持真正的抗锯齿（anti-aliasing）功能</li><li>不能处理半透明物体</li><li>对显卡有一定要求。如果要使用延迟渲染的话，显卡必须支持MRT（MultipleRender Targets）、Shader Mode3.0及以上、深度渲染纹理以及双面的模板缓冲。</li></ul><p>当使用延时渲染时，Unity要求我们提供两个Pass。</p><ol type="1"><li>第一个Pass用于渲染G缓存。在这个Pass中，会把物体的漫反射颜色、高光反射颜色、平滑度、法线、自发光和深度等信息渲染到屏幕空间的G缓冲区中。对每个物体来说，这个Pass仅会执行一次。</li><li>第二个Pass用于计算真正的光照模型。这个Pass会使用上一个Pass中的渲染数据来计算最终光照颜色，存储到帧缓冲中。</li></ol><p>默认的G缓冲中<strong>渲染目标（Render Target,RT）</strong>列举如下（括号内字母表示通道），不同版本可能不同：</p><ul><li>RT0，ARGB32格式，（RGB）存储漫反射颜色，（A）没有被使用</li><li>RT1，ARGB32格式，（RGB）存储高光反射颜色（RGB），（A）存储高光反射的指数部分</li><li>RT2，ARGB2101010格式，（RGB）存储世界空间的法线信息，（A）没有被使用</li><li>RT3，ARGB2101010（非HDR）或ARGBHalf（HDR）格式，用于存储自发光+lightmap+ 反射探针（reflection probes）</li><li>深度缓冲+ <strong>模板缓冲（Stencil buffer）</strong></li></ul><h2 id="选择哪种路径"><strong>选择哪种路径</strong></h2><p>可以去Unity官方文档看看，里面有几种渲染路径的对比，包括特性对比、性能对比以及平台支持。总的来说，会根据游戏发布的目标平台来选择渲染路径。https://docs.unity3d.com/2019.4/Documentation/Manual/RenderingPaths.html</p><p>《Unity Shader入门精要》一书项目相关Github:https://github.com/candycat1992/Unity_Shaders_Book</p>]]></content>
      
      
      <categories>
          
          <category> Real-time Rendering </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Compute Shader 绘制 Mandelbrot Fractal 图案</title>
      <link href="/2021/12/08/Game%20Development/Compute%20Shader%20%E7%BB%98%E5%88%B6%20Mandelbrot%20Fractal%20%E5%9B%BE%E6%A1%88/"/>
      <url>/2021/12/08/Game%20Development/Compute%20Shader%20%E7%BB%98%E5%88%B6%20Mandelbrot%20Fractal%20%E5%9B%BE%E6%A1%88/</url>
      
        <content type="html"><![CDATA[<h2 id="前言">前言</h2><p>文章仅针对unity computeshader，通过案例来学习其基本用法。由于知识浅薄，仅提供一些表层的知识，如发现错误，请指出，万分感谢。</p><p>参考案例来自reddit中的一篇文章，参考资料的第一个链接可以看原文，源码也可以在他的文章中找到链接。</p><p>文章重点在于compute shader 的使用，因此Mandelbrot Fractal只做简单介绍。</p><h2 id="unity-compute-shader-基础">unity compute shader 基础</h2><p>根据unity的官方文档，compute shader与其他shader一样，属于项目的资源文件（assetfiles），以.compute为文件扩展名。一般情况下使用HLSL语言编写（其他情况请参考官方文档），在unity中需要与C#脚本配合使用。</p><p>在unity中通过右键菜单 <em>Create-&gt;Shader-&gt;Compute Shader</em>来创建出.compute文件，被创建的文件中自带以下代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">// Each #kernel tells which function to compile; you can have many kernels</span><br><span class="line">#pragma kernel CSMain</span><br><span class="line"></span><br><span class="line">// Create a RenderTexture with enableRandomWrite flag and set it</span><br><span class="line">// with cs.SetTexture</span><br><span class="line">RWTexture2D&lt;float4&gt; Result;</span><br><span class="line"></span><br><span class="line">[numthreads(8,8,1)]</span><br><span class="line">void CSMain (uint3 id : SV_DispatchThreadID)</span><br><span class="line">&#123;</span><br><span class="line">    // TODO: insert actual code here!</span><br><span class="line"></span><br><span class="line">    Result[id.xy] = float4(id.x &amp; id.y, (id.x &amp; 15)/15.0, (id.y &amp; 15)/15.0, 0.0);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们来逐行对代码进行分析：</p><p><strong>#pragma kernel CSMain</strong></p><p>一个compute shader文件必须包含至少一个 compute kernel,可以使用“#pragma kernel xxxx”指令来增添多个 computekernel。需要注意的是，此编译指令后面不能跟注释代码“//xxx”，这样会造成编译错误。该指令表示CSMain 函数是一个GPU入口，可以在C#脚本中进行调用。</p><p><strong>RWTexture2D<float4> Result;</strong></p><p>这句代码声明了一个用来保存计算结果的结构，可以把它理解为一个纹理，RW表明它可读可写，float是其包含的数据类型，主要用于与C#脚本进行数据传输。我们还可以在此处声明其他变量，像是intfloat这种用于计算。</p><p><strong>[numthreads(8,8,1)]</strong></p><p>如字面意思，每组线程包含的线程数目，需要和C#脚本中的*_shader.Dispatch(kernelIndex,8,8,1)*联系理解。numthreads(X,Y,Z)中的三个参数分别代表每个线程组（threadgroup）在xyz对应维度上的大小，这种定义线程数目的方式可以在逻辑上很容易用[x,y]或[x,y,z]来访问其中独立的线程。</p><p>可设置的线程数目最大值却决于你使用的comput shaderversion，如下表：</p><table><thead><tr><th>Compute Shader</th><th>Maximum Z</th><th>Maximum Threads(X<em>Y</em>Z)</th></tr></thead><tbody><tr><td>cs_4_x</td><td>1</td><td>768</td></tr><tr><td>cs_5_0</td><td>64</td><td>1024</td></tr></tbody></table><p>至于<em>numthreads()</em> 和 <em>Dispatch()</em>之间的联系，可以通过MSDN上的一张图来很快的理解。</p><p><img src="/images/Unity/compute-shader-1.png" /></p><p><strong>void CSMain(uint3 id : SV_DispatchThreadID) &amp;&amp;&amp;Result[id.xy]=float4(id.x &amp; id.y,(id.x &amp;15)/15.0,(id.y&amp;15)/15.0,0.0);</strong></p><p>这就是compute shader 中的一个核函数的实现了，与“<em>#pragma kernelCSMain</em>”相对应。其参数uint3id，就可以用上图来理解啦。就是在一次Dispatch()中某个线程的id，可以使用它来定位我们要输出renderTexture的某个pixel。这也就是后面那句“Result[id.xy]=xxxx”的含义了，等号右边即我们在GPU中处理得到的结果，用id的xy维度的数值（此案例中z维度为1）来确定此次计算的结果保存在Result中的哪个位置。</p><h2 id="mandelbrot-fractal">Mandelbrot Fractal</h2><p>MandelbrotFractal，即曼德勃罗分形，利用GPU并行的特性可以快速绘制。</p><p>Mandelbrot Fractal的图案由以下几个规则（粗略列一下）决定：</p><p>① 使用迭代公式：<span class="math inline">\(Z_{n+1} = Z_{n}^{2} +C\)</span> ，其中 C = a+bi；</p><p>② 从 0 开始进行迭代；</p><p>③ 迭代结果在区间（-2，2）；</p><p>④在复坐标系下绘制满足上述条件C所代表的复数，颜色决定其收敛的快慢；</p><h2 id="compute-shader-c">Compute Shader &amp; C</h2><p>于是，我们可以在Compute Shader 中写出如下代码：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">#pragma kernel pixelCalc</span><br><span class="line"></span><br><span class="line">RWTexture2D&lt;float4&gt; textureOut;</span><br><span class="line">RWStructuredBuffer&lt;double&gt; rect;// 绘制范围</span><br><span class="line">RWStructuredBuffer&lt;float4&gt; colors;// 绘制颜色</span><br><span class="line"></span><br><span class="line">[numthreads(32,32,1)]</span><br><span class="line">void pixelCalc (uint3 id : SV_DispatchThreadID)&#123;</span><br><span class="line">float k = 0.0009765625;// 1 / 1024</span><br><span class="line">double dx, dy;</span><br><span class="line">double p, q;</span><br><span class="line">double x, y, xnew, ynew, d = 0;</span><br><span class="line">uint iteration = 0;    // 迭代次数</span><br><span class="line">dx = rect[2] - rect[0];</span><br><span class="line">dy = rect[3] - rect[1];</span><br><span class="line">p = rect[0] + ((int)id.x) * k * dx;// 区域映射，把[-512,512) 映射到 [-2,2) 来计算</span><br><span class="line">q = rect[1] + ((int)id.y) * k * dy;</span><br><span class="line">x = p;</span><br><span class="line">y = q;</span><br><span class="line">while (iteration &lt; 255 &amp;&amp; d &lt; 4)&#123;// 迭代最多255次，对应255种颜色</span><br><span class="line">xnew = x * x - y * y + p;   // a^2 - b^2 + b </span><br><span class="line">ynew = 2 * x * y + q;       // 2ab + b</span><br><span class="line">x = xnew;</span><br><span class="line">y = ynew;</span><br><span class="line">d = x * x + y * y;</span><br><span class="line">iteration++;</span><br><span class="line">&#125;</span><br><span class="line">textureOut[id.xy] = colors[iteration];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>cshader代码中，与C#发生交互的部分就是那3行声明的变量。下面详细说明，cshader与c#如何配合使用。</p><p><strong>C# 方面：</strong></p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">//  需要声明的变量</span><br><span class="line">ComputeShader _shader            // 对Compute Shader 进行控制</span><br><span class="line">RenderTexture outputTexture      // 接受 GPU 中的计算结果</span><br><span class="line">ComputeBuffer colorsBuffer       // 向 GPU 缓存中传输数据</span><br><span class="line"></span><br><span class="line">// 初始化 textrue</span><br><span class="line">outputTexture = new RenderTexture(1024, 1024, 32);</span><br><span class="line">outputTexture.enableRandomWrite = true; </span><br><span class="line">outputTexture.Create();</span><br><span class="line"></span><br><span class="line">// 向 buffer 中写入数据</span><br><span class="line">colorsBuffer = new ComputeBuffer(colorArray.Length, 4 * 4);</span><br><span class="line">colorsBuffer.SetData(colorArray);</span><br><span class="line"></span><br><span class="line">// 初始化 shader ，并获取或设置shader中的变量</span><br><span class="line">_shader = Resources.Load&lt;ComputeShader&gt;(&quot;csFractal&quot;);</span><br><span class="line">kiCalc = _shader.FindKernel(&quot;pixelCalc&quot;);</span><br><span class="line">_shader.SetBuffer(kiCalc, &quot;colors&quot;, colorsBuffer);</span><br><span class="line">_shader.SetTexture(kiCalc, &quot;textureOut&quot;, outputTexture);</span><br><span class="line"></span><br><span class="line">// 在c#脚本中调用 compute shader</span><br><span class="line">_shader.Dispatch(kiCalc, 32, 32, 1);</span><br></pre></td></tr></table></figure><p>上述并不是完整的代码，只是贴出来方便说明。</p><p><strong>ComputeBuffer</strong> 是在使用computeshader使非常重要的类，官方文档中如此说明：</p><blockquote><p>GPU data buffer, mostly for use with compute shaders. <ahref="https://docs.unity3d.com/ScriptReference/ComputeShader.html">ComputeShader</a>programs often need arbitrary data to be read &amp; written into memorybuffers. ComputeBuffer class is exactly for that - you can create &amp;fill them from script code, and use them in compute shaders or regularshaders.</p></blockquote><p>简单来说，就是ComputeBuffer是 C#脚本 和 Compute Shader数据传输的中介，在C#脚本中创建buffer并写入数据，在ComputeShader中使用数据。</p><p><strong>RenderTexture</strong>正如前面部分说明过的，多用为结算结果载体。需要注意的一点是，如果要在computeshader中使用rendertexture，就需要将其enableRandomWrite属性置为ture。文档解释如下：</p><blockquote><p>Enable random access write into this render texture on Shader Model5.0 level shaders.</p></blockquote><p><strong>ComputeShader</strong>这个不用过多解释，用它在C#中操控shader的行为。</p><p>要提一下的是kiCalc这个变量，它就是个int类型变量，使用ComputeShader.FindKernel(string)来获取核函数在shader中的索引，就像后面用到kiCalc那样用于设置其他变量。</p><p>最后终于到了ComputeShader.Dispatch(int,int,int,int)，在C#脚本中调用这个函数就可以以我们设置的threadgroups num 和 thread num来执行compute中的核函数，参数中也用到了kiCalc即核函数索引这个变量。在调用这个函数之后，执行的结果就会填充在我们特定的RenderTexture中。</p><p>这便是C#脚本与ComputeShader配合使用的大致过程，在此做一个学习的记录，也希望能对你们有所帮助。</p><h2 id="执行结果">执行结果</h2><p><img src="/images/Unity/fractal-1.png" /></p><h2 id="参考资料">参考资料</h2><p>https://www.reddit.com/r/Unity3D/comments/7pa6bq/drawing_mandelbrot_fractal_using_gpu_compute/</p><p>https://docs.unity3d.com/2020.3/Documentation/Manual/class-ComputeShader.html</p><p>https://docs.microsoft.com/en-us/windows/win32/direct3dhlsl/sm5-attributes-numthreads?redirectedfrom=MSDN</p>]]></content>
      
      
      <categories>
          
          <category> Unity </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Unity </tag>
            
            <tag> Shader </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
